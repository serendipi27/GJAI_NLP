{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbf65c70",
   "metadata": {},
   "source": [
    "# 딥러닝 회귀분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85bcb589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>b</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      crim    zn  indus  chas    nox     rm   age     dis  rad  tax  ptratio  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296     15.3   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242     17.8   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242     17.8   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222     18.7   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222     18.7   \n",
       "\n",
       "        b  lstat  medv  \n",
       "0  396.90   4.98  24.0  \n",
       "1  396.90   9.14  21.6  \n",
       "2  392.83   4.03  34.7  \n",
       "3  394.63   2.94  33.4  \n",
       "4  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import rc\n",
    "rc('font', family='AppleGothic')\n",
    "\n",
    "path = '/Users/jsha/gjai/nlp/pytest/'\n",
    "df = pd.read_csv(path+'BostonHousing.csv', delim_whitespace=False, header=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7954b858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 506 entries, 0 to 505\n",
      "Data columns (total 14 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   crim     506 non-null    float64\n",
      " 1   zn       506 non-null    float64\n",
      " 2   indus    506 non-null    float64\n",
      " 3   chas     506 non-null    int64  \n",
      " 4   nox      506 non-null    float64\n",
      " 5   rm       506 non-null    float64\n",
      " 6   age      506 non-null    float64\n",
      " 7   dis      506 non-null    float64\n",
      " 8   rad      506 non-null    int64  \n",
      " 9   tax      506 non-null    int64  \n",
      " 10  ptratio  506 non-null    float64\n",
      " 11  b        506 non-null    float64\n",
      " 12  lstat    506 non-null    float64\n",
      " 13  medv     506 non-null    float64\n",
      "dtypes: float64(11), int64(3)\n",
      "memory usage: 55.5 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "779e6268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>b</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.613524</td>\n",
       "      <td>11.363636</td>\n",
       "      <td>11.136779</td>\n",
       "      <td>0.069170</td>\n",
       "      <td>0.554695</td>\n",
       "      <td>6.284634</td>\n",
       "      <td>68.574901</td>\n",
       "      <td>3.795043</td>\n",
       "      <td>9.549407</td>\n",
       "      <td>408.237154</td>\n",
       "      <td>18.455534</td>\n",
       "      <td>356.674032</td>\n",
       "      <td>12.653063</td>\n",
       "      <td>22.532806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.601545</td>\n",
       "      <td>23.322453</td>\n",
       "      <td>6.860353</td>\n",
       "      <td>0.253994</td>\n",
       "      <td>0.115878</td>\n",
       "      <td>0.702617</td>\n",
       "      <td>28.148861</td>\n",
       "      <td>2.105710</td>\n",
       "      <td>8.707259</td>\n",
       "      <td>168.537116</td>\n",
       "      <td>2.164946</td>\n",
       "      <td>91.294864</td>\n",
       "      <td>7.141062</td>\n",
       "      <td>9.197104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>3.561000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>1.129600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>1.730000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.082045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.190000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.449000</td>\n",
       "      <td>5.885500</td>\n",
       "      <td>45.025000</td>\n",
       "      <td>2.100175</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>375.377500</td>\n",
       "      <td>6.950000</td>\n",
       "      <td>17.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.256510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.690000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>6.208500</td>\n",
       "      <td>77.500000</td>\n",
       "      <td>3.207450</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>19.050000</td>\n",
       "      <td>391.440000</td>\n",
       "      <td>11.360000</td>\n",
       "      <td>21.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.677083</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>6.623500</td>\n",
       "      <td>94.075000</td>\n",
       "      <td>5.188425</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>666.000000</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>396.225000</td>\n",
       "      <td>16.955000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>88.976200</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>27.740000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>8.780000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>12.126500</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>711.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>396.900000</td>\n",
       "      <td>37.970000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             crim          zn       indus        chas         nox          rm  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean     3.613524   11.363636   11.136779    0.069170    0.554695    6.284634   \n",
       "std      8.601545   23.322453    6.860353    0.253994    0.115878    0.702617   \n",
       "min      0.006320    0.000000    0.460000    0.000000    0.385000    3.561000   \n",
       "25%      0.082045    0.000000    5.190000    0.000000    0.449000    5.885500   \n",
       "50%      0.256510    0.000000    9.690000    0.000000    0.538000    6.208500   \n",
       "75%      3.677083   12.500000   18.100000    0.000000    0.624000    6.623500   \n",
       "max     88.976200  100.000000   27.740000    1.000000    0.871000    8.780000   \n",
       "\n",
       "              age         dis         rad         tax     ptratio           b  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean    68.574901    3.795043    9.549407  408.237154   18.455534  356.674032   \n",
       "std     28.148861    2.105710    8.707259  168.537116    2.164946   91.294864   \n",
       "min      2.900000    1.129600    1.000000  187.000000   12.600000    0.320000   \n",
       "25%     45.025000    2.100175    4.000000  279.000000   17.400000  375.377500   \n",
       "50%     77.500000    3.207450    5.000000  330.000000   19.050000  391.440000   \n",
       "75%     94.075000    5.188425   24.000000  666.000000   20.200000  396.225000   \n",
       "max    100.000000   12.126500   24.000000  711.000000   22.000000  396.900000   \n",
       "\n",
       "            lstat        medv  \n",
       "count  506.000000  506.000000  \n",
       "mean    12.653063   22.532806  \n",
       "std      7.141062    9.197104  \n",
       "min      1.730000    5.000000  \n",
       "25%      6.950000   17.025000  \n",
       "50%     11.360000   21.200000  \n",
       "75%     16.955000   25.000000  \n",
       "max     37.970000   50.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dc1452a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'whiskers': [<matplotlib.lines.Line2D at 0x7fd65e476590>,\n",
       "  <matplotlib.lines.Line2D at 0x7fd65e4769d0>],\n",
       " 'caps': [<matplotlib.lines.Line2D at 0x7fd65e476e10>,\n",
       "  <matplotlib.lines.Line2D at 0x7fd65e488290>],\n",
       " 'boxes': [<matplotlib.lines.Line2D at 0x7fd65e476110>],\n",
       " 'medians': [<matplotlib.lines.Line2D at 0x7fd65e488710>],\n",
       " 'fliers': [<matplotlib.lines.Line2D at 0x7fd65e488f90>],\n",
       " 'means': [<matplotlib.lines.Line2D at 0x7fd65e488b50>]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.boxplot(x=[df.crim], notch=True, showmeans=True, \n",
    "           meanline=True, vert=True, labels=['범죄'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26786025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# 모든 컬럼의 boxplot 그리기\n",
    "plt.boxplot(x=[df[col] for col in df.columns], notch=True, showmeans=True, \n",
    "           meanline=True, vert=True,\n",
    "            labels=df.columns.to_list())\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c97c40d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    506.000000\n",
       "mean       3.613524\n",
       "std        8.601545\n",
       "min        0.006320\n",
       "25%        0.082045\n",
       "50%        0.256510\n",
       "75%        3.677083\n",
       "max       88.976200\n",
       "Name: crim, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.crim.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ba0767f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41.37032999999997"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.crim.quantile(0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e7b3f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    506.000000\n",
       "mean      11.363636\n",
       "std       23.322453\n",
       "min        0.000000\n",
       "25%        0.000000\n",
       "50%        0.000000\n",
       "75%       12.500000\n",
       "max      100.000000\n",
       "Name: zn, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.zn.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "304c6e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :13]\n",
    "y = df.iloc[:, 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e48536a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         crim   zn  indus  chas    nox     rm    age     dis  rad  tax  \\\n",
      "369   5.66998  0.0  18.10     1  0.631  6.683   96.8  1.3567   24  666   \n",
      "48    0.25387  0.0   6.91     0  0.448  5.399   95.3  5.8700    3  233   \n",
      "142   3.32105  0.0  19.58     1  0.871  5.403  100.0  1.3216    5  403   \n",
      "379  17.86670  0.0  18.10     0  0.671  6.223  100.0  1.3861   24  666   \n",
      "407  11.95110  0.0  18.10     0  0.659  5.608  100.0  1.2852   24  666   \n",
      "\n",
      "     ptratio       b  lstat  \n",
      "369     20.2  375.33   3.73  \n",
      "48      17.9  396.90  30.81  \n",
      "142     14.7  396.90  26.82  \n",
      "379     20.2  393.74  21.78  \n",
      "407     20.2  332.09  12.13  \n",
      "--------------------------------------------------\n",
      "         crim    zn  indus  chas    nox     rm    age     dis  rad  tax  \\\n",
      "472   3.56868   0.0  18.10     0  0.580  6.437   75.0  2.8965   24  666   \n",
      "254   0.04819  80.0   3.64     0  0.392  6.108   32.0  9.2203    1  315   \n",
      "409  14.43830   0.0  18.10     0  0.597  6.852  100.0  1.4655   24  666   \n",
      "183   0.10008   0.0   2.46     0  0.488  6.563   95.6  2.8470    3  193   \n",
      "449   7.52601   0.0  18.10     0  0.713  6.417   98.3  2.1850   24  666   \n",
      "\n",
      "     ptratio       b  lstat  \n",
      "472     20.2  393.37  14.36  \n",
      "254     16.4  392.89   6.57  \n",
      "409     20.2  179.36  19.78  \n",
      "183     17.8  396.90   5.68  \n",
      "449     20.2  304.21  19.31  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "print(X_train.head())\n",
    "print('-'*50)\n",
    "print(X_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f24f3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37114f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(30, activation='relu', input_dim=13))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1)) # 회귀분석 출력층에는 activation func이 불필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ce7a859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 30)                420       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                496       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 1,061\n",
      "Trainable params: 1,061\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8e84fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 379 samples\n",
      "Epoch 1/200\n",
      "379/379 [==============================] - 0s 1ms/sample - loss: 240.1939\n",
      "Epoch 2/200\n",
      "379/379 [==============================] - 0s 139us/sample - loss: 59.2571\n",
      "Epoch 3/200\n",
      "379/379 [==============================] - 0s 140us/sample - loss: 58.4115\n",
      "Epoch 4/200\n",
      "379/379 [==============================] - 0s 153us/sample - loss: 58.5168\n",
      "Epoch 5/200\n",
      "379/379 [==============================] - 0s 135us/sample - loss: 55.0016\n",
      "Epoch 6/200\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 50.5030\n",
      "Epoch 7/200\n",
      "379/379 [==============================] - 0s 179us/sample - loss: 49.4985\n",
      "Epoch 8/200\n",
      "379/379 [==============================] - 0s 139us/sample - loss: 48.4429\n",
      "Epoch 9/200\n",
      "379/379 [==============================] - 0s 140us/sample - loss: 48.3820\n",
      "Epoch 10/200\n",
      "379/379 [==============================] - 0s 146us/sample - loss: 45.0269\n",
      "Epoch 11/200\n",
      "379/379 [==============================] - 0s 146us/sample - loss: 44.1000\n",
      "Epoch 12/200\n",
      "379/379 [==============================] - 0s 148us/sample - loss: 44.5405\n",
      "Epoch 13/200\n",
      "379/379 [==============================] - 0s 143us/sample - loss: 43.2919\n",
      "Epoch 14/200\n",
      "379/379 [==============================] - 0s 149us/sample - loss: 40.7807\n",
      "Epoch 15/200\n",
      "379/379 [==============================] - 0s 145us/sample - loss: 38.8596\n",
      "Epoch 16/200\n",
      "379/379 [==============================] - 0s 184us/sample - loss: 38.7487\n",
      "Epoch 17/200\n",
      "379/379 [==============================] - 0s 173us/sample - loss: 36.3720\n",
      "Epoch 18/200\n",
      "379/379 [==============================] - 0s 168us/sample - loss: 35.8377\n",
      "Epoch 19/200\n",
      "379/379 [==============================] - 0s 145us/sample - loss: 36.3975\n",
      "Epoch 20/200\n",
      "379/379 [==============================] - 0s 152us/sample - loss: 35.2865\n",
      "Epoch 21/200\n",
      "379/379 [==============================] - 0s 141us/sample - loss: 36.6919\n",
      "Epoch 22/200\n",
      "379/379 [==============================] - 0s 136us/sample - loss: 36.4039\n",
      "Epoch 23/200\n",
      "379/379 [==============================] - 0s 145us/sample - loss: 31.6577\n",
      "Epoch 24/200\n",
      "379/379 [==============================] - 0s 139us/sample - loss: 33.4525\n",
      "Epoch 25/200\n",
      "379/379 [==============================] - 0s 135us/sample - loss: 29.3439\n",
      "Epoch 26/200\n",
      "379/379 [==============================] - 0s 135us/sample - loss: 30.7045\n",
      "Epoch 27/200\n",
      "379/379 [==============================] - 0s 141us/sample - loss: 28.8264\n",
      "Epoch 28/200\n",
      "379/379 [==============================] - 0s 139us/sample - loss: 28.0942\n",
      "Epoch 29/200\n",
      "379/379 [==============================] - 0s 137us/sample - loss: 28.4877\n",
      "Epoch 30/200\n",
      "379/379 [==============================] - 0s 138us/sample - loss: 26.4157\n",
      "Epoch 31/200\n",
      "379/379 [==============================] - 0s 144us/sample - loss: 25.9449\n",
      "Epoch 32/200\n",
      "379/379 [==============================] - 0s 154us/sample - loss: 26.0783\n",
      "Epoch 33/200\n",
      "379/379 [==============================] - 0s 144us/sample - loss: 26.3603\n",
      "Epoch 34/200\n",
      "379/379 [==============================] - 0s 144us/sample - loss: 28.7006\n",
      "Epoch 35/200\n",
      "379/379 [==============================] - 0s 131us/sample - loss: 23.8523\n",
      "Epoch 36/200\n",
      "379/379 [==============================] - 0s 138us/sample - loss: 24.3466\n",
      "Epoch 37/200\n",
      "379/379 [==============================] - 0s 132us/sample - loss: 23.9268\n",
      "Epoch 38/200\n",
      "379/379 [==============================] - 0s 135us/sample - loss: 24.3312\n",
      "Epoch 39/200\n",
      "379/379 [==============================] - 0s 132us/sample - loss: 21.7236\n",
      "Epoch 40/200\n",
      "379/379 [==============================] - 0s 138us/sample - loss: 21.3380\n",
      "Epoch 41/200\n",
      "379/379 [==============================] - 0s 135us/sample - loss: 22.7161\n",
      "Epoch 42/200\n",
      "379/379 [==============================] - 0s 134us/sample - loss: 22.3900\n",
      "Epoch 43/200\n",
      "379/379 [==============================] - 0s 133us/sample - loss: 22.4932\n",
      "Epoch 44/200\n",
      "379/379 [==============================] - 0s 131us/sample - loss: 20.9492\n",
      "Epoch 45/200\n",
      "379/379 [==============================] - 0s 140us/sample - loss: 19.3204\n",
      "Epoch 46/200\n",
      "379/379 [==============================] - 0s 132us/sample - loss: 19.7295\n",
      "Epoch 47/200\n",
      "379/379 [==============================] - 0s 160us/sample - loss: 19.7928\n",
      "Epoch 48/200\n",
      "379/379 [==============================] - 0s 151us/sample - loss: 23.9315\n",
      "Epoch 49/200\n",
      "379/379 [==============================] - 0s 145us/sample - loss: 21.5342\n",
      "Epoch 50/200\n",
      "379/379 [==============================] - 0s 134us/sample - loss: 18.8355\n",
      "Epoch 51/200\n",
      "379/379 [==============================] - 0s 135us/sample - loss: 22.7271\n",
      "Epoch 52/200\n",
      "379/379 [==============================] - 0s 131us/sample - loss: 18.5891\n",
      "Epoch 53/200\n",
      "379/379 [==============================] - 0s 135us/sample - loss: 17.8963\n",
      "Epoch 54/200\n",
      "379/379 [==============================] - 0s 145us/sample - loss: 17.8642\n",
      "Epoch 55/200\n",
      "379/379 [==============================] - 0s 164us/sample - loss: 19.5542\n",
      "Epoch 56/200\n",
      "379/379 [==============================] - 0s 134us/sample - loss: 25.5392\n",
      "Epoch 57/200\n",
      "379/379 [==============================] - 0s 135us/sample - loss: 19.3595\n",
      "Epoch 58/200\n",
      "379/379 [==============================] - 0s 134us/sample - loss: 16.8069\n",
      "Epoch 59/200\n",
      "379/379 [==============================] - 0s 160us/sample - loss: 19.2485\n",
      "Epoch 60/200\n",
      "379/379 [==============================] - 0s 132us/sample - loss: 19.8542\n",
      "Epoch 61/200\n",
      "379/379 [==============================] - 0s 134us/sample - loss: 17.4860\n",
      "Epoch 62/200\n",
      "379/379 [==============================] - 0s 160us/sample - loss: 16.2780\n",
      "Epoch 63/200\n",
      "379/379 [==============================] - 0s 134us/sample - loss: 18.1251\n",
      "Epoch 64/200\n",
      "379/379 [==============================] - 0s 135us/sample - loss: 18.9072\n",
      "Epoch 65/200\n",
      "379/379 [==============================] - 0s 133us/sample - loss: 17.7086\n",
      "Epoch 66/200\n",
      "379/379 [==============================] - 0s 135us/sample - loss: 18.6521\n",
      "Epoch 67/200\n",
      "379/379 [==============================] - 0s 134us/sample - loss: 17.2187\n",
      "Epoch 68/200\n",
      "379/379 [==============================] - 0s 138us/sample - loss: 21.9989\n",
      "Epoch 69/200\n",
      "379/379 [==============================] - 0s 129us/sample - loss: 18.6075\n",
      "Epoch 70/200\n",
      "379/379 [==============================] - 0s 134us/sample - loss: 21.4347\n",
      "Epoch 71/200\n",
      "379/379 [==============================] - 0s 143us/sample - loss: 18.7908\n",
      "Epoch 72/200\n",
      "379/379 [==============================] - 0s 138us/sample - loss: 17.8366\n",
      "Epoch 73/200\n",
      "379/379 [==============================] - 0s 149us/sample - loss: 17.0508\n",
      "Epoch 74/200\n",
      "379/379 [==============================] - 0s 163us/sample - loss: 16.4738\n",
      "Epoch 75/200\n",
      "379/379 [==============================] - 0s 145us/sample - loss: 16.8890\n",
      "Epoch 76/200\n",
      "379/379 [==============================] - 0s 152us/sample - loss: 15.8356\n",
      "Epoch 77/200\n",
      "379/379 [==============================] - 0s 174us/sample - loss: 16.0377\n",
      "Epoch 78/200\n",
      "379/379 [==============================] - 0s 151us/sample - loss: 17.3561\n",
      "Epoch 79/200\n",
      "379/379 [==============================] - 0s 153us/sample - loss: 16.7064\n",
      "Epoch 80/200\n",
      "379/379 [==============================] - 0s 147us/sample - loss: 18.3763\n",
      "Epoch 81/200\n",
      "379/379 [==============================] - 0s 153us/sample - loss: 17.9704\n",
      "Epoch 82/200\n",
      "379/379 [==============================] - 0s 156us/sample - loss: 16.4096\n",
      "Epoch 83/200\n",
      "379/379 [==============================] - 0s 153us/sample - loss: 16.9573\n",
      "Epoch 84/200\n",
      "379/379 [==============================] - 0s 151us/sample - loss: 18.3728\n",
      "Epoch 85/200\n",
      "379/379 [==============================] - 0s 146us/sample - loss: 18.1047\n",
      "Epoch 86/200\n",
      "379/379 [==============================] - 0s 166us/sample - loss: 15.7528\n",
      "Epoch 87/200\n",
      "379/379 [==============================] - 0s 179us/sample - loss: 16.1805\n",
      "Epoch 88/200\n",
      "379/379 [==============================] - 0s 150us/sample - loss: 15.1876\n",
      "Epoch 89/200\n",
      "379/379 [==============================] - 0s 155us/sample - loss: 14.8068\n",
      "Epoch 90/200\n",
      "379/379 [==============================] - 0s 155us/sample - loss: 16.2034\n",
      "Epoch 91/200\n",
      "379/379 [==============================] - 0s 151us/sample - loss: 16.4550\n",
      "Epoch 92/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "379/379 [==============================] - 0s 154us/sample - loss: 16.6275\n",
      "Epoch 93/200\n",
      "379/379 [==============================] - 0s 155us/sample - loss: 15.6667\n",
      "Epoch 94/200\n",
      "379/379 [==============================] - 0s 153us/sample - loss: 16.0707\n",
      "Epoch 95/200\n",
      "379/379 [==============================] - 0s 172us/sample - loss: 15.3920\n",
      "Epoch 96/200\n",
      "379/379 [==============================] - 0s 187us/sample - loss: 14.8460\n",
      "Epoch 97/200\n",
      "379/379 [==============================] - 0s 155us/sample - loss: 15.9476\n",
      "Epoch 98/200\n",
      "379/379 [==============================] - 0s 150us/sample - loss: 13.8817\n",
      "Epoch 99/200\n",
      "379/379 [==============================] - 0s 151us/sample - loss: 15.8586\n",
      "Epoch 100/200\n",
      "379/379 [==============================] - 0s 150us/sample - loss: 15.5200\n",
      "Epoch 101/200\n",
      "379/379 [==============================] - 0s 154us/sample - loss: 15.7538\n",
      "Epoch 102/200\n",
      "379/379 [==============================] - 0s 154us/sample - loss: 16.1891\n",
      "Epoch 103/200\n",
      "379/379 [==============================] - 0s 153us/sample - loss: 15.5009\n",
      "Epoch 104/200\n",
      "379/379 [==============================] - 0s 152us/sample - loss: 15.0531\n",
      "Epoch 105/200\n",
      "379/379 [==============================] - 0s 147us/sample - loss: 17.8690\n",
      "Epoch 106/200\n",
      "379/379 [==============================] - 0s 152us/sample - loss: 13.7782\n",
      "Epoch 107/200\n",
      "379/379 [==============================] - 0s 152us/sample - loss: 13.2203\n",
      "Epoch 108/200\n",
      "379/379 [==============================] - 0s 149us/sample - loss: 15.1790\n",
      "Epoch 109/200\n",
      "379/379 [==============================] - 0s 154us/sample - loss: 15.5522\n",
      "Epoch 110/200\n",
      "379/379 [==============================] - 0s 154us/sample - loss: 14.7382\n",
      "Epoch 111/200\n",
      "379/379 [==============================] - 0s 153us/sample - loss: 15.1978\n",
      "Epoch 112/200\n",
      "379/379 [==============================] - 0s 162us/sample - loss: 18.0285\n",
      "Epoch 113/200\n",
      "379/379 [==============================] - 0s 131us/sample - loss: 15.1950\n",
      "Epoch 114/200\n",
      "379/379 [==============================] - 0s 135us/sample - loss: 17.2824\n",
      "Epoch 115/200\n",
      "379/379 [==============================] - 0s 131us/sample - loss: 14.1602\n",
      "Epoch 116/200\n",
      "379/379 [==============================] - 0s 136us/sample - loss: 14.2881\n",
      "Epoch 117/200\n",
      "379/379 [==============================] - 0s 137us/sample - loss: 15.1340\n",
      "Epoch 118/200\n",
      "379/379 [==============================] - 0s 134us/sample - loss: 15.5420\n",
      "Epoch 119/200\n",
      "379/379 [==============================] - 0s 138us/sample - loss: 15.3975\n",
      "Epoch 120/200\n",
      "379/379 [==============================] - 0s 133us/sample - loss: 13.4661\n",
      "Epoch 121/200\n",
      "379/379 [==============================] - 0s 134us/sample - loss: 14.4621\n",
      "Epoch 122/200\n",
      "379/379 [==============================] - 0s 132us/sample - loss: 14.0202\n",
      "Epoch 123/200\n",
      "379/379 [==============================] - 0s 136us/sample - loss: 15.3189\n",
      "Epoch 124/200\n",
      "379/379 [==============================] - 0s 140us/sample - loss: 14.5381\n",
      "Epoch 125/200\n",
      "379/379 [==============================] - 0s 136us/sample - loss: 15.6275\n",
      "Epoch 126/200\n",
      "379/379 [==============================] - 0s 137us/sample - loss: 16.3049\n",
      "Epoch 127/200\n",
      "379/379 [==============================] - 0s 138us/sample - loss: 16.3060\n",
      "Epoch 128/200\n",
      "379/379 [==============================] - 0s 138us/sample - loss: 15.2080\n",
      "Epoch 129/200\n",
      "379/379 [==============================] - 0s 133us/sample - loss: 14.9628\n",
      "Epoch 130/200\n",
      "379/379 [==============================] - 0s 145us/sample - loss: 13.8313\n",
      "Epoch 131/200\n",
      "379/379 [==============================] - 0s 137us/sample - loss: 15.9793\n",
      "Epoch 132/200\n",
      "379/379 [==============================] - 0s 135us/sample - loss: 15.4001\n",
      "Epoch 133/200\n",
      "379/379 [==============================] - 0s 137us/sample - loss: 15.1422\n",
      "Epoch 134/200\n",
      "379/379 [==============================] - 0s 134us/sample - loss: 13.8691\n",
      "Epoch 135/200\n",
      "379/379 [==============================] - 0s 134us/sample - loss: 14.0020\n",
      "Epoch 136/200\n",
      "379/379 [==============================] - 0s 127us/sample - loss: 14.1088\n",
      "Epoch 137/200\n",
      "379/379 [==============================] - 0s 138us/sample - loss: 13.2790\n",
      "Epoch 138/200\n",
      "379/379 [==============================] - 0s 131us/sample - loss: 17.8052\n",
      "Epoch 139/200\n",
      "379/379 [==============================] - 0s 137us/sample - loss: 13.5498\n",
      "Epoch 140/200\n",
      "379/379 [==============================] - 0s 134us/sample - loss: 16.0801\n",
      "Epoch 141/200\n",
      "379/379 [==============================] - 0s 132us/sample - loss: 16.2568\n",
      "Epoch 142/200\n",
      "379/379 [==============================] - 0s 139us/sample - loss: 16.3487\n",
      "Epoch 143/200\n",
      "379/379 [==============================] - 0s 134us/sample - loss: 13.8588\n",
      "Epoch 144/200\n",
      "379/379 [==============================] - 0s 146us/sample - loss: 13.7540\n",
      "Epoch 145/200\n",
      "379/379 [==============================] - 0s 142us/sample - loss: 15.4464\n",
      "Epoch 146/200\n",
      "379/379 [==============================] - 0s 134us/sample - loss: 14.3727\n",
      "Epoch 147/200\n",
      "379/379 [==============================] - 0s 138us/sample - loss: 14.1652\n",
      "Epoch 148/200\n",
      "379/379 [==============================] - 0s 137us/sample - loss: 13.6846\n",
      "Epoch 149/200\n",
      "379/379 [==============================] - 0s 143us/sample - loss: 13.1516\n",
      "Epoch 150/200\n",
      "379/379 [==============================] - 0s 134us/sample - loss: 13.7099\n",
      "Epoch 151/200\n",
      "379/379 [==============================] - 0s 132us/sample - loss: 12.9167\n",
      "Epoch 152/200\n",
      "379/379 [==============================] - 0s 133us/sample - loss: 15.6322\n",
      "Epoch 153/200\n",
      "379/379 [==============================] - 0s 134us/sample - loss: 14.1281\n",
      "Epoch 154/200\n",
      "379/379 [==============================] - 0s 138us/sample - loss: 12.9482\n",
      "Epoch 155/200\n",
      "379/379 [==============================] - 0s 135us/sample - loss: 12.8790\n",
      "Epoch 156/200\n",
      "379/379 [==============================] - 0s 135us/sample - loss: 14.4481\n",
      "Epoch 157/200\n",
      "379/379 [==============================] - 0s 134us/sample - loss: 16.0168\n",
      "Epoch 158/200\n",
      "379/379 [==============================] - 0s 134us/sample - loss: 14.7891\n",
      "Epoch 159/200\n",
      "379/379 [==============================] - 0s 134us/sample - loss: 12.7222\n",
      "Epoch 160/200\n",
      "379/379 [==============================] - 0s 136us/sample - loss: 13.8306\n",
      "Epoch 161/200\n",
      "379/379 [==============================] - 0s 131us/sample - loss: 14.1926\n",
      "Epoch 162/200\n",
      "379/379 [==============================] - 0s 137us/sample - loss: 12.7708\n",
      "Epoch 163/200\n",
      "379/379 [==============================] - 0s 132us/sample - loss: 12.9198\n",
      "Epoch 164/200\n",
      "379/379 [==============================] - 0s 133us/sample - loss: 15.0383\n",
      "Epoch 165/200\n",
      "379/379 [==============================] - 0s 134us/sample - loss: 12.5525\n",
      "Epoch 166/200\n",
      "379/379 [==============================] - 0s 136us/sample - loss: 14.1379\n",
      "Epoch 167/200\n",
      "379/379 [==============================] - 0s 136us/sample - loss: 14.7535\n",
      "Epoch 168/200\n",
      "379/379 [==============================] - 0s 134us/sample - loss: 12.8110\n",
      "Epoch 169/200\n",
      "379/379 [==============================] - 0s 154us/sample - loss: 12.9048\n",
      "Epoch 170/200\n",
      "379/379 [==============================] - 0s 151us/sample - loss: 13.3962\n",
      "Epoch 171/200\n",
      "379/379 [==============================] - 0s 135us/sample - loss: 16.5707\n",
      "Epoch 172/200\n",
      "379/379 [==============================] - 0s 135us/sample - loss: 12.8652\n",
      "Epoch 173/200\n",
      "379/379 [==============================] - 0s 149us/sample - loss: 20.6698\n",
      "Epoch 174/200\n",
      "379/379 [==============================] - 0s 188us/sample - loss: 13.1801\n",
      "Epoch 175/200\n",
      "379/379 [==============================] - 0s 151us/sample - loss: 14.3760\n",
      "Epoch 176/200\n",
      "379/379 [==============================] - 0s 136us/sample - loss: 12.6005\n",
      "Epoch 177/200\n",
      "379/379 [==============================] - 0s 138us/sample - loss: 13.8401\n",
      "Epoch 178/200\n",
      "379/379 [==============================] - 0s 136us/sample - loss: 13.2769\n",
      "Epoch 179/200\n",
      "379/379 [==============================] - 0s 135us/sample - loss: 12.4803\n",
      "Epoch 180/200\n",
      "379/379 [==============================] - 0s 131us/sample - loss: 13.2021\n",
      "Epoch 181/200\n",
      "379/379 [==============================] - 0s 135us/sample - loss: 13.6995\n",
      "Epoch 182/200\n",
      "379/379 [==============================] - 0s 133us/sample - loss: 13.7530\n",
      "Epoch 183/200\n",
      "379/379 [==============================] - 0s 132us/sample - loss: 17.8370\n",
      "Epoch 184/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "379/379 [==============================] - 0s 133us/sample - loss: 13.7139\n",
      "Epoch 185/200\n",
      "379/379 [==============================] - 0s 135us/sample - loss: 15.0660\n",
      "Epoch 186/200\n",
      "379/379 [==============================] - 0s 138us/sample - loss: 14.2623\n",
      "Epoch 187/200\n",
      "379/379 [==============================] - 0s 135us/sample - loss: 13.1458\n",
      "Epoch 188/200\n",
      "379/379 [==============================] - 0s 137us/sample - loss: 12.4998\n",
      "Epoch 189/200\n",
      "379/379 [==============================] - 0s 130us/sample - loss: 12.6583\n",
      "Epoch 190/200\n",
      "379/379 [==============================] - 0s 160us/sample - loss: 13.5198\n",
      "Epoch 191/200\n",
      "379/379 [==============================] - 0s 136us/sample - loss: 14.2853\n",
      "Epoch 192/200\n",
      "379/379 [==============================] - 0s 133us/sample - loss: 12.8164\n",
      "Epoch 193/200\n",
      "379/379 [==============================] - 0s 136us/sample - loss: 13.9454\n",
      "Epoch 194/200\n",
      "379/379 [==============================] - 0s 132us/sample - loss: 13.4730\n",
      "Epoch 195/200\n",
      "379/379 [==============================] - 0s 133us/sample - loss: 12.4952\n",
      "Epoch 196/200\n",
      "379/379 [==============================] - 0s 136us/sample - loss: 12.3341\n",
      "Epoch 197/200\n",
      "379/379 [==============================] - 0s 133us/sample - loss: 13.6102\n",
      "Epoch 198/200\n",
      "379/379 [==============================] - 0s 135us/sample - loss: 12.9436\n",
      "Epoch 199/200\n",
      "379/379 [==============================] - 0s 133us/sample - loss: 15.6596\n",
      "Epoch 200/200\n",
      "379/379 [==============================] - 0s 136us/sample - loss: 13.1788\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mse', optimizer='adam')\n",
    "history = model.fit(X_train, y_train, epochs=200, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18647cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(X_test, y_test, 'bo')\n",
    "plt.plot(X_test, model.predict(X_test), 'p', color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1652357e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[19.128437 ],\n",
       "       [22.65895  ],\n",
       "       [15.186125 ],\n",
       "       [35.656967 ],\n",
       "       [13.267837 ],\n",
       "       [40.564014 ],\n",
       "       [29.516745 ],\n",
       "       [26.258827 ],\n",
       "       [11.095696 ],\n",
       "       [13.182737 ],\n",
       "       [27.74762  ],\n",
       "       [18.442163 ],\n",
       "       [23.252209 ],\n",
       "       [13.188937 ],\n",
       "       [29.003078 ],\n",
       "       [21.296444 ],\n",
       "       [23.95825  ],\n",
       "       [21.246172 ],\n",
       "       [32.857033 ],\n",
       "       [51.607258 ],\n",
       "       [13.8609915],\n",
       "       [18.226892 ],\n",
       "       [27.602518 ],\n",
       "       [12.992757 ],\n",
       "       [13.575849 ],\n",
       "       [18.224806 ],\n",
       "       [19.238377 ],\n",
       "       [30.962639 ],\n",
       "       [14.402917 ],\n",
       "       [21.354853 ],\n",
       "       [20.09421  ],\n",
       "       [24.252518 ],\n",
       "       [34.676514 ],\n",
       "       [16.222832 ],\n",
       "       [35.817383 ],\n",
       "       [19.981577 ],\n",
       "       [12.296576 ],\n",
       "       [20.54872  ],\n",
       "       [20.681921 ],\n",
       "       [10.350799 ],\n",
       "       [22.836464 ],\n",
       "       [27.97981  ],\n",
       "       [23.594265 ],\n",
       "       [ 9.905128 ],\n",
       "       [26.808847 ],\n",
       "       [13.289961 ],\n",
       "       [37.965397 ],\n",
       "       [20.382265 ],\n",
       "       [15.889842 ],\n",
       "       [16.43347  ],\n",
       "       [22.82652  ],\n",
       "       [ 8.963455 ],\n",
       "       [31.7982   ],\n",
       "       [18.413921 ],\n",
       "       [ 8.96829  ],\n",
       "       [20.896729 ],\n",
       "       [23.894827 ],\n",
       "       [19.827034 ],\n",
       "       [23.231703 ],\n",
       "       [23.815603 ],\n",
       "       [19.150053 ],\n",
       "       [45.724888 ],\n",
       "       [19.448267 ],\n",
       "       [34.05695  ],\n",
       "       [27.2364   ],\n",
       "       [24.52735  ],\n",
       "       [20.217516 ],\n",
       "       [18.816742 ],\n",
       "       [16.821032 ],\n",
       "       [14.933989 ],\n",
       "       [23.80397  ],\n",
       "       [27.500603 ],\n",
       "       [25.869696 ],\n",
       "       [23.633188 ],\n",
       "       [22.758682 ],\n",
       "       [23.002985 ],\n",
       "       [16.140427 ],\n",
       "       [33.855453 ],\n",
       "       [19.435022 ],\n",
       "       [40.194523 ],\n",
       "       [10.931772 ],\n",
       "       [24.30752  ],\n",
       "       [24.289637 ],\n",
       "       [29.523157 ],\n",
       "       [24.765266 ],\n",
       "       [22.98013  ],\n",
       "       [31.422075 ],\n",
       "       [25.799133 ],\n",
       "       [16.357061 ],\n",
       "       [32.844513 ],\n",
       "       [12.945968 ],\n",
       "       [22.171986 ],\n",
       "       [21.515965 ],\n",
       "       [18.670593 ],\n",
       "       [23.533018 ],\n",
       "       [19.597189 ],\n",
       "       [18.858854 ],\n",
       "       [33.737328 ],\n",
       "       [13.045842 ],\n",
       "       [12.2145815],\n",
       "       [19.614697 ],\n",
       "       [11.506679 ],\n",
       "       [22.581736 ],\n",
       "       [22.927313 ],\n",
       "       [11.547664 ],\n",
       "       [23.839933 ],\n",
       "       [20.515003 ],\n",
       "       [19.250446 ],\n",
       "       [17.392263 ],\n",
       "       [24.261868 ],\n",
       "       [28.052172 ],\n",
       "       [28.976305 ],\n",
       "       [12.156594 ],\n",
       "       [26.363014 ],\n",
       "       [14.108503 ],\n",
       "       [19.829002 ],\n",
       "       [19.97283  ],\n",
       "       [23.310486 ],\n",
       "       [13.839372 ],\n",
       "       [ 9.945166 ],\n",
       "       [20.958424 ],\n",
       "       [10.807125 ],\n",
       "       [40.80303  ],\n",
       "       [21.289862 ],\n",
       "       [24.495707 ],\n",
       "       [34.271385 ],\n",
       "       [14.939883 ]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a7d4139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "4.882563775996931\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "pred = model.predict(X_test)\n",
    "print(np.sqrt(mean_squared_error(y_test, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c2ca07",
   "metadata": {},
   "source": [
    "# 딥러닝 이진분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "377eb440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pregnant</th>\n",
       "      <th>plasma</th>\n",
       "      <th>pressure</th>\n",
       "      <th>thickness</th>\n",
       "      <th>insulin</th>\n",
       "      <th>bmi</th>\n",
       "      <th>pedigree</th>\n",
       "      <th>age</th>\n",
       "      <th>diabetes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pregnant  plasma  pressure  thickness  insulin   bmi  pedigree  age  \\\n",
       "0         6     148        72         35        0  33.6     0.627   50   \n",
       "1         1      85        66         29        0  26.6     0.351   31   \n",
       "2         8     183        64          0        0  23.3     0.672   32   \n",
       "3         1      89        66         23       94  28.1     0.167   21   \n",
       "4         0     137        40         35      168  43.1     2.288   33   \n",
       "\n",
       "   diabetes  \n",
       "0         1  \n",
       "1         0  \n",
       "2         1  \n",
       "3         0  \n",
       "4         1  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(path+'pima-indians-diabetes.csv', header=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f9a42c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pregnant</th>\n",
       "      <th>plasma</th>\n",
       "      <th>pressure</th>\n",
       "      <th>thickness</th>\n",
       "      <th>insulin</th>\n",
       "      <th>bmi</th>\n",
       "      <th>pedigree</th>\n",
       "      <th>age</th>\n",
       "      <th>diabetes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>11</td>\n",
       "      <td>127</td>\n",
       "      <td>106</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.190</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>5</td>\n",
       "      <td>96</td>\n",
       "      <td>74</td>\n",
       "      <td>18</td>\n",
       "      <td>67</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.997</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>4</td>\n",
       "      <td>184</td>\n",
       "      <td>78</td>\n",
       "      <td>39</td>\n",
       "      <td>277</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.264</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>1</td>\n",
       "      <td>139</td>\n",
       "      <td>46</td>\n",
       "      <td>19</td>\n",
       "      <td>83</td>\n",
       "      <td>28.7</td>\n",
       "      <td>0.654</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1</td>\n",
       "      <td>146</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29.7</td>\n",
       "      <td>0.564</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     pregnant  plasma  pressure  thickness  insulin   bmi  pedigree  age  \\\n",
       "658        11     127       106          0        0  39.0     0.190   51   \n",
       "265         5      96        74         18       67  33.6     0.997   43   \n",
       "425         4     184        78         39      277  37.0     0.264   31   \n",
       "346         1     139        46         19       83  28.7     0.654   22   \n",
       "46          1     146        56          0        0  29.7     0.564   29   \n",
       "\n",
       "     diabetes  \n",
       "658         0  \n",
       "265         0  \n",
       "425         1  \n",
       "346         0  \n",
       "46          0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "974df270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 768 entries, 658 to 5\n",
      "Data columns (total 9 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   pregnant   768 non-null    int64  \n",
      " 1   plasma     768 non-null    int64  \n",
      " 2   pressure   768 non-null    int64  \n",
      " 3   thickness  768 non-null    int64  \n",
      " 4   insulin    768 non-null    int64  \n",
      " 5   bmi        768 non-null    float64\n",
      " 6   pedigree   768 non-null    float64\n",
      " 7   age        768 non-null    int64  \n",
      " 8   diabetes   768 non-null    int64  \n",
      "dtypes: float64(2), int64(7)\n",
      "memory usage: 60.0 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3085f63e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pregnant</th>\n",
       "      <th>plasma</th>\n",
       "      <th>pressure</th>\n",
       "      <th>thickness</th>\n",
       "      <th>insulin</th>\n",
       "      <th>bmi</th>\n",
       "      <th>pedigree</th>\n",
       "      <th>age</th>\n",
       "      <th>diabetes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.845052</td>\n",
       "      <td>120.894531</td>\n",
       "      <td>69.105469</td>\n",
       "      <td>20.536458</td>\n",
       "      <td>79.799479</td>\n",
       "      <td>31.992578</td>\n",
       "      <td>0.471876</td>\n",
       "      <td>33.240885</td>\n",
       "      <td>0.348958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.369578</td>\n",
       "      <td>31.972618</td>\n",
       "      <td>19.355807</td>\n",
       "      <td>15.952218</td>\n",
       "      <td>115.244002</td>\n",
       "      <td>7.884160</td>\n",
       "      <td>0.331329</td>\n",
       "      <td>11.760232</td>\n",
       "      <td>0.476951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.300000</td>\n",
       "      <td>0.243750</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.372500</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>140.250000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>127.250000</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>0.626250</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>846.000000</td>\n",
       "      <td>67.100000</td>\n",
       "      <td>2.420000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         pregnant      plasma    pressure   thickness     insulin         bmi  \\\n",
       "count  768.000000  768.000000  768.000000  768.000000  768.000000  768.000000   \n",
       "mean     3.845052  120.894531   69.105469   20.536458   79.799479   31.992578   \n",
       "std      3.369578   31.972618   19.355807   15.952218  115.244002    7.884160   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      1.000000   99.000000   62.000000    0.000000    0.000000   27.300000   \n",
       "50%      3.000000  117.000000   72.000000   23.000000   30.500000   32.000000   \n",
       "75%      6.000000  140.250000   80.000000   32.000000  127.250000   36.600000   \n",
       "max     17.000000  199.000000  122.000000   99.000000  846.000000   67.100000   \n",
       "\n",
       "         pedigree         age    diabetes  \n",
       "count  768.000000  768.000000  768.000000  \n",
       "mean     0.471876   33.240885    0.348958  \n",
       "std      0.331329   11.760232    0.476951  \n",
       "min      0.078000   21.000000    0.000000  \n",
       "25%      0.243750   24.000000    0.000000  \n",
       "50%      0.372500   29.000000    0.000000  \n",
       "75%      0.626250   41.000000    1.000000  \n",
       "max      2.420000   81.000000    1.000000  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42196466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.boxplot(x=[df[col] for col in df.columns], \n",
    "            labels=df.columns.to_list())\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# plt.boxplot(x=[df.crim], notch=True, showmeans=True, \n",
    "#            meanline=True, vert=True, labels=['범죄'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78f3726b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :8]\n",
    "y = df.iloc[:, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01aa8905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pregnant  plasma  pressure  thickness  insulin   bmi  pedigree  age\n",
      "112         1      89        76         34       37  31.2     0.192   23\n",
      "221         2     158        90          0        0  31.6     0.805   66\n",
      "548         1     164        82         43       67  32.8     0.341   50\n",
      "82          7      83        78         26       71  29.3     0.767   36\n",
      "306        10     161        68         23      132  25.5     0.326   47\n",
      "--------------------------------------------------\n",
      "     pregnant  plasma  pressure  thickness  insulin   bmi  pedigree  age\n",
      "124         0     113        76          0        0  33.3     0.278   23\n",
      "632         2     111        60          0        0  26.2     0.343   23\n",
      "36         11     138        76          0        0  33.2     0.420   35\n",
      "601         6      96         0          0        0  23.7     0.190   28\n",
      "519         6     129        90          7      326  19.6     0.582   60\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "print(X_train.head())\n",
    "print('-'*50)\n",
    "print(X_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd832248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(units=12, activation='relu', input_dim=8))\n",
    "model.add(tf.keras.layers.Dense(units=8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e38a761d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 12)                108       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 8)                 104       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 221\n",
      "Trainable params: 221\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "db798bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 576 samples\n",
      "Epoch 1/200\n",
      "576/576 [==============================] - 0s 858us/sample - loss: 9.8045 - acc: 0.3594\n",
      "Epoch 2/200\n",
      "576/576 [==============================] - 0s 163us/sample - loss: 9.6970 - acc: 0.3594\n",
      "Epoch 3/200\n",
      "576/576 [==============================] - 0s 163us/sample - loss: 2.6689 - acc: 0.5469\n",
      "Epoch 4/200\n",
      "576/576 [==============================] - 0s 159us/sample - loss: 1.2720 - acc: 0.5816\n",
      "Epoch 5/200\n",
      "576/576 [==============================] - 0s 160us/sample - loss: 0.9677 - acc: 0.6181\n",
      "Epoch 6/200\n",
      "576/576 [==============================] - 0s 160us/sample - loss: 0.8249 - acc: 0.6250\n",
      "Epoch 7/200\n",
      "576/576 [==============================] - 0s 181us/sample - loss: 0.7251 - acc: 0.6250\n",
      "Epoch 8/200\n",
      "576/576 [==============================] - 0s 163us/sample - loss: 0.7294 - acc: 0.6667\n",
      "Epoch 9/200\n",
      "576/576 [==============================] - 0s 174us/sample - loss: 0.7311 - acc: 0.6285\n",
      "Epoch 10/200\n",
      "576/576 [==============================] - 0s 160us/sample - loss: 0.6978 - acc: 0.6372\n",
      "Epoch 11/200\n",
      "576/576 [==============================] - 0s 160us/sample - loss: 0.6992 - acc: 0.6632\n",
      "Epoch 12/200\n",
      "576/576 [==============================] - 0s 165us/sample - loss: 0.6927 - acc: 0.6701\n",
      "Epoch 13/200\n",
      "576/576 [==============================] - 0s 161us/sample - loss: 0.6776 - acc: 0.6597\n",
      "Epoch 14/200\n",
      "576/576 [==============================] - 0s 160us/sample - loss: 0.6814 - acc: 0.6701\n",
      "Epoch 15/200\n",
      "576/576 [==============================] - 0s 162us/sample - loss: 0.6498 - acc: 0.6788\n",
      "Epoch 16/200\n",
      "576/576 [==============================] - 0s 161us/sample - loss: 0.6674 - acc: 0.6840\n",
      "Epoch 17/200\n",
      "576/576 [==============================] - 0s 162us/sample - loss: 0.7084 - acc: 0.6753\n",
      "Epoch 18/200\n",
      "576/576 [==============================] - 0s 162us/sample - loss: 0.6811 - acc: 0.6510\n",
      "Epoch 19/200\n",
      "576/576 [==============================] - 0s 165us/sample - loss: 0.6167 - acc: 0.7014\n",
      "Epoch 20/200\n",
      "576/576 [==============================] - 0s 163us/sample - loss: 0.6419 - acc: 0.6667\n",
      "Epoch 21/200\n",
      "576/576 [==============================] - 0s 159us/sample - loss: 0.6572 - acc: 0.6753\n",
      "Epoch 22/200\n",
      "576/576 [==============================] - 0s 184us/sample - loss: 0.6236 - acc: 0.6788\n",
      "Epoch 23/200\n",
      "576/576 [==============================] - 0s 178us/sample - loss: 0.6282 - acc: 0.7118\n",
      "Epoch 24/200\n",
      "576/576 [==============================] - 0s 177us/sample - loss: 0.7010 - acc: 0.6285\n",
      "Epoch 25/200\n",
      "576/576 [==============================] - 0s 164us/sample - loss: 0.6031 - acc: 0.6927\n",
      "Epoch 26/200\n",
      "576/576 [==============================] - 0s 170us/sample - loss: 0.6536 - acc: 0.6684\n",
      "Epoch 27/200\n",
      "576/576 [==============================] - 0s 168us/sample - loss: 0.6207 - acc: 0.6892\n",
      "Epoch 28/200\n",
      "576/576 [==============================] - 0s 171us/sample - loss: 0.5912 - acc: 0.7049\n",
      "Epoch 29/200\n",
      "576/576 [==============================] - 0s 168us/sample - loss: 0.5970 - acc: 0.7118\n",
      "Epoch 30/200\n",
      "576/576 [==============================] - 0s 193us/sample - loss: 0.6239 - acc: 0.6892\n",
      "Epoch 31/200\n",
      "576/576 [==============================] - 0s 178us/sample - loss: 0.6100 - acc: 0.6892\n",
      "Epoch 32/200\n",
      "576/576 [==============================] - 0s 179us/sample - loss: 0.5909 - acc: 0.7153\n",
      "Epoch 33/200\n",
      "576/576 [==============================] - 0s 179us/sample - loss: 0.6115 - acc: 0.6892\n",
      "Epoch 34/200\n",
      "576/576 [==============================] - 0s 179us/sample - loss: 0.5806 - acc: 0.7205\n",
      "Epoch 35/200\n",
      "576/576 [==============================] - 0s 177us/sample - loss: 0.5775 - acc: 0.7431\n",
      "Epoch 36/200\n",
      "576/576 [==============================] - 0s 181us/sample - loss: 0.6001 - acc: 0.7153\n",
      "Epoch 37/200\n",
      "576/576 [==============================] - 0s 164us/sample - loss: 0.5672 - acc: 0.7188\n",
      "Epoch 38/200\n",
      "576/576 [==============================] - 0s 168us/sample - loss: 0.5749 - acc: 0.7049\n",
      "Epoch 39/200\n",
      "576/576 [==============================] - 0s 166us/sample - loss: 0.5956 - acc: 0.6962\n",
      "Epoch 40/200\n",
      "576/576 [==============================] - 0s 159us/sample - loss: 0.5852 - acc: 0.7153\n",
      "Epoch 41/200\n",
      "576/576 [==============================] - 0s 164us/sample - loss: 0.5809 - acc: 0.7118\n",
      "Epoch 42/200\n",
      "576/576 [==============================] - 0s 163us/sample - loss: 0.5636 - acc: 0.7118\n",
      "Epoch 43/200\n",
      "576/576 [==============================] - 0s 164us/sample - loss: 0.5753 - acc: 0.7413\n",
      "Epoch 44/200\n",
      "576/576 [==============================] - 0s 159us/sample - loss: 0.5620 - acc: 0.7344\n",
      "Epoch 45/200\n",
      "576/576 [==============================] - 0s 187us/sample - loss: 0.5852 - acc: 0.7049\n",
      "Epoch 46/200\n",
      "576/576 [==============================] - 0s 170us/sample - loss: 0.5791 - acc: 0.6944\n",
      "Epoch 47/200\n",
      "576/576 [==============================] - 0s 166us/sample - loss: 0.5881 - acc: 0.7188\n",
      "Epoch 48/200\n",
      "576/576 [==============================] - 0s 186us/sample - loss: 0.5872 - acc: 0.7066\n",
      "Epoch 49/200\n",
      "576/576 [==============================] - 0s 166us/sample - loss: 0.5945 - acc: 0.7101\n",
      "Epoch 50/200\n",
      "576/576 [==============================] - 0s 177us/sample - loss: 0.5433 - acc: 0.7274\n",
      "Epoch 51/200\n",
      "576/576 [==============================] - 0s 164us/sample - loss: 0.5323 - acc: 0.7465\n",
      "Epoch 52/200\n",
      "576/576 [==============================] - 0s 161us/sample - loss: 0.5374 - acc: 0.7326\n",
      "Epoch 53/200\n",
      "576/576 [==============================] - 0s 166us/sample - loss: 0.5539 - acc: 0.7326\n",
      "Epoch 54/200\n",
      "576/576 [==============================] - 0s 166us/sample - loss: 0.6146 - acc: 0.7049\n",
      "Epoch 55/200\n",
      "576/576 [==============================] - 0s 163us/sample - loss: 0.6113 - acc: 0.6927\n",
      "Epoch 56/200\n",
      "576/576 [==============================] - 0s 159us/sample - loss: 0.5777 - acc: 0.7153\n",
      "Epoch 57/200\n",
      "576/576 [==============================] - 0s 160us/sample - loss: 0.5664 - acc: 0.7292\n",
      "Epoch 58/200\n",
      "576/576 [==============================] - 0s 175us/sample - loss: 0.5351 - acc: 0.7101\n",
      "Epoch 59/200\n",
      "576/576 [==============================] - 0s 160us/sample - loss: 0.5607 - acc: 0.7378\n",
      "Epoch 60/200\n",
      "576/576 [==============================] - 0s 164us/sample - loss: 0.5439 - acc: 0.7257\n",
      "Epoch 61/200\n",
      "576/576 [==============================] - 0s 167us/sample - loss: 0.5509 - acc: 0.7257\n",
      "Epoch 62/200\n",
      "576/576 [==============================] - 0s 158us/sample - loss: 0.5566 - acc: 0.7222\n",
      "Epoch 63/200\n",
      "576/576 [==============================] - 0s 159us/sample - loss: 0.5471 - acc: 0.7326\n",
      "Epoch 64/200\n",
      "576/576 [==============================] - 0s 156us/sample - loss: 0.5259 - acc: 0.7500\n",
      "Epoch 65/200\n",
      "576/576 [==============================] - 0s 161us/sample - loss: 0.5410 - acc: 0.7413\n",
      "Epoch 66/200\n",
      "576/576 [==============================] - 0s 163us/sample - loss: 0.5574 - acc: 0.7326\n",
      "Epoch 67/200\n",
      "576/576 [==============================] - 0s 179us/sample - loss: 0.5281 - acc: 0.7708\n",
      "Epoch 68/200\n",
      "576/576 [==============================] - 0s 160us/sample - loss: 0.5343 - acc: 0.7569\n",
      "Epoch 69/200\n",
      "576/576 [==============================] - 0s 160us/sample - loss: 0.5328 - acc: 0.7431\n",
      "Epoch 70/200\n",
      "576/576 [==============================] - 0s 160us/sample - loss: 0.5319 - acc: 0.7344\n",
      "Epoch 71/200\n",
      "576/576 [==============================] - 0s 158us/sample - loss: 0.5674 - acc: 0.7292\n",
      "Epoch 72/200\n",
      "576/576 [==============================] - 0s 162us/sample - loss: 0.5373 - acc: 0.7413\n",
      "Epoch 73/200\n",
      "576/576 [==============================] - 0s 158us/sample - loss: 0.5179 - acc: 0.7674\n",
      "Epoch 74/200\n",
      "576/576 [==============================] - 0s 186us/sample - loss: 0.5343 - acc: 0.7240\n",
      "Epoch 75/200\n",
      "576/576 [==============================] - 0s 166us/sample - loss: 0.5297 - acc: 0.7222\n",
      "Epoch 76/200\n",
      "576/576 [==============================] - 0s 160us/sample - loss: 0.5548 - acc: 0.7205\n",
      "Epoch 77/200\n",
      "576/576 [==============================] - 0s 159us/sample - loss: 0.5342 - acc: 0.7552\n",
      "Epoch 78/200\n",
      "576/576 [==============================] - 0s 156us/sample - loss: 0.5266 - acc: 0.7170\n",
      "Epoch 79/200\n",
      "576/576 [==============================] - 0s 157us/sample - loss: 0.5732 - acc: 0.7326\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 157us/sample - loss: 0.5345 - acc: 0.7622\n",
      "Epoch 81/200\n",
      "576/576 [==============================] - 0s 157us/sample - loss: 0.5183 - acc: 0.7378\n",
      "Epoch 82/200\n",
      "576/576 [==============================] - 0s 156us/sample - loss: 0.5146 - acc: 0.7500\n",
      "Epoch 83/200\n",
      "576/576 [==============================] - 0s 160us/sample - loss: 0.5493 - acc: 0.7188\n",
      "Epoch 84/200\n",
      "576/576 [==============================] - 0s 155us/sample - loss: 0.5413 - acc: 0.7257\n",
      "Epoch 85/200\n",
      "576/576 [==============================] - 0s 156us/sample - loss: 0.5460 - acc: 0.7517\n",
      "Epoch 86/200\n",
      "576/576 [==============================] - 0s 159us/sample - loss: 0.5244 - acc: 0.7378\n",
      "Epoch 87/200\n",
      "576/576 [==============================] - 0s 155us/sample - loss: 0.5248 - acc: 0.7257\n",
      "Epoch 88/200\n",
      "576/576 [==============================] - 0s 155us/sample - loss: 0.5292 - acc: 0.7552\n",
      "Epoch 89/200\n",
      "576/576 [==============================] - 0s 162us/sample - loss: 0.5527 - acc: 0.7448\n",
      "Epoch 90/200\n",
      "576/576 [==============================] - 0s 180us/sample - loss: 0.5212 - acc: 0.7535\n",
      "Epoch 91/200\n",
      "576/576 [==============================] - 0s 164us/sample - loss: 0.5216 - acc: 0.7465\n",
      "Epoch 92/200\n",
      "576/576 [==============================] - 0s 157us/sample - loss: 0.5101 - acc: 0.7535\n",
      "Epoch 93/200\n",
      "576/576 [==============================] - 0s 159us/sample - loss: 0.5122 - acc: 0.7483\n",
      "Epoch 94/200\n",
      "576/576 [==============================] - 0s 155us/sample - loss: 0.5435 - acc: 0.7361\n",
      "Epoch 95/200\n",
      "576/576 [==============================] - 0s 155us/sample - loss: 0.5158 - acc: 0.7760\n",
      "Epoch 96/200\n",
      "576/576 [==============================] - 0s 160us/sample - loss: 0.5276 - acc: 0.7361\n",
      "Epoch 97/200\n",
      "576/576 [==============================] - 0s 158us/sample - loss: 0.5761 - acc: 0.6962\n",
      "Epoch 98/200\n",
      "576/576 [==============================] - 0s 159us/sample - loss: 0.5723 - acc: 0.7413\n",
      "Epoch 99/200\n",
      "576/576 [==============================] - 0s 165us/sample - loss: 0.5571 - acc: 0.7413\n",
      "Epoch 100/200\n",
      "576/576 [==============================] - 0s 158us/sample - loss: 0.5398 - acc: 0.7240\n",
      "Epoch 101/200\n",
      "576/576 [==============================] - 0s 155us/sample - loss: 0.5140 - acc: 0.7517\n",
      "Epoch 102/200\n",
      "576/576 [==============================] - 0s 199us/sample - loss: 0.5057 - acc: 0.7517\n",
      "Epoch 103/200\n",
      "576/576 [==============================] - 0s 178us/sample - loss: 0.5375 - acc: 0.7483\n",
      "Epoch 104/200\n",
      "576/576 [==============================] - 0s 175us/sample - loss: 0.5055 - acc: 0.7656\n",
      "Epoch 105/200\n",
      "576/576 [==============================] - 0s 176us/sample - loss: 0.5218 - acc: 0.7500\n",
      "Epoch 106/200\n",
      "576/576 [==============================] - 0s 160us/sample - loss: 0.5302 - acc: 0.7378\n",
      "Epoch 107/200\n",
      "576/576 [==============================] - 0s 165us/sample - loss: 0.5115 - acc: 0.7656\n",
      "Epoch 108/200\n",
      "576/576 [==============================] - 0s 157us/sample - loss: 0.5046 - acc: 0.7656\n",
      "Epoch 109/200\n",
      "576/576 [==============================] - 0s 161us/sample - loss: 0.5457 - acc: 0.7361\n",
      "Epoch 110/200\n",
      "576/576 [==============================] - 0s 167us/sample - loss: 0.5059 - acc: 0.7569\n",
      "Epoch 111/200\n",
      "576/576 [==============================] - 0s 165us/sample - loss: 0.5291 - acc: 0.7483\n",
      "Epoch 112/200\n",
      "576/576 [==============================] - 0s 160us/sample - loss: 0.5706 - acc: 0.7066\n",
      "Epoch 113/200\n",
      "576/576 [==============================] - 0s 169us/sample - loss: 0.5171 - acc: 0.7552\n",
      "Epoch 114/200\n",
      "576/576 [==============================] - 0s 159us/sample - loss: 0.5121 - acc: 0.7500\n",
      "Epoch 115/200\n",
      "576/576 [==============================] - 0s 154us/sample - loss: 0.5500 - acc: 0.7309\n",
      "Epoch 116/200\n",
      "576/576 [==============================] - 0s 156us/sample - loss: 0.5296 - acc: 0.7413\n",
      "Epoch 117/200\n",
      "576/576 [==============================] - 0s 160us/sample - loss: 0.5158 - acc: 0.7535\n",
      "Epoch 118/200\n",
      "576/576 [==============================] - 0s 159us/sample - loss: 0.5069 - acc: 0.7483\n",
      "Epoch 119/200\n",
      "576/576 [==============================] - 0s 163us/sample - loss: 0.4990 - acc: 0.7726\n",
      "Epoch 120/200\n",
      "576/576 [==============================] - 0s 190us/sample - loss: 0.5779 - acc: 0.7049\n",
      "Epoch 121/200\n",
      "576/576 [==============================] - 0s 162us/sample - loss: 0.5219 - acc: 0.7552\n",
      "Epoch 122/200\n",
      "576/576 [==============================] - 0s 164us/sample - loss: 0.5097 - acc: 0.7656\n",
      "Epoch 123/200\n",
      "576/576 [==============================] - 0s 158us/sample - loss: 0.5098 - acc: 0.7535\n",
      "Epoch 124/200\n",
      "576/576 [==============================] - 0s 158us/sample - loss: 0.5109 - acc: 0.7413\n",
      "Epoch 125/200\n",
      "576/576 [==============================] - 0s 199us/sample - loss: 0.5295 - acc: 0.7465\n",
      "Epoch 126/200\n",
      "576/576 [==============================] - 0s 164us/sample - loss: 0.5661 - acc: 0.7205\n",
      "Epoch 127/200\n",
      "576/576 [==============================] - 0s 190us/sample - loss: 0.5207 - acc: 0.7413\n",
      "Epoch 128/200\n",
      "576/576 [==============================] - 0s 159us/sample - loss: 0.5018 - acc: 0.7691\n",
      "Epoch 129/200\n",
      "576/576 [==============================] - 0s 162us/sample - loss: 0.5303 - acc: 0.7656\n",
      "Epoch 130/200\n",
      "576/576 [==============================] - 0s 177us/sample - loss: 0.5081 - acc: 0.7431\n",
      "Epoch 131/200\n",
      "576/576 [==============================] - 0s 170us/sample - loss: 0.5073 - acc: 0.7604\n",
      "Epoch 132/200\n",
      "576/576 [==============================] - 0s 155us/sample - loss: 0.5550 - acc: 0.7552\n",
      "Epoch 133/200\n",
      "576/576 [==============================] - 0s 163us/sample - loss: 0.5135 - acc: 0.7552\n",
      "Epoch 134/200\n",
      "576/576 [==============================] - 0s 158us/sample - loss: 0.4949 - acc: 0.7622\n",
      "Epoch 135/200\n",
      "576/576 [==============================] - 0s 158us/sample - loss: 0.5227 - acc: 0.7344\n",
      "Epoch 136/200\n",
      "576/576 [==============================] - 0s 155us/sample - loss: 0.5404 - acc: 0.7344\n",
      "Epoch 137/200\n",
      "576/576 [==============================] - 0s 154us/sample - loss: 0.5181 - acc: 0.7639\n",
      "Epoch 138/200\n",
      "576/576 [==============================] - 0s 162us/sample - loss: 0.5167 - acc: 0.7483\n",
      "Epoch 139/200\n",
      "576/576 [==============================] - 0s 159us/sample - loss: 0.5184 - acc: 0.7500\n",
      "Epoch 140/200\n",
      "576/576 [==============================] - 0s 161us/sample - loss: 0.5073 - acc: 0.7552\n",
      "Epoch 141/200\n",
      "576/576 [==============================] - 0s 158us/sample - loss: 0.5101 - acc: 0.7396\n",
      "Epoch 142/200\n",
      "576/576 [==============================] - 0s 159us/sample - loss: 0.4968 - acc: 0.7448\n",
      "Epoch 143/200\n",
      "576/576 [==============================] - 0s 164us/sample - loss: 0.4931 - acc: 0.7552\n",
      "Epoch 144/200\n",
      "576/576 [==============================] - 0s 157us/sample - loss: 0.4907 - acc: 0.7708\n",
      "Epoch 145/200\n",
      "576/576 [==============================] - 0s 158us/sample - loss: 0.4973 - acc: 0.7743\n",
      "Epoch 146/200\n",
      "576/576 [==============================] - 0s 157us/sample - loss: 0.4884 - acc: 0.7639\n",
      "Epoch 147/200\n",
      "576/576 [==============================] - 0s 156us/sample - loss: 0.5203 - acc: 0.7326\n",
      "Epoch 148/200\n",
      "576/576 [==============================] - 0s 158us/sample - loss: 0.4934 - acc: 0.7500\n",
      "Epoch 149/200\n",
      "576/576 [==============================] - 0s 159us/sample - loss: 0.5029 - acc: 0.7639\n",
      "Epoch 150/200\n",
      "576/576 [==============================] - 0s 163us/sample - loss: 0.5132 - acc: 0.7378\n",
      "Epoch 151/200\n",
      "576/576 [==============================] - 0s 158us/sample - loss: 0.4822 - acc: 0.7743\n",
      "Epoch 152/200\n",
      "576/576 [==============================] - 0s 158us/sample - loss: 0.5013 - acc: 0.7674\n",
      "Epoch 153/200\n",
      "576/576 [==============================] - 0s 160us/sample - loss: 0.4828 - acc: 0.7812\n",
      "Epoch 154/200\n",
      "576/576 [==============================] - 0s 158us/sample - loss: 0.4888 - acc: 0.7743\n",
      "Epoch 155/200\n",
      "576/576 [==============================] - 0s 161us/sample - loss: 0.4963 - acc: 0.7622\n",
      "Epoch 156/200\n",
      "576/576 [==============================] - 0s 158us/sample - loss: 0.4948 - acc: 0.7743\n",
      "Epoch 157/200\n",
      "576/576 [==============================] - 0s 168us/sample - loss: 0.5012 - acc: 0.7517\n",
      "Epoch 158/200\n",
      "576/576 [==============================] - 0s 159us/sample - loss: 0.5107 - acc: 0.7378\n",
      "Epoch 159/200\n",
      "576/576 [==============================] - 0s 155us/sample - loss: 0.4942 - acc: 0.7656\n",
      "Epoch 160/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 163us/sample - loss: 0.5162 - acc: 0.7483\n",
      "Epoch 161/200\n",
      "576/576 [==============================] - 0s 155us/sample - loss: 0.4799 - acc: 0.7726\n",
      "Epoch 162/200\n",
      "576/576 [==============================] - 0s 156us/sample - loss: 0.5093 - acc: 0.7639\n",
      "Epoch 163/200\n",
      "576/576 [==============================] - 0s 163us/sample - loss: 0.4888 - acc: 0.7622\n",
      "Epoch 164/200\n",
      "576/576 [==============================] - 0s 162us/sample - loss: 0.5132 - acc: 0.7483\n",
      "Epoch 165/200\n",
      "576/576 [==============================] - 0s 162us/sample - loss: 0.4881 - acc: 0.7726\n",
      "Epoch 166/200\n",
      "576/576 [==============================] - 0s 158us/sample - loss: 0.5074 - acc: 0.7656\n",
      "Epoch 167/200\n",
      "576/576 [==============================] - 0s 156us/sample - loss: 0.5062 - acc: 0.7431\n",
      "Epoch 168/200\n",
      "576/576 [==============================] - 0s 158us/sample - loss: 0.5059 - acc: 0.7639\n",
      "Epoch 169/200\n",
      "576/576 [==============================] - 0s 158us/sample - loss: 0.4997 - acc: 0.7726\n",
      "Epoch 170/200\n",
      "576/576 [==============================] - 0s 162us/sample - loss: 0.4662 - acc: 0.7795\n",
      "Epoch 171/200\n",
      "576/576 [==============================] - 0s 157us/sample - loss: 0.4764 - acc: 0.7656\n",
      "Epoch 172/200\n",
      "576/576 [==============================] - 0s 156us/sample - loss: 0.4778 - acc: 0.7795\n",
      "Epoch 173/200\n",
      "576/576 [==============================] - 0s 168us/sample - loss: 0.4780 - acc: 0.7639\n",
      "Epoch 174/200\n",
      "576/576 [==============================] - 0s 156us/sample - loss: 0.4828 - acc: 0.7691\n",
      "Epoch 175/200\n",
      "576/576 [==============================] - 0s 160us/sample - loss: 0.4806 - acc: 0.7812\n",
      "Epoch 176/200\n",
      "576/576 [==============================] - 0s 157us/sample - loss: 0.5172 - acc: 0.7604\n",
      "Epoch 177/200\n",
      "576/576 [==============================] - 0s 157us/sample - loss: 0.5170 - acc: 0.7483\n",
      "Epoch 178/200\n",
      "576/576 [==============================] - 0s 163us/sample - loss: 0.4960 - acc: 0.7639\n",
      "Epoch 179/200\n",
      "576/576 [==============================] - 0s 161us/sample - loss: 0.4832 - acc: 0.7760\n",
      "Epoch 180/200\n",
      "576/576 [==============================] - 0s 209us/sample - loss: 0.4917 - acc: 0.7778\n",
      "Epoch 181/200\n",
      "576/576 [==============================] - 0s 176us/sample - loss: 0.4831 - acc: 0.7760\n",
      "Epoch 182/200\n",
      "576/576 [==============================] - 0s 177us/sample - loss: 0.5075 - acc: 0.7622\n",
      "Epoch 183/200\n",
      "576/576 [==============================] - 0s 176us/sample - loss: 0.4738 - acc: 0.7847\n",
      "Epoch 184/200\n",
      "576/576 [==============================] - 0s 171us/sample - loss: 0.4937 - acc: 0.7812\n",
      "Epoch 185/200\n",
      "576/576 [==============================] - 0s 160us/sample - loss: 0.4713 - acc: 0.7743\n",
      "Epoch 186/200\n",
      "576/576 [==============================] - 0s 155us/sample - loss: 0.4773 - acc: 0.7778\n",
      "Epoch 187/200\n",
      "576/576 [==============================] - 0s 173us/sample - loss: 0.4915 - acc: 0.7691\n",
      "Epoch 188/200\n",
      "576/576 [==============================] - 0s 158us/sample - loss: 0.4843 - acc: 0.7743\n",
      "Epoch 189/200\n",
      "576/576 [==============================] - 0s 157us/sample - loss: 0.5278 - acc: 0.7431\n",
      "Epoch 190/200\n",
      "576/576 [==============================] - 0s 163us/sample - loss: 0.4646 - acc: 0.7934\n",
      "Epoch 191/200\n",
      "576/576 [==============================] - 0s 154us/sample - loss: 0.4825 - acc: 0.7778\n",
      "Epoch 192/200\n",
      "576/576 [==============================] - 0s 155us/sample - loss: 0.4959 - acc: 0.7622\n",
      "Epoch 193/200\n",
      "576/576 [==============================] - 0s 154us/sample - loss: 0.4768 - acc: 0.7847\n",
      "Epoch 194/200\n",
      "576/576 [==============================] - 0s 158us/sample - loss: 0.4932 - acc: 0.7483\n",
      "Epoch 195/200\n",
      "576/576 [==============================] - 0s 167us/sample - loss: 0.4713 - acc: 0.7778\n",
      "Epoch 196/200\n",
      "576/576 [==============================] - 0s 155us/sample - loss: 0.4837 - acc: 0.7674\n",
      "Epoch 197/200\n",
      "576/576 [==============================] - 0s 156us/sample - loss: 0.4777 - acc: 0.7812\n",
      "Epoch 198/200\n",
      "576/576 [==============================] - 0s 157us/sample - loss: 0.5103 - acc: 0.7656\n",
      "Epoch 199/200\n",
      "576/576 [==============================] - 0s 153us/sample - loss: 0.4706 - acc: 0.7934\n",
      "Epoch 200/200\n",
      "576/576 [==============================] - 0s 154us/sample - loss: 0.4679 - acc: 0.7639\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd6651c2290>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=200, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "81e7d3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "192/192 [==============================] - 0s 570us/sample - loss: 0.5345 - acc: 0.7708\n",
      "Test Data Accuracy: 0.7708333\n"
     ]
    }
   ],
   "source": [
    "print('Test Data Accuracy:', model.evaluate(X_test, y_test)[1])\n",
    "#[0]은 손실값, [1]은 정확도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2fc56096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[7.39103854e-02],\n",
       "       [1.67432904e-01],\n",
       "       [6.71792328e-01],\n",
       "       [1.15804523e-01],\n",
       "       [7.87068963e-01],\n",
       "       [3.16190004e-01],\n",
       "       [6.22741580e-01],\n",
       "       [4.93118495e-01],\n",
       "       [2.38258898e-01],\n",
       "       [1.15837961e-01],\n",
       "       [2.56867826e-01],\n",
       "       [2.58713126e-01],\n",
       "       [1.33205026e-01],\n",
       "       [7.06026196e-01],\n",
       "       [4.72946316e-01],\n",
       "       [3.44008744e-01],\n",
       "       [9.33461368e-01],\n",
       "       [8.79471540e-01],\n",
       "       [6.46113515e-01],\n",
       "       [1.39506906e-01],\n",
       "       [6.17923677e-01],\n",
       "       [8.15522671e-01],\n",
       "       [6.50144935e-01],\n",
       "       [3.57394218e-01],\n",
       "       [6.02106750e-02],\n",
       "       [2.33112335e-01],\n",
       "       [1.69469982e-01],\n",
       "       [1.95405215e-01],\n",
       "       [8.47477436e-01],\n",
       "       [1.58419430e-01],\n",
       "       [1.24509275e-01],\n",
       "       [3.11696172e-01],\n",
       "       [5.73517382e-01],\n",
       "       [6.12828910e-01],\n",
       "       [1.31806284e-01],\n",
       "       [2.72203773e-01],\n",
       "       [3.56657326e-01],\n",
       "       [2.79759645e-01],\n",
       "       [1.97291404e-01],\n",
       "       [6.77211881e-01],\n",
       "       [7.44663179e-02],\n",
       "       [3.53810072e-01],\n",
       "       [3.22170556e-01],\n",
       "       [9.29667294e-01],\n",
       "       [2.87983030e-01],\n",
       "       [8.19015920e-01],\n",
       "       [2.31349468e-03],\n",
       "       [3.34676147e-01],\n",
       "       [1.90594792e-02],\n",
       "       [6.37867153e-02],\n",
       "       [6.63680136e-02],\n",
       "       [5.93434453e-01],\n",
       "       [6.75983667e-01],\n",
       "       [2.66344190e-01],\n",
       "       [9.83085632e-02],\n",
       "       [2.02968925e-01],\n",
       "       [5.25227964e-01],\n",
       "       [3.12979072e-01],\n",
       "       [6.87860847e-01],\n",
       "       [1.55831128e-01],\n",
       "       [2.21899629e-01],\n",
       "       [7.71375537e-01],\n",
       "       [2.04423130e-01],\n",
       "       [1.01003110e-01],\n",
       "       [2.51069248e-01],\n",
       "       [1.40829414e-01],\n",
       "       [2.22763836e-01],\n",
       "       [1.85913056e-01],\n",
       "       [1.77883387e-01],\n",
       "       [2.13073611e-01],\n",
       "       [2.84485400e-01],\n",
       "       [1.98553145e-01],\n",
       "       [7.23021865e-01],\n",
       "       [6.09539747e-01],\n",
       "       [1.30666018e-01],\n",
       "       [1.34455949e-01],\n",
       "       [8.44546854e-02],\n",
       "       [9.70636845e-01],\n",
       "       [1.68067247e-01],\n",
       "       [7.63234735e-01],\n",
       "       [6.62583411e-01],\n",
       "       [4.11990285e-02],\n",
       "       [2.75829554e-01],\n",
       "       [6.83376789e-02],\n",
       "       [3.44900310e-01],\n",
       "       [8.20649862e-02],\n",
       "       [2.74895608e-01],\n",
       "       [8.59126449e-02],\n",
       "       [5.26311576e-01],\n",
       "       [6.25657082e-01],\n",
       "       [4.75752980e-01],\n",
       "       [9.17962790e-02],\n",
       "       [1.46773070e-01],\n",
       "       [6.53427541e-01],\n",
       "       [4.57588494e-01],\n",
       "       [3.57727110e-02],\n",
       "       [3.69126678e-01],\n",
       "       [4.57184345e-01],\n",
       "       [2.32290417e-01],\n",
       "       [1.18573546e-01],\n",
       "       [2.14704663e-01],\n",
       "       [1.68610066e-01],\n",
       "       [9.26484168e-02],\n",
       "       [1.61669403e-01],\n",
       "       [2.90608704e-01],\n",
       "       [3.26339304e-01],\n",
       "       [1.00580186e-01],\n",
       "       [4.86049354e-01],\n",
       "       [8.22830498e-02],\n",
       "       [7.46801853e-01],\n",
       "       [8.43163729e-02],\n",
       "       [3.13333213e-01],\n",
       "       [9.72378254e-01],\n",
       "       [2.82973558e-01],\n",
       "       [2.59976238e-01],\n",
       "       [6.78780973e-02],\n",
       "       [5.45309305e-01],\n",
       "       [5.20038068e-01],\n",
       "       [5.44321656e-01],\n",
       "       [1.39495373e-01],\n",
       "       [2.40264744e-01],\n",
       "       [5.39354920e-01],\n",
       "       [2.12874949e-01],\n",
       "       [7.52931714e-01],\n",
       "       [6.88964725e-02],\n",
       "       [5.29723823e-01],\n",
       "       [1.19130433e-01],\n",
       "       [1.39572263e-01],\n",
       "       [2.79439956e-01],\n",
       "       [2.01972842e-01],\n",
       "       [5.54916084e-01],\n",
       "       [3.12662780e-01],\n",
       "       [1.64092928e-01],\n",
       "       [4.39010501e-01],\n",
       "       [3.76242518e-01],\n",
       "       [5.98268270e-01],\n",
       "       [1.55284345e-01],\n",
       "       [1.34675831e-01],\n",
       "       [6.53193057e-01],\n",
       "       [5.96015394e-01],\n",
       "       [5.10067225e-01],\n",
       "       [1.77837729e-01],\n",
       "       [5.18353760e-01],\n",
       "       [5.00776350e-01],\n",
       "       [3.68223935e-01],\n",
       "       [5.65606058e-02],\n",
       "       [6.40381575e-02],\n",
       "       [6.31947637e-01],\n",
       "       [7.17601478e-02],\n",
       "       [3.39576662e-01],\n",
       "       [4.38685566e-01],\n",
       "       [1.33214146e-01],\n",
       "       [4.11606550e-01],\n",
       "       [2.20842332e-01],\n",
       "       [5.95911562e-01],\n",
       "       [9.99918103e-01],\n",
       "       [5.83136618e-01],\n",
       "       [3.31913739e-01],\n",
       "       [2.65462399e-01],\n",
       "       [2.21374005e-01],\n",
       "       [2.58555859e-01],\n",
       "       [2.31203556e-01],\n",
       "       [4.84885484e-01],\n",
       "       [2.37057030e-01],\n",
       "       [3.46598029e-01],\n",
       "       [2.53564715e-02],\n",
       "       [3.48966777e-01],\n",
       "       [5.31432331e-01],\n",
       "       [2.31923938e-01],\n",
       "       [4.55985636e-01],\n",
       "       [4.15460110e-01],\n",
       "       [3.08009505e-01],\n",
       "       [1.67448014e-01],\n",
       "       [5.48311114e-01],\n",
       "       [1.27158821e-01],\n",
       "       [2.57774889e-01],\n",
       "       [9.58804190e-02],\n",
       "       [3.59996021e-01],\n",
       "       [2.44212449e-02],\n",
       "       [1.09615922e-01],\n",
       "       [9.02970135e-01],\n",
       "       [5.04997969e-01],\n",
       "       [8.89756083e-01],\n",
       "       [2.10725516e-01],\n",
       "       [3.44140708e-01],\n",
       "       [2.48969138e-01],\n",
       "       [1.83343887e-04],\n",
       "       [7.90295362e-01],\n",
       "       [3.36603105e-01],\n",
       "       [2.76483804e-01],\n",
       "       [9.99875665e-01],\n",
       "       [1.32593989e-01]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "380c39a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.07391039], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5871f37",
   "metadata": {},
   "source": [
    "### 결과: 이진 분류이므로 class가 1이 될 확률을 보여준다. 즉, 위의 예시에는 0.68의 확률로 당뇨병에 걸렸을거라 예상된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3ccce479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(X_test, y_test, 'bo')\n",
    "plt.plot(X_test, model.predict(X_test), 'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f213b468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "(192, 1)\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test)\n",
    "print(predictions.shape)\n",
    "predictions2 = np.array([1 if pre > 0.5 else 0 for pre in predictions]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6ac3e221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(X_test, y_test, 'bo')\n",
    "plt.plot(X_test, predictions2, 'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad6f3d3",
   "metadata": {},
   "source": [
    "## 모델 저장 및 재사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fec08872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "temp_model_path = '/Users/jsha/gjai/nlp/pytest/temp_model/'\n",
    "os.makedirs(temp_model_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8bcdaf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(temp_model_path+'pima_indians_diabetes.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "27ead953",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "loaded_model = load_model(temp_model_path+'pima_indians_diabetes.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1df86ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[7.39103854e-02],\n",
       "       [1.67432904e-01],\n",
       "       [6.71792328e-01],\n",
       "       [1.15804523e-01],\n",
       "       [7.87068963e-01],\n",
       "       [3.16190004e-01],\n",
       "       [6.22741580e-01],\n",
       "       [4.93118495e-01],\n",
       "       [2.38258898e-01],\n",
       "       [1.15837961e-01],\n",
       "       [2.56867826e-01],\n",
       "       [2.58713126e-01],\n",
       "       [1.33205026e-01],\n",
       "       [7.06026196e-01],\n",
       "       [4.72946316e-01],\n",
       "       [3.44008744e-01],\n",
       "       [9.33461368e-01],\n",
       "       [8.79471540e-01],\n",
       "       [6.46113515e-01],\n",
       "       [1.39506906e-01],\n",
       "       [6.17923677e-01],\n",
       "       [8.15522671e-01],\n",
       "       [6.50144935e-01],\n",
       "       [3.57394218e-01],\n",
       "       [6.02106750e-02],\n",
       "       [2.33112335e-01],\n",
       "       [1.69469982e-01],\n",
       "       [1.95405215e-01],\n",
       "       [8.47477436e-01],\n",
       "       [1.58419430e-01],\n",
       "       [1.24509275e-01],\n",
       "       [3.11696172e-01],\n",
       "       [5.73517382e-01],\n",
       "       [6.12828910e-01],\n",
       "       [1.31806284e-01],\n",
       "       [2.72203773e-01],\n",
       "       [3.56657326e-01],\n",
       "       [2.79759645e-01],\n",
       "       [1.97291404e-01],\n",
       "       [6.77211881e-01],\n",
       "       [7.44663179e-02],\n",
       "       [3.53810072e-01],\n",
       "       [3.22170556e-01],\n",
       "       [9.29667294e-01],\n",
       "       [2.87983030e-01],\n",
       "       [8.19015920e-01],\n",
       "       [2.31349468e-03],\n",
       "       [3.34676147e-01],\n",
       "       [1.90594792e-02],\n",
       "       [6.37867153e-02],\n",
       "       [6.63680136e-02],\n",
       "       [5.93434453e-01],\n",
       "       [6.75983667e-01],\n",
       "       [2.66344190e-01],\n",
       "       [9.83085632e-02],\n",
       "       [2.02968925e-01],\n",
       "       [5.25227964e-01],\n",
       "       [3.12979072e-01],\n",
       "       [6.87860847e-01],\n",
       "       [1.55831128e-01],\n",
       "       [2.21899629e-01],\n",
       "       [7.71375537e-01],\n",
       "       [2.04423130e-01],\n",
       "       [1.01003110e-01],\n",
       "       [2.51069248e-01],\n",
       "       [1.40829414e-01],\n",
       "       [2.22763836e-01],\n",
       "       [1.85913056e-01],\n",
       "       [1.77883387e-01],\n",
       "       [2.13073611e-01],\n",
       "       [2.84485400e-01],\n",
       "       [1.98553145e-01],\n",
       "       [7.23021865e-01],\n",
       "       [6.09539747e-01],\n",
       "       [1.30666018e-01],\n",
       "       [1.34455949e-01],\n",
       "       [8.44546854e-02],\n",
       "       [9.70636845e-01],\n",
       "       [1.68067247e-01],\n",
       "       [7.63234735e-01],\n",
       "       [6.62583411e-01],\n",
       "       [4.11990285e-02],\n",
       "       [2.75829554e-01],\n",
       "       [6.83376789e-02],\n",
       "       [3.44900310e-01],\n",
       "       [8.20649862e-02],\n",
       "       [2.74895608e-01],\n",
       "       [8.59126449e-02],\n",
       "       [5.26311576e-01],\n",
       "       [6.25657082e-01],\n",
       "       [4.75752980e-01],\n",
       "       [9.17962790e-02],\n",
       "       [1.46773070e-01],\n",
       "       [6.53427541e-01],\n",
       "       [4.57588494e-01],\n",
       "       [3.57727110e-02],\n",
       "       [3.69126678e-01],\n",
       "       [4.57184345e-01],\n",
       "       [2.32290417e-01],\n",
       "       [1.18573546e-01],\n",
       "       [2.14704663e-01],\n",
       "       [1.68610066e-01],\n",
       "       [9.26484168e-02],\n",
       "       [1.61669403e-01],\n",
       "       [2.90608704e-01],\n",
       "       [3.26339304e-01],\n",
       "       [1.00580186e-01],\n",
       "       [4.86049354e-01],\n",
       "       [8.22830498e-02],\n",
       "       [7.46801853e-01],\n",
       "       [8.43163729e-02],\n",
       "       [3.13333213e-01],\n",
       "       [9.72378254e-01],\n",
       "       [2.82973558e-01],\n",
       "       [2.59976238e-01],\n",
       "       [6.78780973e-02],\n",
       "       [5.45309305e-01],\n",
       "       [5.20038068e-01],\n",
       "       [5.44321656e-01],\n",
       "       [1.39495373e-01],\n",
       "       [2.40264744e-01],\n",
       "       [5.39354920e-01],\n",
       "       [2.12874949e-01],\n",
       "       [7.52931714e-01],\n",
       "       [6.88964725e-02],\n",
       "       [5.29723823e-01],\n",
       "       [1.19130433e-01],\n",
       "       [1.39572263e-01],\n",
       "       [2.79439956e-01],\n",
       "       [2.01972842e-01],\n",
       "       [5.54916084e-01],\n",
       "       [3.12662780e-01],\n",
       "       [1.64092928e-01],\n",
       "       [4.39010501e-01],\n",
       "       [3.76242518e-01],\n",
       "       [5.98268270e-01],\n",
       "       [1.55284345e-01],\n",
       "       [1.34675831e-01],\n",
       "       [6.53193057e-01],\n",
       "       [5.96015394e-01],\n",
       "       [5.10067225e-01],\n",
       "       [1.77837729e-01],\n",
       "       [5.18353760e-01],\n",
       "       [5.00776350e-01],\n",
       "       [3.68223935e-01],\n",
       "       [5.65606058e-02],\n",
       "       [6.40381575e-02],\n",
       "       [6.31947637e-01],\n",
       "       [7.17601478e-02],\n",
       "       [3.39576662e-01],\n",
       "       [4.38685566e-01],\n",
       "       [1.33214146e-01],\n",
       "       [4.11606550e-01],\n",
       "       [2.20842332e-01],\n",
       "       [5.95911562e-01],\n",
       "       [9.99918103e-01],\n",
       "       [5.83136618e-01],\n",
       "       [3.31913739e-01],\n",
       "       [2.65462399e-01],\n",
       "       [2.21374005e-01],\n",
       "       [2.58555859e-01],\n",
       "       [2.31203556e-01],\n",
       "       [4.84885484e-01],\n",
       "       [2.37057030e-01],\n",
       "       [3.46598029e-01],\n",
       "       [2.53564715e-02],\n",
       "       [3.48966777e-01],\n",
       "       [5.31432331e-01],\n",
       "       [2.31923938e-01],\n",
       "       [4.55985636e-01],\n",
       "       [4.15460110e-01],\n",
       "       [3.08009505e-01],\n",
       "       [1.67448014e-01],\n",
       "       [5.48311114e-01],\n",
       "       [1.27158821e-01],\n",
       "       [2.57774889e-01],\n",
       "       [9.58804190e-02],\n",
       "       [3.59996021e-01],\n",
       "       [2.44212449e-02],\n",
       "       [1.09615922e-01],\n",
       "       [9.02970135e-01],\n",
       "       [5.04997969e-01],\n",
       "       [8.89756083e-01],\n",
       "       [2.10725516e-01],\n",
       "       [3.44140708e-01],\n",
       "       [2.48969138e-01],\n",
       "       [1.83343887e-04],\n",
       "       [7.90295362e-01],\n",
       "       [3.36603105e-01],\n",
       "       [2.76483804e-01],\n",
       "       [9.99875665e-01],\n",
       "       [1.32593989e-01]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "879a2d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   pregnant  plasma  pressure  thickness  insulin   BMI  pedigree  age\n",
      "0         6     105        80         28        0  32.5     0.878   26\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.18127096]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data = {\"pregnant\":[6], 'plasma':[105], 'pressure':[80],\n",
    "           \"thickness\":[28], \"insulin\":[0], \"BMI\":[32.5],\n",
    "           \"pedigree\":[0.878], 'age':[26]}\n",
    "new_data = pd.DataFrame(new_data)\n",
    "print(new_data)\n",
    "loaded_model.predict(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a617264",
   "metadata": {},
   "source": [
    "# 딥러닝 다중분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cec7ea10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sepal.Length</th>\n",
       "      <th>Sepal.Width</th>\n",
       "      <th>Petal.Length</th>\n",
       "      <th>Petal.Width</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sepal.Length  Sepal.Width  Petal.Length  Petal.Width Species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = '/Users/jsha/gjai/nlp/pytest/'\n",
    "\n",
    "df = pd.read_csv(path+'iris.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7a83a491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sepal.Length</th>\n",
       "      <th>Sepal.Width</th>\n",
       "      <th>Petal.Length</th>\n",
       "      <th>Petal.Width</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sepal.Length  Sepal.Width  Petal.Length  Petal.Width    Species\n",
       "145           6.7          3.0           5.2          2.3  virginica\n",
       "146           6.3          2.5           5.0          1.9  virginica\n",
       "147           6.5          3.0           5.2          2.0  virginica\n",
       "148           6.2          3.4           5.4          2.3  virginica\n",
       "149           5.9          3.0           5.1          1.8  virginica"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3b52e13c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype=object)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Species.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "22642183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sepal.Length</th>\n",
       "      <th>Sepal.Width</th>\n",
       "      <th>Petal.Length</th>\n",
       "      <th>Petal.Width</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>4.5</td>\n",
       "      <td>2.3</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.1</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.4</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sepal.Length  Sepal.Width  Petal.Length  Petal.Width     Species\n",
       "60            5.0          2.0           3.5          1.0  versicolor\n",
       "120           6.9          3.2           5.7          2.3   virginica\n",
       "41            4.5          2.3           1.3          0.3      setosa\n",
       "69            5.6          2.5           3.9          1.1  versicolor\n",
       "63            6.1          2.9           4.7          1.4  versicolor"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9d1e03d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.boxplot(x=[df[col] for col in df.iloc[:,:-1].columns], \n",
    "           labels=df.iloc[:,:-1].columns.to_list())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4878a35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "46cee3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n",
      "109           7.2          3.6           6.1          2.5\n",
      "75            6.6          3.0           4.4          1.4\n",
      "49            5.0          3.3           1.4          0.2\n",
      "32            5.2          4.1           1.5          0.1\n",
      "58            6.6          2.9           4.6          1.3\n",
      "--------------------------------------------------\n",
      "     Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n",
      "92            5.8          2.6           4.0          1.2\n",
      "80            5.5          2.4           3.8          1.1\n",
      "96            5.7          2.9           4.2          1.3\n",
      "115           6.4          3.2           5.3          2.3\n",
      "36            5.5          3.5           1.3          0.2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "print(X_train.head())\n",
    "print('-'*50)\n",
    "print(X_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a3078c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109     virginica\n",
      "75     versicolor\n",
      "49         setosa\n",
      "32         setosa\n",
      "58     versicolor\n",
      "          ...    \n",
      "74     versicolor\n",
      "5          setosa\n",
      "63     versicolor\n",
      "70     versicolor\n",
      "105     virginica\n",
      "Name: Species, Length: 112, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "eedd5081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train: [2 1 0 0 1 2 2 0 2 0 1 0 0 1 1 0 2 1 1 2 0 2 0 2 1 0 2 0 0 2 2 0 2 1 0 0 0\n",
      " 1 0 0 2 0 2 1 1 2 1 1 2 0 1 1 1 2 0 0 2 0 2 0 2 2 0 1 0 0 2 0 1 1 2 2 2 1\n",
      " 1 1 2 2 1 0 0 0 0 2 2 0 2 1 1 2 2 2 1 0 1 1 1 2 1 2 2 1 2 2 1 0 1 1 0 1 1\n",
      " 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "e = LabelEncoder()\n",
    "e.fit(y_train)\n",
    "y_train = e.transform(y_train)\n",
    "\n",
    "print('y_train:', y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5a8425c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'setosa', 1: 'versicolor', 2: 'virginica'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_transformed = e.transform(['setosa', 'versicolor', 'virginica'])\n",
    "check = [(j, i) for i, j in zip(['setosa', 'versicolor', 'virginica'], y_train_transformed)]\n",
    "check = dict(check)\n",
    "check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ff786ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "# print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0d2842c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(16, activation='relu', input_dim=4))\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b54688c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 112 samples\n",
      "Epoch 1/50\n",
      "112/112 [==============================] - 1s 5ms/sample - loss: 1.4688 - acc: 0.3304\n",
      "Epoch 2/50\n",
      "112/112 [==============================] - 0s 2ms/sample - loss: 0.8055 - acc: 0.6607\n",
      "Epoch 3/50\n",
      "112/112 [==============================] - 0s 2ms/sample - loss: 0.6486 - acc: 0.7589\n",
      "Epoch 4/50\n",
      "112/112 [==============================] - 0s 2ms/sample - loss: 0.5794 - acc: 0.8125\n",
      "Epoch 5/50\n",
      "112/112 [==============================] - 0s 2ms/sample - loss: 0.5289 - acc: 0.8661\n",
      "Epoch 6/50\n",
      "112/112 [==============================] - 0s 2ms/sample - loss: 0.4983 - acc: 0.9196\n",
      "Epoch 7/50\n",
      "112/112 [==============================] - 0s 2ms/sample - loss: 0.4624 - acc: 0.8393\n",
      "Epoch 8/50\n",
      "112/112 [==============================] - 0s 2ms/sample - loss: 0.4439 - acc: 0.8929\n",
      "Epoch 9/50\n",
      "112/112 [==============================] - 0s 2ms/sample - loss: 0.4210 - acc: 0.9107\n",
      "Epoch 10/50\n",
      "112/112 [==============================] - 0s 2ms/sample - loss: 0.4005 - acc: 0.8929\n",
      "Epoch 11/50\n",
      "112/112 [==============================] - 0s 2ms/sample - loss: 0.3899 - acc: 0.9018\n",
      "Epoch 12/50\n",
      "112/112 [==============================] - 0s 2ms/sample - loss: 0.3789 - acc: 0.9107\n",
      "Epoch 13/50\n",
      "112/112 [==============================] - 0s 2ms/sample - loss: 0.3519 - acc: 0.9107\n",
      "Epoch 14/50\n",
      "112/112 [==============================] - 0s 2ms/sample - loss: 0.3469 - acc: 0.9018\n",
      "Epoch 15/50\n",
      "112/112 [==============================] - 0s 2ms/sample - loss: 0.3345 - acc: 0.8929\n",
      "Epoch 16/50\n",
      "112/112 [==============================] - 0s 2ms/sample - loss: 0.3188 - acc: 0.9732\n",
      "Epoch 17/50\n",
      "112/112 [==============================] - 0s 2ms/sample - loss: 0.3061 - acc: 0.9375\n",
      "Epoch 18/50\n",
      "112/112 [==============================] - 0s 2ms/sample - loss: 0.3028 - acc: 0.9554\n",
      "Epoch 19/50\n",
      "112/112 [==============================] - 0s 2ms/sample - loss: 0.2909 - acc: 0.9464\n",
      "Epoch 20/50\n",
      "112/112 [==============================] - 0s 2ms/sample - loss: 0.2776 - acc: 0.9464\n",
      "Epoch 21/50\n",
      "112/112 [==============================] - 0s 2ms/sample - loss: 0.2725 - acc: 0.9732\n",
      "Epoch 22/50\n",
      "112/112 [==============================] - 0s 2ms/sample - loss: 0.2630 - acc: 0.9464\n",
      "Epoch 23/50\n",
      "112/112 [==============================] - 0s 2ms/sample - loss: 0.2481 - acc: 0.9732\n",
      "Epoch 24/50\n",
      "112/112 [==============================] - 0s 2ms/sample - loss: 0.2510 - acc: 0.9554\n",
      "Epoch 25/50\n",
      "112/112 [==============================] - 0s 2ms/sample - loss: 0.2335 - acc: 0.9732\n",
      "Epoch 26/50\n",
      "112/112 [==============================] - 0s 2ms/sample - loss: 0.2401 - acc: 0.9643\n",
      "Epoch 27/50\n",
      "112/112 [==============================] - 0s 2ms/sample - loss: 0.2315 - acc: 0.9554\n",
      "Epoch 28/50\n",
      "112/112 [==============================] - 0s 2ms/sample - loss: 0.2141 - acc: 0.9464\n",
      "Epoch 29/50\n",
      "112/112 [==============================] - 0s 2ms/sample - loss: 0.2090 - acc: 0.9554\n",
      "Epoch 30/50\n",
      "112/112 [==============================] - 0s 2ms/sample - loss: 0.2067 - acc: 0.9554\n",
      "Epoch 31/50\n",
      "112/112 [==============================] - 0s 2ms/sample - loss: 0.1972 - acc: 0.9554\n",
      "Epoch 32/50\n",
      "112/112 [==============================] - 0s 2ms/sample - loss: 0.1954 - acc: 0.9464\n",
      "Epoch 33/50\n",
      "112/112 [==============================] - 0s 2ms/sample - loss: 0.1942 - acc: 0.9643\n",
      "Epoch 34/50\n",
      "112/112 [==============================] - 0s 2ms/sample - loss: 0.1899 - acc: 0.9643\n",
      "Epoch 35/50\n",
      "112/112 [==============================] - 0s 2ms/sample - loss: 0.1842 - acc: 0.9464\n",
      "Epoch 36/50\n",
      "112/112 [==============================] - 0s 2ms/sample - loss: 0.1769 - acc: 0.9554\n",
      "Epoch 37/50\n",
      "112/112 [==============================] - 0s 2ms/sample - loss: 0.1761 - acc: 0.9732\n",
      "Epoch 38/50\n",
      "112/112 [==============================] - 0s 2ms/sample - loss: 0.1748 - acc: 0.9554\n",
      "Epoch 39/50\n",
      "112/112 [==============================] - 0s 2ms/sample - loss: 0.1706 - acc: 0.9643\n",
      "Epoch 40/50\n",
      "112/112 [==============================] - 0s 2ms/sample - loss: 0.1685 - acc: 0.9554\n",
      "Epoch 41/50\n",
      "112/112 [==============================] - 0s 2ms/sample - loss: 0.1651 - acc: 0.9732\n",
      "Epoch 42/50\n",
      "112/112 [==============================] - 0s 2ms/sample - loss: 0.1556 - acc: 0.9643\n",
      "Epoch 43/50\n",
      "112/112 [==============================] - 0s 2ms/sample - loss: 0.1495 - acc: 0.9732\n",
      "Epoch 44/50\n",
      "112/112 [==============================] - 0s 2ms/sample - loss: 0.1562 - acc: 0.9732\n",
      "Epoch 45/50\n",
      "112/112 [==============================] - 0s 2ms/sample - loss: 0.1439 - acc: 0.9821\n",
      "Epoch 46/50\n",
      "112/112 [==============================] - 0s 2ms/sample - loss: 0.1522 - acc: 0.9643\n",
      "Epoch 47/50\n",
      "112/112 [==============================] - 0s 2ms/sample - loss: 0.1444 - acc: 0.9732\n",
      "Epoch 48/50\n",
      "112/112 [==============================] - 0s 2ms/sample - loss: 0.1374 - acc: 0.9643\n",
      "Epoch 49/50\n",
      "112/112 [==============================] - 0s 2ms/sample - loss: 0.1448 - acc: 0.9732\n",
      "Epoch 50/50\n",
      "112/112 [==============================] - 0s 2ms/sample - loss: 0.1328 - acc: 0.9643\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd647e5ab50>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
    "             metrics=['acc'])\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "052442e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 2 0 1 0 2 0 2 0 2 0 1 2 1 0 2 1 0 2 0 1 2 1 0 1 0 0 1 0 0 1 0 1 2 2\n",
      " 2]\n"
     ]
    }
   ],
   "source": [
    "y_test = e.transform(y_test)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9182c484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38, 3)\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "38/38 [==============================] - 0s 2ms/sample - loss: 0.1477 - acc: 0.9474\n",
      "Test Data Accuracy: 0.94736844\n"
     ]
    }
   ],
   "source": [
    "y_test = tf.keras.utils.to_categorical(y_test)\n",
    "print(y_test.shape)\n",
    "\n",
    "print('Test Data Accuracy:', model.evaluate(X_test, y_test)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b952fa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '/Users/jsha/gjai/nlp/pytest/temp_model/'\n",
    "model.save(save_path+'iris.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c2b3c71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "loaded_model = load_model(save_path+'iris.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "442b649e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.3353805e-03, 8.9707863e-01, 1.0158598e-01],\n",
       "       [2.2055365e-03, 9.0445077e-01, 9.3343735e-02],\n",
       "       [3.8644724e-04, 7.7646816e-01, 2.2314544e-01],\n",
       "       [1.4776383e-08, 2.6561597e-02, 9.7343838e-01],\n",
       "       [9.9787247e-01, 2.1274881e-03, 7.1314714e-09],\n",
       "       [3.1248766e-03, 8.8284266e-01, 1.1403250e-01],\n",
       "       [9.9551684e-01, 4.4830046e-03, 8.2843535e-08],\n",
       "       [2.2871141e-06, 1.8773013e-01, 8.1226760e-01],\n",
       "       [9.9781370e-01, 2.1862874e-03, 1.2520397e-08],\n",
       "       [1.5356325e-07, 1.2664720e-01, 8.7335277e-01],\n",
       "       [9.9527353e-01, 4.7264211e-03, 9.8498887e-08],\n",
       "       [3.1440333e-11, 4.0160837e-03, 9.9598396e-01],\n",
       "       [9.9375421e-01, 6.2455558e-03, 1.9881202e-07],\n",
       "       [1.3435156e-05, 4.1724700e-01, 5.8273959e-01],\n",
       "       [4.7312803e-09, 1.4876761e-02, 9.8512322e-01],\n",
       "       [2.5372978e-04, 7.0647728e-01, 2.9326895e-01],\n",
       "       [9.9657911e-01, 3.4209248e-03, 4.2629654e-08],\n",
       "       [1.4473804e-07, 7.4812174e-02, 9.2518777e-01],\n",
       "       [2.4170520e-04, 8.1678009e-01, 1.8297820e-01],\n",
       "       [9.9609083e-01, 3.9091324e-03, 4.6282707e-08],\n",
       "       [5.1421541e-08, 3.4801174e-02, 9.6519887e-01],\n",
       "       [9.9699402e-01, 3.0059286e-03, 3.3884227e-08],\n",
       "       [2.2692756e-04, 6.0913157e-01, 3.9064145e-01],\n",
       "       [3.5185751e-06, 2.4031053e-01, 7.5968599e-01],\n",
       "       [2.1644859e-03, 9.2395771e-01, 7.3877811e-02],\n",
       "       [9.9352175e-01, 6.4780768e-03, 2.5694368e-07],\n",
       "       [1.7752196e-04, 8.2502979e-01, 1.7479272e-01],\n",
       "       [9.9534875e-01, 4.6510934e-03, 8.0298918e-08],\n",
       "       [9.9216300e-01, 7.8368299e-03, 2.5979074e-07],\n",
       "       [1.3205573e-05, 2.8567377e-01, 7.1431303e-01],\n",
       "       [9.9590623e-01, 4.0935879e-03, 7.3092018e-08],\n",
       "       [9.9572229e-01, 4.2775925e-03, 6.8177755e-08],\n",
       "       [3.7230398e-03, 9.3272328e-01, 6.3553751e-02],\n",
       "       [9.9696916e-01, 3.0308231e-03, 3.7464375e-08],\n",
       "       [5.1154278e-05, 6.0444695e-01, 3.9550182e-01],\n",
       "       [1.0119009e-08, 4.6959609e-02, 9.5304036e-01],\n",
       "       [2.9009081e-07, 1.0352162e-01, 8.9647812e-01],\n",
       "       [2.5170976e-09, 1.2663832e-02, 9.8733610e-01]], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e19ccc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Sepal.Legnth  Sepal.Width  Petal.Length  Petal.Width\n",
      "0           5.3          3.4           1.4          0.2\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "result: [[9.9726933e-01 2.7306627e-03 1.7605370e-08]]\n"
     ]
    }
   ],
   "source": [
    "new_data = {'Sepal.Legnth':[5.3], \"Sepal.Width\":[3.4],\n",
    "           'Petal.Length':[1.4], 'Petal.Width':[0.2]}\n",
    "new_data = pd.DataFrame(new_data)\n",
    "print(new_data)\n",
    "\n",
    "result = loaded_model.predict(new_data)\n",
    "print('result:', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4c8ce580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setosa\n",
      "setosa\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "result_idx = np.argmax(result)\n",
    "\n",
    "# method1\n",
    "if result_idx == 0:\n",
    "    print('setosa')\n",
    "elif result_idx == 1:\n",
    "    print('versicolor')\n",
    "elif result_idx == 2:\n",
    "    print('virginica')\n",
    "\n",
    "# method2\n",
    "if result_idx in check.keys():\n",
    "    print(check[result_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4314f696",
   "metadata": {},
   "source": [
    "# 베스트 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e940488f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2    3      4     5     6       7     8     9    10  11  12\n",
       "0   7.4  0.70  0.00  1.9  0.076  11.0  34.0  0.9978  3.51  0.56  9.4   5   1\n",
       "1   7.8  0.88  0.00  2.6  0.098  25.0  67.0  0.9968  3.20  0.68  9.8   5   1\n",
       "2   7.8  0.76  0.04  2.3  0.092  15.0  54.0  0.9970  3.26  0.65  9.8   5   1\n",
       "3  11.2  0.28  0.56  1.9  0.075  17.0  60.0  0.9980  3.16  0.58  9.8   6   1\n",
       "4   7.4  0.70  0.00  1.9  0.076  11.0  34.0  0.9978  3.51  0.56  9.4   5   1"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = '/Users/jsha/gjai/nlp/pytest/'\n",
    "save_path = '/Users/jsha/gjai/nlp/pytest/temp_model/'\n",
    "df = pd.read_csv(path+'wine.csv', header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0079a656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3908</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.035</td>\n",
       "      <td>11.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0.99232</td>\n",
       "      <td>3.14</td>\n",
       "      <td>0.50</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4945</th>\n",
       "      <td>6.7</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.24</td>\n",
       "      <td>10.3</td>\n",
       "      <td>0.057</td>\n",
       "      <td>64.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>0.99519</td>\n",
       "      <td>3.12</td>\n",
       "      <td>0.50</td>\n",
       "      <td>10.6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3634</th>\n",
       "      <td>8.8</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.28</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.018</td>\n",
       "      <td>18.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.99260</td>\n",
       "      <td>2.97</td>\n",
       "      <td>0.35</td>\n",
       "      <td>10.4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5749</th>\n",
       "      <td>7.6</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.31</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0.046</td>\n",
       "      <td>6.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>0.99714</td>\n",
       "      <td>3.07</td>\n",
       "      <td>0.61</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>8.5</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.11</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.084</td>\n",
       "      <td>9.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.53</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0     1     2     3      4     5      6        7     8     9     10  \\\n",
       "3908  8.1  0.28  0.34   1.3  0.035  11.0  126.0  0.99232  3.14  0.50   9.8   \n",
       "4945  6.7  0.18  0.24  10.3  0.057  64.0  185.0  0.99519  3.12  0.50  10.6   \n",
       "3634  8.8  0.20  0.28   1.1  0.018  18.0   72.0  0.99260  2.97  0.35  10.4   \n",
       "5749  7.6  0.48  0.31   9.4  0.046   6.0  194.0  0.99714  3.07  0.61   9.4   \n",
       "23    8.5  0.49  0.11   2.3  0.084   9.0   67.0  0.99680  3.17  0.53   9.4   \n",
       "\n",
       "      11  12  \n",
       "3908   6   0  \n",
       "4945   6   0  \n",
       "3634   5   0  \n",
       "5749   5   0  \n",
       "23     5   1  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "33aac28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 6497 entries, 3908 to 599\n",
      "Data columns (total 13 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       6497 non-null   float64\n",
      " 1   1       6497 non-null   float64\n",
      " 2   2       6497 non-null   float64\n",
      " 3   3       6497 non-null   float64\n",
      " 4   4       6497 non-null   float64\n",
      " 5   5       6497 non-null   float64\n",
      " 6   6       6497 non-null   float64\n",
      " 7   7       6497 non-null   float64\n",
      " 8   8       6497 non-null   float64\n",
      " 9   9       6497 non-null   float64\n",
      " 10  10      6497 non-null   float64\n",
      " 11  11      6497 non-null   int64  \n",
      " 12  12      6497 non-null   int64  \n",
      "dtypes: float64(11), int64(2)\n",
      "memory usage: 710.6 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "16cdd16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1a947c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0      1     2     3      4     5      6        7     8     9     10  \\\n",
      "6209  5.9  0.440  0.36   2.5  0.030  12.0   73.0  0.99201  3.22  0.48  10.8   \n",
      "1390  6.0  0.490  0.00   2.3  0.068  15.0   33.0  0.99292  3.58  0.59  12.5   \n",
      "5673  6.6  0.350  0.19  10.5  0.060  15.0   82.0  0.99588  3.13  0.38   9.9   \n",
      "3021  5.3  0.165  0.24   1.1  0.051  25.0  105.0  0.99250  3.32  0.47   9.1   \n",
      "5698  6.4  0.500  0.20   2.4  0.059  19.0  112.0  0.99314  3.18  0.40   9.2   \n",
      "\n",
      "      11  \n",
      "6209   6  \n",
      "1390   6  \n",
      "5673   4  \n",
      "3021   5  \n",
      "5698   6  \n",
      "--------------------------------------------------\n",
      "       0      1     2     3      4     5      6        7     8     9     10  \\\n",
      "6209  5.9  0.440  0.36   2.5  0.030  12.0   73.0  0.99201  3.22  0.48  10.8   \n",
      "1390  6.0  0.490  0.00   2.3  0.068  15.0   33.0  0.99292  3.58  0.59  12.5   \n",
      "5673  6.6  0.350  0.19  10.5  0.060  15.0   82.0  0.99588  3.13  0.38   9.9   \n",
      "3021  5.3  0.165  0.24   1.1  0.051  25.0  105.0  0.99250  3.32  0.47   9.1   \n",
      "5698  6.4  0.500  0.20   2.4  0.059  19.0  112.0  0.99314  3.18  0.40   9.2   \n",
      "\n",
      "      11  \n",
      "6209   6  \n",
      "1390   6  \n",
      "5673   4  \n",
      "3021   5  \n",
      "5698   6  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "print(X_train.head())\n",
    "print('-'*50)\n",
    "print(X_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d64f8821",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9da5f0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(30, activation='relu', input_dim=12))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c484c832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 30)                390       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                620       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 168       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 1,187\n",
      "Trainable params: 1,187\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1e48c197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "4872/4872 [==============================] - 1s 172us/step - loss: 0.6858 - acc: 0.7116\n",
      "Epoch 2/50\n",
      "4872/4872 [==============================] - 0s 25us/step - loss: 0.2517 - acc: 0.9175\n",
      "Epoch 3/50\n",
      "4872/4872 [==============================] - 0s 24us/step - loss: 0.2147 - acc: 0.9288\n",
      "Epoch 4/50\n",
      "4872/4872 [==============================] - 0s 24us/step - loss: 0.2028 - acc: 0.9319\n",
      "Epoch 5/50\n",
      "4872/4872 [==============================] - 0s 24us/step - loss: 0.1999 - acc: 0.9335\n",
      "Epoch 6/50\n",
      "4872/4872 [==============================] - 0s 25us/step - loss: 0.1925 - acc: 0.9347\n",
      "Epoch 7/50\n",
      "4872/4872 [==============================] - 0s 24us/step - loss: 0.1822 - acc: 0.9399\n",
      "Epoch 8/50\n",
      "4872/4872 [==============================] - 0s 27us/step - loss: 0.1757 - acc: 0.9423\n",
      "Epoch 9/50\n",
      "4872/4872 [==============================] - 0s 25us/step - loss: 0.1642 - acc: 0.9456\n",
      "Epoch 10/50\n",
      "4872/4872 [==============================] - 0s 25us/step - loss: 0.1534 - acc: 0.9431\n",
      "Epoch 11/50\n",
      "4872/4872 [==============================] - 0s 24us/step - loss: 0.1364 - acc: 0.9509\n",
      "Epoch 12/50\n",
      "4872/4872 [==============================] - 0s 21us/step - loss: 0.1286 - acc: 0.9514\n",
      "Epoch 13/50\n",
      "4872/4872 [==============================] - 0s 21us/step - loss: 0.1190 - acc: 0.9548\n",
      "Epoch 14/50\n",
      "4872/4872 [==============================] - 0s 25us/step - loss: 0.1143 - acc: 0.9579\n",
      "Epoch 15/50\n",
      "4872/4872 [==============================] - 0s 24us/step - loss: 0.1058 - acc: 0.9626\n",
      "Epoch 16/50\n",
      "4872/4872 [==============================] - 0s 25us/step - loss: 0.1085 - acc: 0.9608\n",
      "Epoch 17/50\n",
      "4872/4872 [==============================] - 0s 22us/step - loss: 0.1003 - acc: 0.9637\n",
      "Epoch 18/50\n",
      "4872/4872 [==============================] - 0s 23us/step - loss: 0.1018 - acc: 0.9631\n",
      "Epoch 19/50\n",
      "4872/4872 [==============================] - 0s 23us/step - loss: 0.0931 - acc: 0.9698\n",
      "Epoch 20/50\n",
      "4872/4872 [==============================] - 0s 22us/step - loss: 0.0858 - acc: 0.9737\n",
      "Epoch 21/50\n",
      "4872/4872 [==============================] - 0s 22us/step - loss: 0.0841 - acc: 0.9719\n",
      "Epoch 22/50\n",
      "4872/4872 [==============================] - 0s 21us/step - loss: 0.0847 - acc: 0.9727\n",
      "Epoch 23/50\n",
      "4872/4872 [==============================] - 0s 21us/step - loss: 0.0791 - acc: 0.9762\n",
      "Epoch 24/50\n",
      "4872/4872 [==============================] - 0s 22us/step - loss: 0.0714 - acc: 0.9772\n",
      "Epoch 25/50\n",
      "4872/4872 [==============================] - 0s 21us/step - loss: 0.0718 - acc: 0.9789\n",
      "Epoch 26/50\n",
      "4872/4872 [==============================] - 0s 21us/step - loss: 0.0752 - acc: 0.9743\n",
      "Epoch 27/50\n",
      "4872/4872 [==============================] - 0s 21us/step - loss: 0.0671 - acc: 0.9807\n",
      "Epoch 28/50\n",
      "4872/4872 [==============================] - 0s 22us/step - loss: 0.0659 - acc: 0.9803\n",
      "Epoch 29/50\n",
      "4872/4872 [==============================] - 0s 23us/step - loss: 0.0686 - acc: 0.9791\n",
      "Epoch 30/50\n",
      "4872/4872 [==============================] - 0s 22us/step - loss: 0.0608 - acc: 0.9813\n",
      "Epoch 31/50\n",
      "4872/4872 [==============================] - 0s 23us/step - loss: 0.0653 - acc: 0.9797\n",
      "Epoch 32/50\n",
      "4872/4872 [==============================] - 0s 21us/step - loss: 0.0595 - acc: 0.9826\n",
      "Epoch 33/50\n",
      "4872/4872 [==============================] - 0s 21us/step - loss: 0.0635 - acc: 0.9776\n",
      "Epoch 34/50\n",
      "4872/4872 [==============================] - 0s 21us/step - loss: 0.0561 - acc: 0.9830\n",
      "Epoch 35/50\n",
      "4872/4872 [==============================] - 0s 21us/step - loss: 0.0572 - acc: 0.9823\n",
      "Epoch 36/50\n",
      "4872/4872 [==============================] - 0s 21us/step - loss: 0.0576 - acc: 0.9821\n",
      "Epoch 37/50\n",
      "4872/4872 [==============================] - 0s 22us/step - loss: 0.0598 - acc: 0.9811\n",
      "Epoch 38/50\n",
      "4872/4872 [==============================] - 0s 20us/step - loss: 0.0544 - acc: 0.9828\n",
      "Epoch 39/50\n",
      "4872/4872 [==============================] - 0s 21us/step - loss: 0.0556 - acc: 0.9836\n",
      "Epoch 40/50\n",
      "4872/4872 [==============================] - 0s 21us/step - loss: 0.0612 - acc: 0.9789\n",
      "Epoch 41/50\n",
      "4872/4872 [==============================] - 0s 21us/step - loss: 0.0542 - acc: 0.9832\n",
      "Epoch 42/50\n",
      "4872/4872 [==============================] - 0s 21us/step - loss: 0.0578 - acc: 0.9828\n",
      "Epoch 43/50\n",
      "4872/4872 [==============================] - 0s 24us/step - loss: 0.0614 - acc: 0.9787\n",
      "Epoch 44/50\n",
      "4872/4872 [==============================] - 0s 24us/step - loss: 0.0541 - acc: 0.9821\n",
      "Epoch 45/50\n",
      "4872/4872 [==============================] - 0s 23us/step - loss: 0.0574 - acc: 0.9803\n",
      "Epoch 46/50\n",
      "4872/4872 [==============================] - 0s 21us/step - loss: 0.0521 - acc: 0.9840\n",
      "Epoch 47/50\n",
      "4872/4872 [==============================] - 0s 21us/step - loss: 0.0509 - acc: 0.9844\n",
      "Epoch 48/50\n",
      "4872/4872 [==============================] - 0s 21us/step - loss: 0.0524 - acc: 0.9826\n",
      "Epoch 49/50\n",
      "4872/4872 [==============================] - 0s 23us/step - loss: 0.0588 - acc: 0.9826\n",
      "Epoch 50/50\n",
      "4872/4872 [==============================] - 0s 22us/step - loss: 0.0514 - acc: 0.9842\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fd6498ef250>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "             metrics=['acc'])\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1acdcae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1625/1625 [==============================] - 0s 82us/step\n",
      "loss: 0.07326306091592862\n",
      "accuracy: 0.9766153693199158\n"
     ]
    }
   ],
   "source": [
    "evaluation = model.evaluate(X_test, y_test)\n",
    "print('loss:', evaluation[0])\n",
    "print('accuracy:', evaluation[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1a833a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(path+'wine.csv', header=None)\n",
    "df = df.sample(frac=1)\n",
    "\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "51cd7893",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(30, activation='relu', input_dim=12))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4023d790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4157 samples, validate on 1040 samples\n",
      "Epoch 1/200\n",
      "4157/4157 [==============================] - 1s 240us/step - loss: 0.7162 - acc: 0.7361 - val_loss: 0.4412 - val_acc: 0.7702\n",
      "Epoch 2/200\n",
      "4157/4157 [==============================] - 0s 21us/step - loss: 0.2996 - acc: 0.8739 - val_loss: 0.1942 - val_acc: 0.9548\n",
      "Epoch 3/200\n",
      "4157/4157 [==============================] - 0s 26us/step - loss: 0.1995 - acc: 0.9399 - val_loss: 0.1604 - val_acc: 0.9510\n",
      "Epoch 4/200\n",
      "4157/4157 [==============================] - 0s 25us/step - loss: 0.1814 - acc: 0.9375 - val_loss: 0.1480 - val_acc: 0.9519\n",
      "Epoch 5/200\n",
      "4157/4157 [==============================] - 0s 23us/step - loss: 0.1739 - acc: 0.9413 - val_loss: 0.1514 - val_acc: 0.9490\n",
      "Epoch 6/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.1707 - acc: 0.9401 - val_loss: 0.1433 - val_acc: 0.9519\n",
      "Epoch 7/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.1647 - acc: 0.9408 - val_loss: 0.1378 - val_acc: 0.9519\n",
      "Epoch 8/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.1608 - acc: 0.9430 - val_loss: 0.1311 - val_acc: 0.9567\n",
      "Epoch 9/200\n",
      "4157/4157 [==============================] - 0s 21us/step - loss: 0.1537 - acc: 0.9442 - val_loss: 0.1268 - val_acc: 0.9587\n",
      "Epoch 10/200\n",
      "4157/4157 [==============================] - 0s 21us/step - loss: 0.1535 - acc: 0.9454 - val_loss: 0.1252 - val_acc: 0.9558\n",
      "Epoch 11/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.1450 - acc: 0.9454 - val_loss: 0.1218 - val_acc: 0.9577\n",
      "Epoch 12/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.1448 - acc: 0.9456 - val_loss: 0.1164 - val_acc: 0.9606\n",
      "Epoch 13/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.1427 - acc: 0.9483 - val_loss: 0.1123 - val_acc: 0.9615\n",
      "Epoch 14/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.1417 - acc: 0.9519 - val_loss: 0.1112 - val_acc: 0.9625\n",
      "Epoch 15/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.1414 - acc: 0.9514 - val_loss: 0.1080 - val_acc: 0.9635\n",
      "Epoch 16/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.1365 - acc: 0.9492 - val_loss: 0.1120 - val_acc: 0.9596\n",
      "Epoch 17/200\n",
      "4157/4157 [==============================] - 0s 21us/step - loss: 0.1340 - acc: 0.9500 - val_loss: 0.1029 - val_acc: 0.9654\n",
      "Epoch 18/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.1281 - acc: 0.9548 - val_loss: 0.1010 - val_acc: 0.9654\n",
      "Epoch 19/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.1268 - acc: 0.9548 - val_loss: 0.1024 - val_acc: 0.9712\n",
      "Epoch 20/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.1277 - acc: 0.9560 - val_loss: 0.1249 - val_acc: 0.9654\n",
      "Epoch 21/200\n",
      "4157/4157 [==============================] - 0s 22us/step - loss: 0.1357 - acc: 0.9545 - val_loss: 0.0970 - val_acc: 0.9702\n",
      "Epoch 22/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.1197 - acc: 0.9601 - val_loss: 0.0942 - val_acc: 0.9721\n",
      "Epoch 23/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.1217 - acc: 0.9608 - val_loss: 0.0930 - val_acc: 0.9721\n",
      "Epoch 24/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.1176 - acc: 0.9615 - val_loss: 0.0942 - val_acc: 0.9731\n",
      "Epoch 25/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.1171 - acc: 0.9608 - val_loss: 0.0958 - val_acc: 0.9769\n",
      "Epoch 26/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.1173 - acc: 0.9581 - val_loss: 0.0879 - val_acc: 0.9721\n",
      "Epoch 27/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.1224 - acc: 0.9581 - val_loss: 0.1057 - val_acc: 0.9644\n",
      "Epoch 28/200\n",
      "4157/4157 [==============================] - 0s 21us/step - loss: 0.1208 - acc: 0.9610 - val_loss: 0.0867 - val_acc: 0.9769\n",
      "Epoch 29/200\n",
      "4157/4157 [==============================] - 0s 22us/step - loss: 0.1090 - acc: 0.9666 - val_loss: 0.0845 - val_acc: 0.9731\n",
      "Epoch 30/200\n",
      "4157/4157 [==============================] - 0s 30us/step - loss: 0.1087 - acc: 0.9663 - val_loss: 0.0845 - val_acc: 0.9779\n",
      "Epoch 31/200\n",
      "4157/4157 [==============================] - 0s 21us/step - loss: 0.1098 - acc: 0.9658 - val_loss: 0.0923 - val_acc: 0.9769\n",
      "Epoch 32/200\n",
      "4157/4157 [==============================] - 0s 22us/step - loss: 0.1073 - acc: 0.9661 - val_loss: 0.0815 - val_acc: 0.9750\n",
      "Epoch 33/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.1108 - acc: 0.9642 - val_loss: 0.0911 - val_acc: 0.9673\n",
      "Epoch 34/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.1116 - acc: 0.9618 - val_loss: 0.0802 - val_acc: 0.9731\n",
      "Epoch 35/200\n",
      "4157/4157 [==============================] - 0s 22us/step - loss: 0.1057 - acc: 0.9670 - val_loss: 0.0849 - val_acc: 0.9692\n",
      "Epoch 36/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.1152 - acc: 0.9620 - val_loss: 0.0804 - val_acc: 0.9788\n",
      "Epoch 37/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.1087 - acc: 0.9642 - val_loss: 0.0937 - val_acc: 0.9769\n",
      "Epoch 38/200\n",
      "4157/4157 [==============================] - 0s 22us/step - loss: 0.1032 - acc: 0.9680 - val_loss: 0.0784 - val_acc: 0.9808\n",
      "Epoch 39/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.1035 - acc: 0.9685 - val_loss: 0.0812 - val_acc: 0.9692\n",
      "Epoch 40/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.1004 - acc: 0.9694 - val_loss: 0.0744 - val_acc: 0.9788\n",
      "Epoch 41/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0970 - acc: 0.9709 - val_loss: 0.0728 - val_acc: 0.9779\n",
      "Epoch 42/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0957 - acc: 0.9731 - val_loss: 0.0723 - val_acc: 0.9808\n",
      "Epoch 43/200\n",
      "4157/4157 [==============================] - 0s 21us/step - loss: 0.0953 - acc: 0.9731 - val_loss: 0.0804 - val_acc: 0.9702\n",
      "Epoch 44/200\n",
      "4157/4157 [==============================] - 0s 22us/step - loss: 0.1095 - acc: 0.9642 - val_loss: 0.0766 - val_acc: 0.9702\n",
      "Epoch 45/200\n",
      "4157/4157 [==============================] - 0s 24us/step - loss: 0.0991 - acc: 0.9680 - val_loss: 0.0704 - val_acc: 0.9769\n",
      "Epoch 46/200\n",
      "4157/4157 [==============================] - 0s 21us/step - loss: 0.1016 - acc: 0.9666 - val_loss: 0.0688 - val_acc: 0.9779\n",
      "Epoch 47/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0935 - acc: 0.9728 - val_loss: 0.0680 - val_acc: 0.9817\n",
      "Epoch 48/200\n",
      "4157/4157 [==============================] - 0s 21us/step - loss: 0.0895 - acc: 0.9752 - val_loss: 0.0707 - val_acc: 0.9837\n",
      "Epoch 49/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0914 - acc: 0.9714 - val_loss: 0.0680 - val_acc: 0.9788\n",
      "Epoch 50/200\n",
      "4157/4157 [==============================] - 0s 21us/step - loss: 0.0930 - acc: 0.9719 - val_loss: 0.0655 - val_acc: 0.9808\n",
      "Epoch 51/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0938 - acc: 0.9711 - val_loss: 0.0654 - val_acc: 0.9817\n",
      "Epoch 52/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0995 - acc: 0.9668 - val_loss: 0.0706 - val_acc: 0.9827\n",
      "Epoch 53/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0905 - acc: 0.9733 - val_loss: 0.0626 - val_acc: 0.9817\n",
      "Epoch 54/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0911 - acc: 0.9690 - val_loss: 0.0676 - val_acc: 0.9827\n",
      "Epoch 55/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0855 - acc: 0.9750 - val_loss: 0.0618 - val_acc: 0.9808\n",
      "Epoch 56/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0866 - acc: 0.9738 - val_loss: 0.0617 - val_acc: 0.9808\n",
      "Epoch 57/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0855 - acc: 0.9757 - val_loss: 0.0641 - val_acc: 0.9846\n",
      "Epoch 58/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0839 - acc: 0.9769 - val_loss: 0.0591 - val_acc: 0.9846\n",
      "Epoch 59/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0908 - acc: 0.9711 - val_loss: 0.0735 - val_acc: 0.9740\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0861 - acc: 0.9726 - val_loss: 0.0617 - val_acc: 0.9837\n",
      "Epoch 61/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0850 - acc: 0.9762 - val_loss: 0.0607 - val_acc: 0.9788\n",
      "Epoch 62/200\n",
      "4157/4157 [==============================] - 0s 26us/step - loss: 0.0845 - acc: 0.9752 - val_loss: 0.0575 - val_acc: 0.9808\n",
      "Epoch 63/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0808 - acc: 0.9762 - val_loss: 0.0740 - val_acc: 0.9827\n",
      "Epoch 64/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0849 - acc: 0.9711 - val_loss: 0.0617 - val_acc: 0.9837\n",
      "Epoch 65/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0802 - acc: 0.9771 - val_loss: 0.0548 - val_acc: 0.9856\n",
      "Epoch 66/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0788 - acc: 0.9767 - val_loss: 0.0575 - val_acc: 0.9856\n",
      "Epoch 67/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0776 - acc: 0.9769 - val_loss: 0.0545 - val_acc: 0.9856\n",
      "Epoch 68/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0801 - acc: 0.9759 - val_loss: 0.0552 - val_acc: 0.9856\n",
      "Epoch 69/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0777 - acc: 0.9774 - val_loss: 0.0532 - val_acc: 0.9865\n",
      "Epoch 70/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0832 - acc: 0.9733 - val_loss: 0.0607 - val_acc: 0.9808\n",
      "Epoch 71/200\n",
      "4157/4157 [==============================] - 0s 18us/step - loss: 0.0869 - acc: 0.9711 - val_loss: 0.0511 - val_acc: 0.9856\n",
      "Epoch 72/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0791 - acc: 0.9762 - val_loss: 0.0547 - val_acc: 0.9865\n",
      "Epoch 73/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0762 - acc: 0.9774 - val_loss: 0.0702 - val_acc: 0.9750\n",
      "Epoch 74/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0869 - acc: 0.9738 - val_loss: 0.0505 - val_acc: 0.9865\n",
      "Epoch 75/200\n",
      "4157/4157 [==============================] - 0s 21us/step - loss: 0.0771 - acc: 0.9788 - val_loss: 0.0508 - val_acc: 0.9856\n",
      "Epoch 76/200\n",
      "4157/4157 [==============================] - 0s 22us/step - loss: 0.0747 - acc: 0.9783 - val_loss: 0.0494 - val_acc: 0.9875\n",
      "Epoch 77/200\n",
      "4157/4157 [==============================] - 0s 22us/step - loss: 0.0823 - acc: 0.9769 - val_loss: 0.0640 - val_acc: 0.9779\n",
      "Epoch 78/200\n",
      "4157/4157 [==============================] - 0s 22us/step - loss: 0.0808 - acc: 0.9762 - val_loss: 0.0484 - val_acc: 0.9865\n",
      "Epoch 79/200\n",
      "4157/4157 [==============================] - 0s 23us/step - loss: 0.0730 - acc: 0.9791 - val_loss: 0.0507 - val_acc: 0.9856\n",
      "Epoch 80/200\n",
      "4157/4157 [==============================] - 0s 23us/step - loss: 0.0764 - acc: 0.9783 - val_loss: 0.0475 - val_acc: 0.9875\n",
      "Epoch 81/200\n",
      "4157/4157 [==============================] - 0s 23us/step - loss: 0.0739 - acc: 0.9786 - val_loss: 0.0523 - val_acc: 0.9837\n",
      "Epoch 82/200\n",
      "4157/4157 [==============================] - 0s 25us/step - loss: 0.0824 - acc: 0.9740 - val_loss: 0.0465 - val_acc: 0.9875\n",
      "Epoch 83/200\n",
      "4157/4157 [==============================] - 0s 25us/step - loss: 0.0762 - acc: 0.9774 - val_loss: 0.0517 - val_acc: 0.9856\n",
      "Epoch 84/200\n",
      "4157/4157 [==============================] - 0s 24us/step - loss: 0.0885 - acc: 0.9719 - val_loss: 0.0585 - val_acc: 0.9846\n",
      "Epoch 85/200\n",
      "4157/4157 [==============================] - 0s 23us/step - loss: 0.0739 - acc: 0.9779 - val_loss: 0.0494 - val_acc: 0.9856\n",
      "Epoch 86/200\n",
      "4157/4157 [==============================] - 0s 21us/step - loss: 0.0694 - acc: 0.9803 - val_loss: 0.0629 - val_acc: 0.9837\n",
      "Epoch 87/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0771 - acc: 0.9769 - val_loss: 0.0545 - val_acc: 0.9856\n",
      "Epoch 88/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0723 - acc: 0.9798 - val_loss: 0.0444 - val_acc: 0.9885\n",
      "Epoch 89/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0690 - acc: 0.9798 - val_loss: 0.0445 - val_acc: 0.9875\n",
      "Epoch 90/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0761 - acc: 0.9781 - val_loss: 0.0433 - val_acc: 0.9885\n",
      "Epoch 91/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0721 - acc: 0.9800 - val_loss: 0.0426 - val_acc: 0.9885\n",
      "Epoch 92/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0713 - acc: 0.9793 - val_loss: 0.0439 - val_acc: 0.9894\n",
      "Epoch 93/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0712 - acc: 0.9793 - val_loss: 0.0488 - val_acc: 0.9856\n",
      "Epoch 94/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0713 - acc: 0.9788 - val_loss: 0.0430 - val_acc: 0.9885\n",
      "Epoch 95/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0682 - acc: 0.9793 - val_loss: 0.0420 - val_acc: 0.9894\n",
      "Epoch 96/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0690 - acc: 0.9803 - val_loss: 0.0541 - val_acc: 0.9885\n",
      "Epoch 97/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0677 - acc: 0.9815 - val_loss: 0.0410 - val_acc: 0.9894\n",
      "Epoch 98/200\n",
      "4157/4157 [==============================] - 0s 23us/step - loss: 0.0678 - acc: 0.9783 - val_loss: 0.0414 - val_acc: 0.9894\n",
      "Epoch 99/200\n",
      "4157/4157 [==============================] - 0s 22us/step - loss: 0.0711 - acc: 0.9810 - val_loss: 0.0405 - val_acc: 0.9885\n",
      "Epoch 100/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0682 - acc: 0.9805 - val_loss: 0.0431 - val_acc: 0.9875\n",
      "Epoch 101/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0671 - acc: 0.9796 - val_loss: 0.0608 - val_acc: 0.9856\n",
      "Epoch 102/200\n",
      "4157/4157 [==============================] - 0s 21us/step - loss: 0.0744 - acc: 0.9793 - val_loss: 0.0405 - val_acc: 0.9894\n",
      "Epoch 103/200\n",
      "4157/4157 [==============================] - 0s 22us/step - loss: 0.0717 - acc: 0.9781 - val_loss: 0.0415 - val_acc: 0.9875\n",
      "Epoch 104/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0665 - acc: 0.9803 - val_loss: 0.0391 - val_acc: 0.9894\n",
      "Epoch 105/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0692 - acc: 0.9803 - val_loss: 0.0406 - val_acc: 0.9885\n",
      "Epoch 106/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0699 - acc: 0.9798 - val_loss: 0.0396 - val_acc: 0.9894\n",
      "Epoch 107/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0729 - acc: 0.9781 - val_loss: 0.0593 - val_acc: 0.9846\n",
      "Epoch 108/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0752 - acc: 0.9774 - val_loss: 0.0401 - val_acc: 0.9885\n",
      "Epoch 109/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0909 - acc: 0.9723 - val_loss: 0.0542 - val_acc: 0.9817\n",
      "Epoch 110/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0812 - acc: 0.9733 - val_loss: 0.0469 - val_acc: 0.9827\n",
      "Epoch 111/200\n",
      "4157/4157 [==============================] - 0s 22us/step - loss: 0.0717 - acc: 0.9779 - val_loss: 0.0383 - val_acc: 0.9894\n",
      "Epoch 112/200\n",
      "4157/4157 [==============================] - 0s 21us/step - loss: 0.0642 - acc: 0.9829 - val_loss: 0.0377 - val_acc: 0.9894\n",
      "Epoch 113/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0658 - acc: 0.9817 - val_loss: 0.0374 - val_acc: 0.9894\n",
      "Epoch 114/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0635 - acc: 0.9822 - val_loss: 0.0419 - val_acc: 0.9894\n",
      "Epoch 115/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0666 - acc: 0.9815 - val_loss: 0.0738 - val_acc: 0.9798\n",
      "Epoch 116/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0710 - acc: 0.9800 - val_loss: 0.0370 - val_acc: 0.9894\n",
      "Epoch 117/200\n",
      "4157/4157 [==============================] - 0s 22us/step - loss: 0.0655 - acc: 0.9805 - val_loss: 0.0385 - val_acc: 0.9875\n",
      "Epoch 118/200\n",
      "4157/4157 [==============================] - 0s 23us/step - loss: 0.0656 - acc: 0.9815 - val_loss: 0.0365 - val_acc: 0.9894\n",
      "Epoch 119/200\n",
      "4157/4157 [==============================] - 0s 22us/step - loss: 0.0644 - acc: 0.9817 - val_loss: 0.0415 - val_acc: 0.9913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/200\n",
      "4157/4157 [==============================] - 0s 22us/step - loss: 0.0633 - acc: 0.9822 - val_loss: 0.0379 - val_acc: 0.9885\n",
      "Epoch 121/200\n",
      "4157/4157 [==============================] - 0s 24us/step - loss: 0.0636 - acc: 0.9822 - val_loss: 0.0414 - val_acc: 0.9904\n",
      "Epoch 122/200\n",
      "4157/4157 [==============================] - 0s 26us/step - loss: 0.0672 - acc: 0.9808 - val_loss: 0.0569 - val_acc: 0.9875\n",
      "Epoch 123/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0644 - acc: 0.9817 - val_loss: 0.0457 - val_acc: 0.9913\n",
      "Epoch 124/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0629 - acc: 0.9827 - val_loss: 0.0361 - val_acc: 0.9894\n",
      "Epoch 125/200\n",
      "4157/4157 [==============================] - 0s 28us/step - loss: 0.0612 - acc: 0.9829 - val_loss: 0.0393 - val_acc: 0.9913\n",
      "Epoch 126/200\n",
      "4157/4157 [==============================] - 0s 21us/step - loss: 0.0699 - acc: 0.9798 - val_loss: 0.0653 - val_acc: 0.9779\n",
      "Epoch 127/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0731 - acc: 0.9779 - val_loss: 0.0436 - val_acc: 0.9904\n",
      "Epoch 128/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0635 - acc: 0.9827 - val_loss: 0.0539 - val_acc: 0.9817\n",
      "Epoch 129/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0733 - acc: 0.9779 - val_loss: 0.0370 - val_acc: 0.9885\n",
      "Epoch 130/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0625 - acc: 0.9829 - val_loss: 0.0360 - val_acc: 0.9913\n",
      "Epoch 131/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0631 - acc: 0.9827 - val_loss: 0.0361 - val_acc: 0.9904\n",
      "Epoch 132/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0666 - acc: 0.9810 - val_loss: 0.0344 - val_acc: 0.9885\n",
      "Epoch 133/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0609 - acc: 0.9815 - val_loss: 0.0338 - val_acc: 0.9904\n",
      "Epoch 134/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0602 - acc: 0.9856 - val_loss: 0.0337 - val_acc: 0.9904\n",
      "Epoch 135/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0603 - acc: 0.9822 - val_loss: 0.0391 - val_acc: 0.9904\n",
      "Epoch 136/200\n",
      "4157/4157 [==============================] - 0s 21us/step - loss: 0.0636 - acc: 0.9824 - val_loss: 0.0442 - val_acc: 0.9904\n",
      "Epoch 137/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0656 - acc: 0.9808 - val_loss: 0.0368 - val_acc: 0.9913\n",
      "Epoch 138/200\n",
      "4157/4157 [==============================] - 0s 21us/step - loss: 0.0622 - acc: 0.9829 - val_loss: 0.0328 - val_acc: 0.9904\n",
      "Epoch 139/200\n",
      "4157/4157 [==============================] - 0s 21us/step - loss: 0.0606 - acc: 0.9829 - val_loss: 0.0331 - val_acc: 0.9904\n",
      "Epoch 140/200\n",
      "4157/4157 [==============================] - 0s 22us/step - loss: 0.0610 - acc: 0.9824 - val_loss: 0.0363 - val_acc: 0.9913\n",
      "Epoch 141/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0623 - acc: 0.9812 - val_loss: 0.0516 - val_acc: 0.9904\n",
      "Epoch 142/200\n",
      "4157/4157 [==============================] - 0s 21us/step - loss: 0.0696 - acc: 0.9803 - val_loss: 0.0773 - val_acc: 0.9692\n",
      "Epoch 143/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0682 - acc: 0.9796 - val_loss: 0.0319 - val_acc: 0.9904\n",
      "Epoch 144/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0595 - acc: 0.9846 - val_loss: 0.0319 - val_acc: 0.9904\n",
      "Epoch 145/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0600 - acc: 0.9834 - val_loss: 0.0399 - val_acc: 0.9856\n",
      "Epoch 146/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0629 - acc: 0.9820 - val_loss: 0.0319 - val_acc: 0.9894\n",
      "Epoch 147/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0596 - acc: 0.9844 - val_loss: 0.0324 - val_acc: 0.9904\n",
      "Epoch 148/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0645 - acc: 0.9812 - val_loss: 0.0400 - val_acc: 0.9856\n",
      "Epoch 149/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0730 - acc: 0.9771 - val_loss: 0.0343 - val_acc: 0.9875\n",
      "Epoch 150/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0601 - acc: 0.9834 - val_loss: 0.0314 - val_acc: 0.9913\n",
      "Epoch 151/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0591 - acc: 0.9827 - val_loss: 0.0322 - val_acc: 0.9913\n",
      "Epoch 152/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0609 - acc: 0.9824 - val_loss: 0.0347 - val_acc: 0.9904\n",
      "Epoch 153/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0594 - acc: 0.9839 - val_loss: 0.0321 - val_acc: 0.9913\n",
      "Epoch 154/200\n",
      "4157/4157 [==============================] - 0s 18us/step - loss: 0.0592 - acc: 0.9829 - val_loss: 0.0308 - val_acc: 0.9913\n",
      "Epoch 155/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0584 - acc: 0.9834 - val_loss: 0.0312 - val_acc: 0.9885\n",
      "Epoch 156/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0612 - acc: 0.9839 - val_loss: 0.0323 - val_acc: 0.9894\n",
      "Epoch 157/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0564 - acc: 0.9841 - val_loss: 0.0335 - val_acc: 0.9923\n",
      "Epoch 158/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0587 - acc: 0.9841 - val_loss: 0.0401 - val_acc: 0.9913\n",
      "Epoch 159/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0637 - acc: 0.9834 - val_loss: 0.0308 - val_acc: 0.9894\n",
      "Epoch 160/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0630 - acc: 0.9824 - val_loss: 0.0429 - val_acc: 0.9923\n",
      "Epoch 161/200\n",
      "4157/4157 [==============================] - 0s 18us/step - loss: 0.0606 - acc: 0.9832 - val_loss: 0.0333 - val_acc: 0.9923\n",
      "Epoch 162/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0584 - acc: 0.9841 - val_loss: 0.0345 - val_acc: 0.9865\n",
      "Epoch 163/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0567 - acc: 0.9841 - val_loss: 0.0312 - val_acc: 0.9894\n",
      "Epoch 164/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0694 - acc: 0.9815 - val_loss: 0.0317 - val_acc: 0.9894\n",
      "Epoch 165/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0580 - acc: 0.9846 - val_loss: 0.0301 - val_acc: 0.9923\n",
      "Epoch 166/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0576 - acc: 0.9846 - val_loss: 0.0316 - val_acc: 0.9894\n",
      "Epoch 167/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0594 - acc: 0.9834 - val_loss: 0.0370 - val_acc: 0.9856\n",
      "Epoch 168/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0660 - acc: 0.9808 - val_loss: 0.0449 - val_acc: 0.9856\n",
      "Epoch 169/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0615 - acc: 0.9820 - val_loss: 0.0292 - val_acc: 0.9894\n",
      "Epoch 170/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0576 - acc: 0.9844 - val_loss: 0.0312 - val_acc: 0.9923\n",
      "Epoch 171/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0570 - acc: 0.9829 - val_loss: 0.0294 - val_acc: 0.9894\n",
      "Epoch 172/200\n",
      "4157/4157 [==============================] - 0s 21us/step - loss: 0.0615 - acc: 0.9815 - val_loss: 0.0344 - val_acc: 0.9856\n",
      "Epoch 173/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0593 - acc: 0.9846 - val_loss: 0.0297 - val_acc: 0.9904\n",
      "Epoch 174/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0586 - acc: 0.9836 - val_loss: 0.0291 - val_acc: 0.9904\n",
      "Epoch 175/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0611 - acc: 0.9827 - val_loss: 0.0296 - val_acc: 0.9923\n",
      "Epoch 176/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0565 - acc: 0.9841 - val_loss: 0.0519 - val_acc: 0.9894\n",
      "Epoch 177/200\n",
      "4157/4157 [==============================] - 0s 21us/step - loss: 0.0564 - acc: 0.9848 - val_loss: 0.0289 - val_acc: 0.9894\n",
      "Epoch 178/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0598 - acc: 0.9812 - val_loss: 0.0296 - val_acc: 0.9885\n",
      "Epoch 179/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0576 - acc: 0.9836 - val_loss: 0.0305 - val_acc: 0.9913\n",
      "Epoch 180/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0607 - acc: 0.9832 - val_loss: 0.0518 - val_acc: 0.9808\n",
      "Epoch 181/200\n",
      "4157/4157 [==============================] - 0s 18us/step - loss: 0.0719 - acc: 0.9805 - val_loss: 0.0336 - val_acc: 0.9856\n",
      "Epoch 182/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0596 - acc: 0.9834 - val_loss: 0.0304 - val_acc: 0.9913\n",
      "Epoch 183/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0570 - acc: 0.9844 - val_loss: 0.0307 - val_acc: 0.9923\n",
      "Epoch 184/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0557 - acc: 0.9839 - val_loss: 0.0345 - val_acc: 0.9856\n",
      "Epoch 185/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0643 - acc: 0.9820 - val_loss: 0.0302 - val_acc: 0.9894\n",
      "Epoch 186/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0600 - acc: 0.9848 - val_loss: 0.0314 - val_acc: 0.9923\n",
      "Epoch 187/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0609 - acc: 0.9829 - val_loss: 0.0372 - val_acc: 0.9923\n",
      "Epoch 188/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0603 - acc: 0.9834 - val_loss: 0.0313 - val_acc: 0.9933\n",
      "Epoch 189/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0569 - acc: 0.9836 - val_loss: 0.0355 - val_acc: 0.9913\n",
      "Epoch 190/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0565 - acc: 0.9856 - val_loss: 0.0350 - val_acc: 0.9913\n",
      "Epoch 191/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0576 - acc: 0.9853 - val_loss: 0.0381 - val_acc: 0.9894\n",
      "Epoch 192/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0589 - acc: 0.9844 - val_loss: 0.0290 - val_acc: 0.9885\n",
      "Epoch 193/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0563 - acc: 0.9846 - val_loss: 0.0311 - val_acc: 0.9923\n",
      "Epoch 194/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0551 - acc: 0.9841 - val_loss: 0.0290 - val_acc: 0.9913\n",
      "Epoch 195/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0554 - acc: 0.9841 - val_loss: 0.0316 - val_acc: 0.9933\n",
      "Epoch 196/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0560 - acc: 0.9851 - val_loss: 0.0285 - val_acc: 0.9913\n",
      "Epoch 197/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0549 - acc: 0.9848 - val_loss: 0.0366 - val_acc: 0.9933\n",
      "Epoch 198/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0581 - acc: 0.9844 - val_loss: 0.0362 - val_acc: 0.9856\n",
      "Epoch 199/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0563 - acc: 0.9846 - val_loss: 0.0361 - val_acc: 0.9923\n",
      "Epoch 200/200\n",
      "4157/4157 [==============================] - 0s 25us/step - loss: 0.0564 - acc: 0.9853 - val_loss: 0.0286 - val_acc: 0.9942\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', \n",
    "             metrics=['acc'])\n",
    "history = model.fit(X_train, y_train, epochs=200, batch_size=200,\n",
    "                   validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4caf4657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy of each epoch: [0.77, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.98, 0.97, 0.96, 0.98, 0.97, 0.98, 0.98, 0.98, 0.97, 0.97, 0.97, 0.98, 0.98, 0.98, 0.97, 0.98, 0.98, 0.98, 0.97, 0.97, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.97, 0.98, 0.98, 0.98, 0.98, 0.98, 0.99, 0.99, 0.99, 0.99, 0.99, 0.98, 0.99, 0.99, 0.98, 0.99, 0.99, 0.99, 0.98, 0.99, 0.99, 0.99, 0.98, 0.99, 0.99, 0.98, 0.99, 0.98, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.98, 0.99, 0.98, 0.98, 0.99, 0.99, 0.99, 0.99, 0.98, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.98, 0.99, 0.98, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.97, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.98, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "val_acc = [round(va, 2) for va in val_acc]\n",
    "print('Validation accuracy of each epoch:', val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "41084c4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = range(1, len(acc)+1)\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.plot(epochs, acc, 'bo', label='Training Acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation Acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bf0f3937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1300/1300 [==============================] - 0s 33us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.983846127986908"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5090057b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "model.save(save_path+'my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "41907886",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "loaded_model = load_model(save_path+'my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "37b3ce96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4157 samples, validate on 1040 samples\n",
      "Epoch 1/200\n",
      "4157/4157 [==============================] - 0s 23us/step - loss: 0.0568 - acc: 0.9834 - val_loss: 0.0287 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.02874, saving model to /Users/jsha/gjai/nlp/pytest/temp_modelmy_model.h5\n",
      "Epoch 2/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0536 - acc: 0.9846 - val_loss: 0.0272 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.02874 to 0.02723, saving model to /Users/jsha/gjai/nlp/pytest/temp_modelmy_model.h5\n",
      "Epoch 3/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0556 - acc: 0.9848 - val_loss: 0.0280 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.02723\n",
      "Epoch 4/200\n",
      "4157/4157 [==============================] - 0s 18us/step - loss: 0.0548 - acc: 0.9851 - val_loss: 0.0383 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.02723\n",
      "Epoch 5/200\n",
      "4157/4157 [==============================] - 0s 18us/step - loss: 0.0561 - acc: 0.9839 - val_loss: 0.0294 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.02723\n",
      "Epoch 6/200\n",
      "4157/4157 [==============================] - 0s 21us/step - loss: 0.0553 - acc: 0.9848 - val_loss: 0.0280 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.02723\n",
      "Epoch 7/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0569 - acc: 0.9844 - val_loss: 0.0266 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.02723 to 0.02664, saving model to /Users/jsha/gjai/nlp/pytest/temp_modelmy_model.h5\n",
      "Epoch 8/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0565 - acc: 0.9848 - val_loss: 0.0331 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.02664\n",
      "Epoch 9/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0548 - acc: 0.9844 - val_loss: 0.0589 - val_acc: 0.9817\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.02664\n",
      "Epoch 10/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0671 - acc: 0.9827 - val_loss: 0.0281 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.02664\n",
      "Epoch 11/200\n",
      "4157/4157 [==============================] - 0s 21us/step - loss: 0.0567 - acc: 0.9836 - val_loss: 0.0286 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.02664\n",
      "Epoch 12/200\n",
      "4157/4157 [==============================] - 0s 23us/step - loss: 0.0587 - acc: 0.9839 - val_loss: 0.0319 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.02664\n",
      "Epoch 13/200\n",
      "4157/4157 [==============================] - 0s 22us/step - loss: 0.0554 - acc: 0.9844 - val_loss: 0.0268 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.02664\n",
      "Epoch 14/200\n",
      "4157/4157 [==============================] - 0s 22us/step - loss: 0.0548 - acc: 0.9841 - val_loss: 0.0303 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.02664\n",
      "Epoch 15/200\n",
      "4157/4157 [==============================] - 0s 22us/step - loss: 0.0569 - acc: 0.9858 - val_loss: 0.0283 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.02664\n",
      "Epoch 16/200\n",
      "4157/4157 [==============================] - 0s 23us/step - loss: 0.0552 - acc: 0.9839 - val_loss: 0.0276 - val_acc: 0.9942\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.02664\n",
      "Epoch 17/200\n",
      "4157/4157 [==============================] - 0s 21us/step - loss: 0.0545 - acc: 0.9863 - val_loss: 0.0327 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.02664\n",
      "Epoch 18/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0555 - acc: 0.9846 - val_loss: 0.0316 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.02664\n",
      "Epoch 19/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0559 - acc: 0.9839 - val_loss: 0.0262 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.02664 to 0.02620, saving model to /Users/jsha/gjai/nlp/pytest/temp_modelmy_model.h5\n",
      "Epoch 20/200\n",
      "4157/4157 [==============================] - 0s 18us/step - loss: 0.0637 - acc: 0.9815 - val_loss: 0.0371 - val_acc: 0.9875\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.02620\n",
      "Epoch 21/200\n",
      "4157/4157 [==============================] - 0s 18us/step - loss: 0.0622 - acc: 0.9815 - val_loss: 0.0266 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.02620\n",
      "Epoch 22/200\n",
      "4157/4157 [==============================] - 0s 18us/step - loss: 0.0531 - acc: 0.9860 - val_loss: 0.0267 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.02620\n",
      "Epoch 23/200\n",
      "4157/4157 [==============================] - 0s 21us/step - loss: 0.0540 - acc: 0.9856 - val_loss: 0.0252 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.02620 to 0.02521, saving model to /Users/jsha/gjai/nlp/pytest/temp_modelmy_model.h5\n",
      "Epoch 24/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0550 - acc: 0.9841 - val_loss: 0.0276 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.02521\n",
      "Epoch 25/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0523 - acc: 0.9851 - val_loss: 0.0271 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.02521\n",
      "Epoch 26/200\n",
      "4157/4157 [==============================] - 0s 18us/step - loss: 0.0577 - acc: 0.9841 - val_loss: 0.0306 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.02521\n",
      "Epoch 27/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0638 - acc: 0.9810 - val_loss: 0.0251 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.02521 to 0.02511, saving model to /Users/jsha/gjai/nlp/pytest/temp_modelmy_model.h5\n",
      "Epoch 28/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0612 - acc: 0.9829 - val_loss: 0.0286 - val_acc: 0.9885\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.02511\n",
      "Epoch 29/200\n",
      "4157/4157 [==============================] - 0s 18us/step - loss: 0.0590 - acc: 0.9844 - val_loss: 0.0315 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.02511\n",
      "Epoch 30/200\n",
      "4157/4157 [==============================] - 0s 18us/step - loss: 0.0559 - acc: 0.9851 - val_loss: 0.0305 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.02511\n",
      "Epoch 31/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0542 - acc: 0.9844 - val_loss: 0.0287 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.02511\n",
      "Epoch 32/200\n",
      "4157/4157 [==============================] - 0s 18us/step - loss: 0.0514 - acc: 0.9856 - val_loss: 0.0345 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.02511\n",
      "Epoch 33/200\n",
      "4157/4157 [==============================] - 0s 18us/step - loss: 0.0537 - acc: 0.9856 - val_loss: 0.0345 - val_acc: 0.9885\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.02511\n",
      "Epoch 34/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0554 - acc: 0.9846 - val_loss: 0.0298 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.02511\n",
      "Epoch 35/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0568 - acc: 0.9829 - val_loss: 0.0258 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.02511\n",
      "Epoch 36/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0567 - acc: 0.9841 - val_loss: 0.0295 - val_acc: 0.9885\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.02511\n",
      "Epoch 37/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0562 - acc: 0.9858 - val_loss: 0.0310 - val_acc: 0.9885\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.02511\n",
      "Epoch 38/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0604 - acc: 0.9824 - val_loss: 0.0324 - val_acc: 0.9865\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.02511\n",
      "Epoch 39/200\n",
      "4157/4157 [==============================] - 0s 22us/step - loss: 0.0579 - acc: 0.9832 - val_loss: 0.0260 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.02511\n",
      "Epoch 40/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0560 - acc: 0.9853 - val_loss: 0.0291 - val_acc: 0.9894\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.02511\n",
      "Epoch 41/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0638 - acc: 0.9805 - val_loss: 0.0282 - val_acc: 0.9885\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.02511\n",
      "Epoch 42/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0558 - acc: 0.9856 - val_loss: 0.0423 - val_acc: 0.9875\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.02511\n",
      "Epoch 43/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0576 - acc: 0.9839 - val_loss: 0.0590 - val_acc: 0.9817\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.02511\n",
      "Epoch 44/200\n",
      "4157/4157 [==============================] - 0s 21us/step - loss: 0.0556 - acc: 0.9834 - val_loss: 0.0405 - val_acc: 0.9894\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.02511\n",
      "Epoch 45/200\n",
      "4157/4157 [==============================] - 0s 22us/step - loss: 0.0523 - acc: 0.9853 - val_loss: 0.0254 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.02511\n",
      "Epoch 46/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0516 - acc: 0.9853 - val_loss: 0.0283 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.02511\n",
      "Epoch 47/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0555 - acc: 0.9848 - val_loss: 0.0304 - val_acc: 0.9856\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.02511\n",
      "Epoch 48/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0557 - acc: 0.9839 - val_loss: 0.0372 - val_acc: 0.9846\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.02511\n",
      "Epoch 49/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0593 - acc: 0.9841 - val_loss: 0.0276 - val_acc: 0.9885\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.02511\n",
      "Epoch 50/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0533 - acc: 0.9858 - val_loss: 0.0363 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.02511\n",
      "Epoch 51/200\n",
      "4157/4157 [==============================] - 0s 18us/step - loss: 0.0598 - acc: 0.9824 - val_loss: 0.0255 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.02511\n",
      "Epoch 52/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0572 - acc: 0.9839 - val_loss: 0.0258 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.02511\n",
      "Epoch 53/200\n",
      "4157/4157 [==============================] - 0s 18us/step - loss: 0.0569 - acc: 0.9839 - val_loss: 0.0314 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.02511\n",
      "Epoch 54/200\n",
      "4157/4157 [==============================] - 0s 18us/step - loss: 0.0527 - acc: 0.9853 - val_loss: 0.0333 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.02511\n",
      "Epoch 55/200\n",
      "4157/4157 [==============================] - 0s 18us/step - loss: 0.0633 - acc: 0.9817 - val_loss: 0.0376 - val_acc: 0.9875\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.02511\n",
      "Epoch 56/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0550 - acc: 0.9846 - val_loss: 0.0268 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.02511\n",
      "Epoch 57/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0518 - acc: 0.9865 - val_loss: 0.0350 - val_acc: 0.9894\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.02511\n",
      "Epoch 58/200\n",
      "4157/4157 [==============================] - 0s 21us/step - loss: 0.0527 - acc: 0.9858 - val_loss: 0.0252 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.02511\n",
      "Epoch 59/200\n",
      "4157/4157 [==============================] - 0s 21us/step - loss: 0.0506 - acc: 0.9858 - val_loss: 0.0316 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.02511\n",
      "Epoch 60/200\n",
      "4157/4157 [==============================] - 0s 18us/step - loss: 0.0512 - acc: 0.9868 - val_loss: 0.0239 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.02511 to 0.02393, saving model to /Users/jsha/gjai/nlp/pytest/temp_modelmy_model.h5\n",
      "Epoch 61/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0531 - acc: 0.9865 - val_loss: 0.0247 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.02393\n",
      "Epoch 62/200\n",
      "4157/4157 [==============================] - 0s 21us/step - loss: 0.0528 - acc: 0.9853 - val_loss: 0.0579 - val_acc: 0.9798\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.02393\n",
      "Epoch 63/200\n",
      "4157/4157 [==============================] - 0s 22us/step - loss: 0.0609 - acc: 0.9817 - val_loss: 0.0395 - val_acc: 0.9875\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.02393\n",
      "Epoch 64/200\n",
      "4157/4157 [==============================] - 0s 22us/step - loss: 0.0537 - acc: 0.9846 - val_loss: 0.0287 - val_acc: 0.9885\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.02393\n",
      "Epoch 65/200\n",
      "4157/4157 [==============================] - 0s 22us/step - loss: 0.0566 - acc: 0.9844 - val_loss: 0.0266 - val_acc: 0.9894\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.02393\n",
      "Epoch 66/200\n",
      "4157/4157 [==============================] - 0s 22us/step - loss: 0.0523 - acc: 0.9868 - val_loss: 0.0304 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.02393\n",
      "Epoch 67/200\n",
      "4157/4157 [==============================] - 0s 22us/step - loss: 0.0506 - acc: 0.9865 - val_loss: 0.0250 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.02393\n",
      "Epoch 68/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0523 - acc: 0.9856 - val_loss: 0.0622 - val_acc: 0.9769\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.02393\n",
      "Epoch 69/200\n",
      "4157/4157 [==============================] - 0s 21us/step - loss: 0.0541 - acc: 0.9860 - val_loss: 0.0255 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.02393\n",
      "Epoch 70/200\n",
      "4157/4157 [==============================] - 0s 21us/step - loss: 0.0566 - acc: 0.9844 - val_loss: 0.0256 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.02393\n",
      "Epoch 71/200\n",
      "4157/4157 [==============================] - 0s 21us/step - loss: 0.0504 - acc: 0.9868 - val_loss: 0.0262 - val_acc: 0.9942\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.02393\n",
      "Epoch 72/200\n",
      "4157/4157 [==============================] - 0s 26us/step - loss: 0.0531 - acc: 0.9851 - val_loss: 0.0388 - val_acc: 0.9885\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.02393\n",
      "Epoch 73/200\n",
      "4157/4157 [==============================] - 0s 22us/step - loss: 0.0722 - acc: 0.9781 - val_loss: 0.0274 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.02393\n",
      "Epoch 74/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0544 - acc: 0.9836 - val_loss: 0.0483 - val_acc: 0.9846\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.02393\n",
      "Epoch 75/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0588 - acc: 0.9839 - val_loss: 0.0288 - val_acc: 0.9942\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.02393\n",
      "Epoch 76/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0510 - acc: 0.9865 - val_loss: 0.0253 - val_acc: 0.9942\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.02393\n",
      "Epoch 77/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0499 - acc: 0.9868 - val_loss: 0.0249 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.02393\n",
      "Epoch 78/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0495 - acc: 0.9870 - val_loss: 0.0261 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.02393\n",
      "Epoch 79/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0492 - acc: 0.9868 - val_loss: 0.0248 - val_acc: 0.9942\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.02393\n",
      "Epoch 80/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0493 - acc: 0.9863 - val_loss: 0.0270 - val_acc: 0.9952\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.02393\n",
      "Epoch 81/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0530 - acc: 0.9856 - val_loss: 0.0336 - val_acc: 0.9894\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.02393\n",
      "Epoch 82/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0492 - acc: 0.9885 - val_loss: 0.0254 - val_acc: 0.9942\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.02393\n",
      "Epoch 83/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0489 - acc: 0.9858 - val_loss: 0.0436 - val_acc: 0.9885\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.02393\n",
      "Epoch 84/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0502 - acc: 0.9856 - val_loss: 0.0249 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.02393\n",
      "Epoch 85/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0481 - acc: 0.9880 - val_loss: 0.0261 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.02393\n",
      "Epoch 86/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0492 - acc: 0.9858 - val_loss: 0.0279 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.02393\n",
      "Epoch 87/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0488 - acc: 0.9865 - val_loss: 0.0239 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.02393 to 0.02387, saving model to /Users/jsha/gjai/nlp/pytest/temp_modelmy_model.h5\n",
      "Epoch 88/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0520 - acc: 0.9860 - val_loss: 0.0275 - val_acc: 0.9894\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.02387\n",
      "Epoch 89/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0520 - acc: 0.9858 - val_loss: 0.0253 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.02387\n",
      "Epoch 90/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0525 - acc: 0.9860 - val_loss: 0.0262 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.02387\n",
      "Epoch 91/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0499 - acc: 0.9875 - val_loss: 0.0256 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.02387\n",
      "Epoch 92/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0486 - acc: 0.9863 - val_loss: 0.0238 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.02387 to 0.02380, saving model to /Users/jsha/gjai/nlp/pytest/temp_modelmy_model.h5\n",
      "Epoch 93/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0485 - acc: 0.9863 - val_loss: 0.0246 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.02380\n",
      "Epoch 94/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0524 - acc: 0.9858 - val_loss: 0.0273 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.02380\n",
      "Epoch 95/200\n",
      "4157/4157 [==============================] - 0s 18us/step - loss: 0.0493 - acc: 0.9868 - val_loss: 0.0321 - val_acc: 0.9894\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.02380\n",
      "Epoch 96/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0586 - acc: 0.9834 - val_loss: 0.0287 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.02380\n",
      "Epoch 97/200\n",
      "4157/4157 [==============================] - 0s 32us/step - loss: 0.0501 - acc: 0.9865 - val_loss: 0.0245 - val_acc: 0.9942\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.02380\n",
      "Epoch 98/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0478 - acc: 0.9873 - val_loss: 0.0260 - val_acc: 0.9942\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.02380\n",
      "Epoch 99/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0507 - acc: 0.9858 - val_loss: 0.0242 - val_acc: 0.9942\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.02380\n",
      "Epoch 100/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0508 - acc: 0.9863 - val_loss: 0.0254 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.02380\n",
      "Epoch 101/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0479 - acc: 0.9856 - val_loss: 0.0267 - val_acc: 0.9942\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.02380\n",
      "Epoch 102/200\n",
      "4157/4157 [==============================] - 0s 18us/step - loss: 0.0488 - acc: 0.9865 - val_loss: 0.0237 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.02380 to 0.02372, saving model to /Users/jsha/gjai/nlp/pytest/temp_modelmy_model.h5\n",
      "Epoch 103/200\n",
      "4157/4157 [==============================] - 0s 18us/step - loss: 0.0494 - acc: 0.9853 - val_loss: 0.0243 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.02372\n",
      "Epoch 104/200\n",
      "4157/4157 [==============================] - 0s 18us/step - loss: 0.0513 - acc: 0.9858 - val_loss: 0.0222 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.02372 to 0.02221, saving model to /Users/jsha/gjai/nlp/pytest/temp_modelmy_model.h5\n",
      "Epoch 105/200\n",
      "4157/4157 [==============================] - 0s 18us/step - loss: 0.0473 - acc: 0.9880 - val_loss: 0.0274 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.02221\n",
      "Epoch 106/200\n",
      "4157/4157 [==============================] - 0s 18us/step - loss: 0.0590 - acc: 0.9810 - val_loss: 0.0396 - val_acc: 0.9875\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.02221\n",
      "Epoch 107/200\n",
      "4157/4157 [==============================] - 0s 18us/step - loss: 0.0599 - acc: 0.9832 - val_loss: 0.0625 - val_acc: 0.9788\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.02221\n",
      "Epoch 108/200\n",
      "4157/4157 [==============================] - 0s 18us/step - loss: 0.0510 - acc: 0.9858 - val_loss: 0.0257 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.02221\n",
      "Epoch 109/200\n",
      "4157/4157 [==============================] - 0s 18us/step - loss: 0.0494 - acc: 0.9858 - val_loss: 0.0279 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.02221\n",
      "Epoch 110/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0494 - acc: 0.9870 - val_loss: 0.0366 - val_acc: 0.9894\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.02221\n",
      "Epoch 111/200\n",
      "4157/4157 [==============================] - 0s 18us/step - loss: 0.0500 - acc: 0.9863 - val_loss: 0.0264 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.02221\n",
      "Epoch 112/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0510 - acc: 0.9856 - val_loss: 0.0239 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.02221\n",
      "Epoch 113/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0500 - acc: 0.9870 - val_loss: 0.0234 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.02221\n",
      "Epoch 114/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0503 - acc: 0.9873 - val_loss: 0.0389 - val_acc: 0.9885\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.02221\n",
      "Epoch 115/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0500 - acc: 0.9868 - val_loss: 0.0248 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.02221\n",
      "Epoch 116/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0519 - acc: 0.9853 - val_loss: 0.0230 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.02221\n",
      "Epoch 117/200\n",
      "4157/4157 [==============================] - 0s 18us/step - loss: 0.0480 - acc: 0.9880 - val_loss: 0.0298 - val_acc: 0.9894\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.02221\n",
      "Epoch 118/200\n",
      "4157/4157 [==============================] - 0s 18us/step - loss: 0.0507 - acc: 0.9863 - val_loss: 0.0256 - val_acc: 0.9885\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.02221\n",
      "Epoch 119/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0492 - acc: 0.9868 - val_loss: 0.0235 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.02221\n",
      "Epoch 120/200\n",
      "4157/4157 [==============================] - 0s 25us/step - loss: 0.0465 - acc: 0.9894 - val_loss: 0.0236 - val_acc: 0.9942\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.02221\n",
      "Epoch 121/200\n",
      "4157/4157 [==============================] - 0s 22us/step - loss: 0.0458 - acc: 0.9882 - val_loss: 0.0255 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.02221\n",
      "Epoch 122/200\n",
      "4157/4157 [==============================] - 0s 21us/step - loss: 0.0527 - acc: 0.9841 - val_loss: 0.0266 - val_acc: 0.9894\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.02221\n",
      "Epoch 123/200\n",
      "4157/4157 [==============================] - 0s 21us/step - loss: 0.0636 - acc: 0.9788 - val_loss: 0.0234 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.02221\n",
      "Epoch 124/200\n",
      "4157/4157 [==============================] - 0s 21us/step - loss: 0.0563 - acc: 0.9839 - val_loss: 0.0232 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.02221\n",
      "Epoch 125/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0496 - acc: 0.9865 - val_loss: 0.0287 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.02221\n",
      "Epoch 126/200\n",
      "4157/4157 [==============================] - 0s 22us/step - loss: 0.0501 - acc: 0.9877 - val_loss: 0.0247 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.02221\n",
      "Epoch 127/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4157/4157 [==============================] - 0s 23us/step - loss: 0.0475 - acc: 0.9880 - val_loss: 0.0303 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.02221\n",
      "Epoch 128/200\n",
      "4157/4157 [==============================] - 0s 23us/step - loss: 0.0464 - acc: 0.9870 - val_loss: 0.0243 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.02221\n",
      "Epoch 129/200\n",
      "4157/4157 [==============================] - 0s 22us/step - loss: 0.0477 - acc: 0.9868 - val_loss: 0.0243 - val_acc: 0.9942\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.02221\n",
      "Epoch 130/200\n",
      "4157/4157 [==============================] - 0s 22us/step - loss: 0.0484 - acc: 0.9868 - val_loss: 0.0321 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.02221\n",
      "Epoch 131/200\n",
      "4157/4157 [==============================] - 0s 22us/step - loss: 0.0477 - acc: 0.9875 - val_loss: 0.0244 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.02221\n",
      "Epoch 132/200\n",
      "4157/4157 [==============================] - 0s 21us/step - loss: 0.0486 - acc: 0.9873 - val_loss: 0.0251 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.02221\n",
      "Epoch 133/200\n",
      "4157/4157 [==============================] - 0s 21us/step - loss: 0.0523 - acc: 0.9858 - val_loss: 0.0233 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.02221\n",
      "Epoch 134/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0513 - acc: 0.9860 - val_loss: 0.0434 - val_acc: 0.9846\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.02221\n",
      "Epoch 135/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0555 - acc: 0.9827 - val_loss: 0.0249 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.02221\n",
      "Epoch 136/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0525 - acc: 0.9848 - val_loss: 0.0223 - val_acc: 0.9942\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.02221\n",
      "Epoch 137/200\n",
      "4157/4157 [==============================] - 0s 21us/step - loss: 0.0476 - acc: 0.9860 - val_loss: 0.0246 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.02221\n",
      "Epoch 138/200\n",
      "4157/4157 [==============================] - 0s 21us/step - loss: 0.0488 - acc: 0.9863 - val_loss: 0.0224 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.02221\n",
      "Epoch 139/200\n",
      "4157/4157 [==============================] - 0s 21us/step - loss: 0.0500 - acc: 0.9868 - val_loss: 0.0274 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.02221\n",
      "Epoch 140/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0564 - acc: 0.9827 - val_loss: 0.0305 - val_acc: 0.9894\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.02221\n",
      "Epoch 141/200\n",
      "4157/4157 [==============================] - 0s 21us/step - loss: 0.0595 - acc: 0.9812 - val_loss: 0.0405 - val_acc: 0.9865\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.02221\n",
      "Epoch 142/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0678 - acc: 0.9769 - val_loss: 0.0243 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.02221\n",
      "Epoch 143/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0508 - acc: 0.9860 - val_loss: 0.0295 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.02221\n",
      "Epoch 144/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0472 - acc: 0.9865 - val_loss: 0.0238 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.02221\n",
      "Epoch 145/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0505 - acc: 0.9860 - val_loss: 0.0244 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.02221\n",
      "Epoch 146/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0528 - acc: 0.9851 - val_loss: 0.0305 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.02221\n",
      "Epoch 147/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0488 - acc: 0.9870 - val_loss: 0.0212 - val_acc: 0.9942\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.02221 to 0.02119, saving model to /Users/jsha/gjai/nlp/pytest/temp_modelmy_model.h5\n",
      "Epoch 148/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0484 - acc: 0.9868 - val_loss: 0.0220 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.02119\n",
      "Epoch 149/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0458 - acc: 0.9875 - val_loss: 0.0227 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.02119\n",
      "Epoch 150/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0463 - acc: 0.9868 - val_loss: 0.0216 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.02119\n",
      "Epoch 151/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0467 - acc: 0.9868 - val_loss: 0.0246 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.02119\n",
      "Epoch 152/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0480 - acc: 0.9873 - val_loss: 0.0360 - val_acc: 0.9885\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.02119\n",
      "Epoch 153/200\n",
      "4157/4157 [==============================] - 0s 21us/step - loss: 0.0463 - acc: 0.9882 - val_loss: 0.0232 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.02119\n",
      "Epoch 154/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0498 - acc: 0.9880 - val_loss: 0.0215 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.02119\n",
      "Epoch 155/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0570 - acc: 0.9836 - val_loss: 0.0216 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.02119\n",
      "Epoch 156/200\n",
      "4157/4157 [==============================] - 0s 21us/step - loss: 0.0484 - acc: 0.9885 - val_loss: 0.0214 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.02119\n",
      "Epoch 157/200\n",
      "4157/4157 [==============================] - 0s 30us/step - loss: 0.0483 - acc: 0.9868 - val_loss: 0.0218 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.02119\n",
      "Epoch 158/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0461 - acc: 0.9885 - val_loss: 0.0224 - val_acc: 0.9942\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.02119\n",
      "Epoch 159/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0462 - acc: 0.9877 - val_loss: 0.0310 - val_acc: 0.9894\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.02119\n",
      "Epoch 160/200\n",
      "4157/4157 [==============================] - 0s 21us/step - loss: 0.0480 - acc: 0.9875 - val_loss: 0.0292 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.02119\n",
      "Epoch 161/200\n",
      "4157/4157 [==============================] - 0s 21us/step - loss: 0.0510 - acc: 0.9851 - val_loss: 0.0348 - val_acc: 0.9885\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.02119\n",
      "Epoch 162/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0464 - acc: 0.9877 - val_loss: 0.0238 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.02119\n",
      "Epoch 163/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0471 - acc: 0.9885 - val_loss: 0.0289 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.02119\n",
      "Epoch 164/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0478 - acc: 0.9882 - val_loss: 0.0218 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.02119\n",
      "Epoch 165/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0458 - acc: 0.9877 - val_loss: 0.0239 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.02119\n",
      "Epoch 166/200\n",
      "4157/4157 [==============================] - 0s 23us/step - loss: 0.0464 - acc: 0.9875 - val_loss: 0.0302 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.02119\n",
      "Epoch 167/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0476 - acc: 0.9868 - val_loss: 0.0221 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.02119\n",
      "Epoch 168/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0478 - acc: 0.9875 - val_loss: 0.0233 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.02119\n",
      "Epoch 169/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0478 - acc: 0.9858 - val_loss: 0.0276 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.02119\n",
      "Epoch 170/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0457 - acc: 0.9873 - val_loss: 0.0224 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.02119\n",
      "Epoch 171/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0476 - acc: 0.9875 - val_loss: 0.0226 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.02119\n",
      "Epoch 172/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0446 - acc: 0.9875 - val_loss: 0.0206 - val_acc: 0.9942\n",
      "\n",
      "Epoch 00172: val_loss improved from 0.02119 to 0.02061, saving model to /Users/jsha/gjai/nlp/pytest/temp_modelmy_model.h5\n",
      "Epoch 173/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0465 - acc: 0.9889 - val_loss: 0.0254 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.02061\n",
      "Epoch 174/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0480 - acc: 0.9877 - val_loss: 0.0293 - val_acc: 0.9913\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.02061\n",
      "Epoch 175/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0456 - acc: 0.9887 - val_loss: 0.0221 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.02061\n",
      "Epoch 176/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0451 - acc: 0.9885 - val_loss: 0.0248 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.02061\n",
      "Epoch 177/200\n",
      "4157/4157 [==============================] - 0s 21us/step - loss: 0.0507 - acc: 0.9863 - val_loss: 0.0202 - val_acc: 0.9942\n",
      "\n",
      "Epoch 00177: val_loss improved from 0.02061 to 0.02020, saving model to /Users/jsha/gjai/nlp/pytest/temp_modelmy_model.h5\n",
      "Epoch 178/200\n",
      "4157/4157 [==============================] - 0s 21us/step - loss: 0.0488 - acc: 0.9877 - val_loss: 0.0229 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.02020\n",
      "Epoch 179/200\n",
      "4157/4157 [==============================] - 0s 21us/step - loss: 0.0455 - acc: 0.9887 - val_loss: 0.0199 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00179: val_loss improved from 0.02020 to 0.01988, saving model to /Users/jsha/gjai/nlp/pytest/temp_modelmy_model.h5\n",
      "Epoch 180/200\n",
      "4157/4157 [==============================] - 0s 21us/step - loss: 0.0459 - acc: 0.9894 - val_loss: 0.0214 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.01988\n",
      "Epoch 181/200\n",
      "4157/4157 [==============================] - 0s 21us/step - loss: 0.0475 - acc: 0.9870 - val_loss: 0.0228 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.01988\n",
      "Epoch 182/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0460 - acc: 0.9870 - val_loss: 0.0198 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.01988 to 0.01980, saving model to /Users/jsha/gjai/nlp/pytest/temp_modelmy_model.h5\n",
      "Epoch 183/200\n",
      "4157/4157 [==============================] - 0s 21us/step - loss: 0.0484 - acc: 0.9880 - val_loss: 0.0232 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.01980\n",
      "Epoch 184/200\n",
      "4157/4157 [==============================] - 0s 21us/step - loss: 0.0449 - acc: 0.9882 - val_loss: 0.0239 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.01980\n",
      "Epoch 185/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0481 - acc: 0.9868 - val_loss: 0.0230 - val_acc: 0.9942\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.01980\n",
      "Epoch 186/200\n",
      "4157/4157 [==============================] - 0s 20us/step - loss: 0.0535 - acc: 0.9848 - val_loss: 0.0353 - val_acc: 0.9885\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.01980\n",
      "Epoch 187/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0465 - acc: 0.9882 - val_loss: 0.0260 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.01980\n",
      "Epoch 188/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0467 - acc: 0.9885 - val_loss: 0.0219 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.01980\n",
      "Epoch 189/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0451 - acc: 0.9887 - val_loss: 0.0287 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.01980\n",
      "Epoch 190/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0471 - acc: 0.9873 - val_loss: 0.0352 - val_acc: 0.9865\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.01980\n",
      "Epoch 191/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0533 - acc: 0.9860 - val_loss: 0.0229 - val_acc: 0.9952\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.01980\n",
      "Epoch 192/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0486 - acc: 0.9865 - val_loss: 0.0229 - val_acc: 0.9923\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.01980\n",
      "Epoch 193/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0455 - acc: 0.9875 - val_loss: 0.0221 - val_acc: 0.9942\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.01980\n",
      "Epoch 194/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0484 - acc: 0.9870 - val_loss: 0.0205 - val_acc: 0.9952\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.01980\n",
      "Epoch 195/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0452 - acc: 0.9880 - val_loss: 0.0197 - val_acc: 0.9942\n",
      "\n",
      "Epoch 00195: val_loss improved from 0.01980 to 0.01973, saving model to /Users/jsha/gjai/nlp/pytest/temp_modelmy_model.h5\n",
      "Epoch 196/200\n",
      "4157/4157 [==============================] - 0s 18us/step - loss: 0.0553 - acc: 0.9812 - val_loss: 0.0290 - val_acc: 0.9885\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.01973\n",
      "Epoch 197/200\n",
      "4157/4157 [==============================] - 0s 18us/step - loss: 0.0509 - acc: 0.9860 - val_loss: 0.0192 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00197: val_loss improved from 0.01973 to 0.01922, saving model to /Users/jsha/gjai/nlp/pytest/temp_modelmy_model.h5\n",
      "Epoch 198/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0470 - acc: 0.9875 - val_loss: 0.0212 - val_acc: 0.9942\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.01922\n",
      "Epoch 199/200\n",
      "4157/4157 [==============================] - 0s 18us/step - loss: 0.0459 - acc: 0.9868 - val_loss: 0.0301 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.01922\n",
      "Epoch 200/200\n",
      "4157/4157 [==============================] - 0s 19us/step - loss: 0.0504 - acc: 0.9858 - val_loss: 0.0224 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.01922\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fd6500a82d0>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# modelpath = save_path+'{epoch:d}-{val_loss:.4f}.h5'\n",
    "modelpath = save_path+'my_model.h5'\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss',\n",
    "                              verbose=1, save_best_only=True)\n",
    "model.fit(X_train, y_train, epochs=200, batch_size=200,\n",
    "         validation_split=0.2, verbose=1, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "0f9382fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "loaded_model = load_model(save_path+'9-0.4456.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "5caa377c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "52/52 [==============================] - 0s 2ms/sample - loss: 0.4534 - acc: 0.7885\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.78846157"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.evaluate(X_test, y_test)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d6d017",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0150af9a",
   "metadata": {},
   "source": [
    "# 연습문제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6be63e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attribute_1</th>\n",
       "      <th>attribute_2</th>\n",
       "      <th>attribute_3</th>\n",
       "      <th>attribute_4</th>\n",
       "      <th>attribute_5</th>\n",
       "      <th>attribute_6</th>\n",
       "      <th>attribute_7</th>\n",
       "      <th>attribute_8</th>\n",
       "      <th>attribute_9</th>\n",
       "      <th>attribute_10</th>\n",
       "      <th>...</th>\n",
       "      <th>attribute_52</th>\n",
       "      <th>attribute_53</th>\n",
       "      <th>attribute_54</th>\n",
       "      <th>attribute_55</th>\n",
       "      <th>attribute_56</th>\n",
       "      <th>attribute_57</th>\n",
       "      <th>attribute_58</th>\n",
       "      <th>attribute_59</th>\n",
       "      <th>attribute_60</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   attribute_1  attribute_2  attribute_3  attribute_4  attribute_5  \\\n",
       "0       0.0200       0.0371       0.0428       0.0207       0.0954   \n",
       "1       0.0453       0.0523       0.0843       0.0689       0.1183   \n",
       "2       0.0262       0.0582       0.1099       0.1083       0.0974   \n",
       "3       0.0100       0.0171       0.0623       0.0205       0.0205   \n",
       "4       0.0762       0.0666       0.0481       0.0394       0.0590   \n",
       "\n",
       "   attribute_6  attribute_7  attribute_8  attribute_9  attribute_10  ...  \\\n",
       "0       0.0986       0.1539       0.1601       0.3109        0.2111  ...   \n",
       "1       0.2583       0.2156       0.3481       0.3337        0.2872  ...   \n",
       "2       0.2280       0.2431       0.3771       0.5598        0.6194  ...   \n",
       "3       0.0368       0.1098       0.1276       0.0598        0.1264  ...   \n",
       "4       0.0649       0.1209       0.2467       0.3564        0.4459  ...   \n",
       "\n",
       "   attribute_52  attribute_53  attribute_54  attribute_55  attribute_56  \\\n",
       "0        0.0027        0.0065        0.0159        0.0072        0.0167   \n",
       "1        0.0084        0.0089        0.0048        0.0094        0.0191   \n",
       "2        0.0232        0.0166        0.0095        0.0180        0.0244   \n",
       "3        0.0121        0.0036        0.0150        0.0085        0.0073   \n",
       "4        0.0031        0.0054        0.0105        0.0110        0.0015   \n",
       "\n",
       "   attribute_57  attribute_58  attribute_59  attribute_60  Class  \n",
       "0        0.0180        0.0084        0.0090        0.0032   Rock  \n",
       "1        0.0140        0.0049        0.0052        0.0044   Rock  \n",
       "2        0.0316        0.0164        0.0095        0.0078   Rock  \n",
       "3        0.0050        0.0044        0.0040        0.0117   Rock  \n",
       "4        0.0072        0.0048        0.0107        0.0094   Rock  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = '/Users/jsha/gjai/nlp/pytest/'\n",
    "save_path = '/Users/jsha/gjai/nlp/pytest/temp_model/'\n",
    "df = pd.read_csv(path+'sonar.csv', header=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4b9d2f36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Rock', 'Mine'], dtype=object)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:, -1].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cc230184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "76fb2fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.sample(frac=1)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0f59db4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "78b9f148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     attribute_1  attribute_2  attribute_3  attribute_4  attribute_5  \\\n",
      "100       0.0629       0.1065       0.1526       0.1229       0.1437   \n",
      "126       0.0715       0.0849       0.0587       0.0218       0.0862   \n",
      "82        0.0409       0.0421       0.0573       0.0130       0.0183   \n",
      "75        0.0202       0.0104       0.0325       0.0239       0.0807   \n",
      "39        0.0091       0.0213       0.0206       0.0505       0.0657   \n",
      "\n",
      "     attribute_6  attribute_7  attribute_8  attribute_9  attribute_10  ...  \\\n",
      "100       0.1190       0.0884       0.0907       0.2107        0.3597  ...   \n",
      "126       0.1801       0.1916       0.1896       0.2960        0.4186  ...   \n",
      "82        0.1019       0.1054       0.1070       0.2302        0.2259  ...   \n",
      "75        0.1529       0.1154       0.0608       0.1317        0.1370  ...   \n",
      "39        0.0795       0.0970       0.0872       0.0743        0.0837  ...   \n",
      "\n",
      "     attribute_51  attribute_52  attribute_53  attribute_54  attribute_55  \\\n",
      "100        0.0257        0.0089        0.0262        0.0108        0.0138   \n",
      "126        0.0216        0.0153        0.0121        0.0096        0.0196   \n",
      "82         0.0113        0.0028        0.0036        0.0105        0.0120   \n",
      "75         0.0188        0.0127        0.0081        0.0067        0.0043   \n",
      "39         0.0300        0.0112        0.0112        0.0102        0.0026   \n",
      "\n",
      "     attribute_56  attribute_57  attribute_58  attribute_59  attribute_60  \n",
      "100        0.0187        0.0230        0.0057        0.0113        0.0131  \n",
      "126        0.0042        0.0066        0.0099        0.0083        0.0124  \n",
      "82         0.0087        0.0061        0.0061        0.0030        0.0078  \n",
      "75         0.0065        0.0049        0.0054        0.0073        0.0054  \n",
      "39         0.0097        0.0098        0.0043        0.0071        0.0108  \n",
      "\n",
      "[5 rows x 60 columns]\n",
      "--------------------------------------------------\n",
      "     attribute_1  attribute_2  attribute_3  attribute_4  attribute_5  \\\n",
      "100       0.0629       0.1065       0.1526       0.1229       0.1437   \n",
      "126       0.0715       0.0849       0.0587       0.0218       0.0862   \n",
      "82        0.0409       0.0421       0.0573       0.0130       0.0183   \n",
      "75        0.0202       0.0104       0.0325       0.0239       0.0807   \n",
      "39        0.0091       0.0213       0.0206       0.0505       0.0657   \n",
      "\n",
      "     attribute_6  attribute_7  attribute_8  attribute_9  attribute_10  ...  \\\n",
      "100       0.1190       0.0884       0.0907       0.2107        0.3597  ...   \n",
      "126       0.1801       0.1916       0.1896       0.2960        0.4186  ...   \n",
      "82        0.1019       0.1054       0.1070       0.2302        0.2259  ...   \n",
      "75        0.1529       0.1154       0.0608       0.1317        0.1370  ...   \n",
      "39        0.0795       0.0970       0.0872       0.0743        0.0837  ...   \n",
      "\n",
      "     attribute_51  attribute_52  attribute_53  attribute_54  attribute_55  \\\n",
      "100        0.0257        0.0089        0.0262        0.0108        0.0138   \n",
      "126        0.0216        0.0153        0.0121        0.0096        0.0196   \n",
      "82         0.0113        0.0028        0.0036        0.0105        0.0120   \n",
      "75         0.0188        0.0127        0.0081        0.0067        0.0043   \n",
      "39         0.0300        0.0112        0.0112        0.0102        0.0026   \n",
      "\n",
      "     attribute_56  attribute_57  attribute_58  attribute_59  attribute_60  \n",
      "100        0.0187        0.0230        0.0057        0.0113        0.0131  \n",
      "126        0.0042        0.0066        0.0099        0.0083        0.0124  \n",
      "82         0.0087        0.0061        0.0061        0.0030        0.0078  \n",
      "75         0.0065        0.0049        0.0054        0.0073        0.0054  \n",
      "39         0.0097        0.0098        0.0043        0.0071        0.0108  \n",
      "\n",
      "[5 rows x 60 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1111)\n",
    "\n",
    "print(X_train.head())\n",
    "print('-'*50)\n",
    "print(X_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0e8ab5a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100    Mine\n",
       "126    Mine\n",
       "82     Rock\n",
       "75     Rock\n",
       "39     Rock\n",
       "Name: Class, dtype: object"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "eef65017",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "e = LabelEncoder()\n",
    "e.fit(y_train)\n",
    "y_train = e.transform(y_train)\n",
    "y_test = e.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6a6c644b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train: [0 0 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 0 0 0 1 0 1 0 1 1 1 1 0 1 0 0 0 0\n",
      " 1 0 0 1 0 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 1 1 0 0 1 1 1 0 1 0 0 0 1 1 0 0 0\n",
      " 1 0 0 1 1 0 1 0 0 1 1 0 0 0 0 0 1 1 0 0 0 1 1 0 1 1 0 1 0 0 0 0 1 1 1 1 1\n",
      " 1 0 1 0 1 1 0 0 0 1 1 0 0 0 1 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 0 1\n",
      " 0 0 0 0 0 1 1 0]\n",
      "156\n",
      "y_test: [0 1 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 1 1 0 0 1 0 0 1 1 0 1 0 0 0 0 1 0 0 0 0\n",
      " 0 1 0 0 0 1 0 1 1 1 1 0 1 1 0]\n",
      "{0, 1}\n"
     ]
    }
   ],
   "source": [
    "print('y_train:', y_train)\n",
    "print(len(y_train))\n",
    "print('y_test:', y_test)\n",
    "print(set(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c54dfa21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Mine', 1: 'Rock'}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_transformed = e.transform(['Mine', 'Rock'])\n",
    "check = [(j, i) for i, j in zip(['Mine', 'Rock'], y_train_transformed)]\n",
    "check = dict(check)\n",
    "check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "13802c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# y_train = tf.keras.utils.to_categorical(y_train)\n",
    "# # print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8b96d6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ece64d7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "d2e5532f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(60, activation='relu', input_dim=60))\n",
    "model.add(Dense(36, activation='relu'))\n",
    "model.add(Dense(18, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "2134361e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_28\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_117 (Dense)            (None, 60)                3660      \n",
      "_________________________________________________________________\n",
      "dense_118 (Dense)            (None, 36)                2196      \n",
      "_________________________________________________________________\n",
      "dense_119 (Dense)            (None, 18)                666       \n",
      "_________________________________________________________________\n",
      "dense_120 (Dense)            (None, 8)                 152       \n",
      "_________________________________________________________________\n",
      "dense_121 (Dense)            (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 6,683\n",
      "Trainable params: 6,683\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "6c5078d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 124 samples, validate on 32 samples\n",
      "Epoch 1/100\n",
      "124/124 [==============================] - 1s 8ms/step - loss: 0.6856 - acc: 0.5323 - val_loss: 0.6485 - val_acc: 0.5938\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.64849, saving model to /Users/jsha/gjai/nlp/pytest/temp_model/1-0.6485.h5\n",
      "Epoch 2/100\n",
      "124/124 [==============================] - 0s 617us/step - loss: 0.6469 - acc: 0.6774 - val_loss: 0.6143 - val_acc: 0.6562\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.64849 to 0.61431, saving model to /Users/jsha/gjai/nlp/pytest/temp_model/2-0.6143.h5\n",
      "Epoch 3/100\n",
      "124/124 [==============================] - 0s 571us/step - loss: 0.6255 - acc: 0.6452 - val_loss: 0.6058 - val_acc: 0.7188\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.61431 to 0.60584, saving model to /Users/jsha/gjai/nlp/pytest/temp_model/3-0.6058.h5\n",
      "Epoch 4/100\n",
      "124/124 [==============================] - 0s 596us/step - loss: 0.5744 - acc: 0.7661 - val_loss: 0.5504 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.60584 to 0.55042, saving model to /Users/jsha/gjai/nlp/pytest/temp_model/4-0.5504.h5\n",
      "Epoch 5/100\n",
      "124/124 [==============================] - 0s 595us/step - loss: 0.5337 - acc: 0.7742 - val_loss: 0.5162 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.55042 to 0.51618, saving model to /Users/jsha/gjai/nlp/pytest/temp_model/5-0.5162.h5\n",
      "Epoch 6/100\n",
      "124/124 [==============================] - 0s 632us/step - loss: 0.4937 - acc: 0.8065 - val_loss: 0.4988 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.51618 to 0.49884, saving model to /Users/jsha/gjai/nlp/pytest/temp_model/6-0.4988.h5\n",
      "Epoch 7/100\n",
      "124/124 [==============================] - 0s 610us/step - loss: 0.4627 - acc: 0.8145 - val_loss: 0.4607 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.49884 to 0.46070, saving model to /Users/jsha/gjai/nlp/pytest/temp_model/7-0.4607.h5\n",
      "Epoch 8/100\n",
      "124/124 [==============================] - 0s 577us/step - loss: 0.4158 - acc: 0.8387 - val_loss: 0.4562 - val_acc: 0.7812\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.46070 to 0.45616, saving model to /Users/jsha/gjai/nlp/pytest/temp_model/8-0.4562.h5\n",
      "Epoch 9/100\n",
      "124/124 [==============================] - 0s 624us/step - loss: 0.4028 - acc: 0.7984 - val_loss: 0.4456 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.45616 to 0.44555, saving model to /Users/jsha/gjai/nlp/pytest/temp_model/9-0.4456.h5\n",
      "Epoch 10/100\n",
      "124/124 [==============================] - 0s 669us/step - loss: 0.4145 - acc: 0.8226 - val_loss: 0.6404 - val_acc: 0.6250\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.44555\n",
      "Epoch 11/100\n",
      "124/124 [==============================] - 0s 667us/step - loss: 0.3736 - acc: 0.8145 - val_loss: 0.4657 - val_acc: 0.7812\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.44555\n",
      "Epoch 12/100\n",
      "124/124 [==============================] - 0s 597us/step - loss: 0.3517 - acc: 0.8387 - val_loss: 0.4650 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.44555\n",
      "Epoch 13/100\n",
      "124/124 [==============================] - 0s 626us/step - loss: 0.3163 - acc: 0.8790 - val_loss: 0.4326 - val_acc: 0.7812\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.44555 to 0.43257, saving model to /Users/jsha/gjai/nlp/pytest/temp_model/13-0.4326.h5\n",
      "Epoch 14/100\n",
      "124/124 [==============================] - 0s 600us/step - loss: 0.3173 - acc: 0.8629 - val_loss: 0.4316 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.43257 to 0.43159, saving model to /Users/jsha/gjai/nlp/pytest/temp_model/14-0.4316.h5\n",
      "Epoch 15/100\n",
      "124/124 [==============================] - 0s 687us/step - loss: 0.3174 - acc: 0.8710 - val_loss: 0.4219 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.43159 to 0.42186, saving model to /Users/jsha/gjai/nlp/pytest/temp_model/15-0.4219.h5\n",
      "Epoch 16/100\n",
      "124/124 [==============================] - 0s 602us/step - loss: 0.3103 - acc: 0.8468 - val_loss: 0.4215 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.42186 to 0.42150, saving model to /Users/jsha/gjai/nlp/pytest/temp_model/16-0.4215.h5\n",
      "Epoch 17/100\n",
      "124/124 [==============================] - 0s 595us/step - loss: 0.2905 - acc: 0.9113 - val_loss: 0.4664 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.42150\n",
      "Epoch 18/100\n",
      "124/124 [==============================] - 0s 633us/step - loss: 0.2401 - acc: 0.9032 - val_loss: 0.4258 - val_acc: 0.7812\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.42150\n",
      "Epoch 19/100\n",
      "124/124 [==============================] - 0s 631us/step - loss: 0.2064 - acc: 0.9435 - val_loss: 0.4280 - val_acc: 0.7188\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.42150\n",
      "Epoch 20/100\n",
      "124/124 [==============================] - 0s 607us/step - loss: 0.2180 - acc: 0.9113 - val_loss: 0.4695 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.42150\n",
      "Epoch 21/100\n",
      "124/124 [==============================] - 0s 597us/step - loss: 0.1818 - acc: 0.9516 - val_loss: 0.4518 - val_acc: 0.7812\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.42150\n",
      "Epoch 22/100\n",
      "124/124 [==============================] - 0s 611us/step - loss: 0.1720 - acc: 0.9435 - val_loss: 0.5007 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.42150\n",
      "Epoch 23/100\n",
      "124/124 [==============================] - 0s 630us/step - loss: 0.1468 - acc: 0.9435 - val_loss: 0.4622 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.42150\n",
      "Epoch 24/100\n",
      "124/124 [==============================] - 0s 645us/step - loss: 0.1624 - acc: 0.9274 - val_loss: 0.4409 - val_acc: 0.7188\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.42150\n",
      "Epoch 25/100\n",
      "124/124 [==============================] - 0s 630us/step - loss: 0.2024 - acc: 0.9032 - val_loss: 0.4946 - val_acc: 0.7188\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.42150\n",
      "Epoch 26/100\n",
      "124/124 [==============================] - 0s 667us/step - loss: 0.1285 - acc: 0.9677 - val_loss: 0.5708 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.42150\n",
      "Epoch 27/100\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 0.1042 - acc: 0.9758 - val_loss: 0.6896 - val_acc: 0.7812\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.42150\n",
      "Epoch 28/100\n",
      "124/124 [==============================] - 0s 596us/step - loss: 0.1028 - acc: 0.9597 - val_loss: 0.5526 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.42150\n",
      "Epoch 29/100\n",
      "124/124 [==============================] - 0s 683us/step - loss: 0.0861 - acc: 0.9839 - val_loss: 0.5947 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.42150\n",
      "Epoch 30/100\n",
      "124/124 [==============================] - 0s 634us/step - loss: 0.0876 - acc: 0.9758 - val_loss: 0.4492 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.42150\n",
      "Epoch 31/100\n",
      "124/124 [==============================] - 0s 716us/step - loss: 0.0722 - acc: 0.9919 - val_loss: 0.5133 - val_acc: 0.7812\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.42150\n",
      "Epoch 32/100\n",
      "124/124 [==============================] - 0s 775us/step - loss: 0.0668 - acc: 0.9919 - val_loss: 0.6442 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.42150\n",
      "Epoch 33/100\n",
      "124/124 [==============================] - 0s 905us/step - loss: 0.0596 - acc: 0.9839 - val_loss: 0.5296 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.42150\n",
      "Epoch 34/100\n",
      "124/124 [==============================] - 0s 685us/step - loss: 0.0557 - acc: 0.9919 - val_loss: 0.8069 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.42150\n",
      "Epoch 35/100\n",
      "124/124 [==============================] - 0s 617us/step - loss: 0.0503 - acc: 0.9919 - val_loss: 0.5508 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.42150\n",
      "Epoch 36/100\n",
      "124/124 [==============================] - 0s 659us/step - loss: 0.0538 - acc: 1.0000 - val_loss: 0.8181 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.42150\n",
      "Epoch 37/100\n",
      "124/124 [==============================] - 0s 589us/step - loss: 0.0456 - acc: 0.9919 - val_loss: 0.5698 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.42150\n",
      "Epoch 38/100\n",
      "124/124 [==============================] - 0s 611us/step - loss: 0.0712 - acc: 0.9839 - val_loss: 0.5171 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.42150\n",
      "Epoch 39/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/124 [==============================] - 0s 623us/step - loss: 0.0368 - acc: 1.0000 - val_loss: 0.6524 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.42150\n",
      "Epoch 40/100\n",
      "124/124 [==============================] - 0s 586us/step - loss: 0.0261 - acc: 1.0000 - val_loss: 0.8701 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.42150\n",
      "Epoch 41/100\n",
      "124/124 [==============================] - 0s 602us/step - loss: 0.0347 - acc: 0.9919 - val_loss: 0.8011 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.42150\n",
      "Epoch 42/100\n",
      "124/124 [==============================] - 0s 568us/step - loss: 0.0222 - acc: 1.0000 - val_loss: 0.6632 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.42150\n",
      "Epoch 43/100\n",
      "124/124 [==============================] - 0s 584us/step - loss: 0.0157 - acc: 1.0000 - val_loss: 0.7398 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.42150\n",
      "Epoch 44/100\n",
      "124/124 [==============================] - 0s 624us/step - loss: 0.0147 - acc: 1.0000 - val_loss: 0.6997 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.42150\n",
      "Epoch 45/100\n",
      "124/124 [==============================] - 0s 603us/step - loss: 0.0149 - acc: 1.0000 - val_loss: 0.8651 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.42150\n",
      "Epoch 46/100\n",
      "124/124 [==============================] - 0s 581us/step - loss: 0.0119 - acc: 1.0000 - val_loss: 0.6637 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.42150\n",
      "Epoch 47/100\n",
      "124/124 [==============================] - 0s 622us/step - loss: 0.0099 - acc: 1.0000 - val_loss: 0.7486 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.42150\n",
      "Epoch 48/100\n",
      "124/124 [==============================] - 0s 621us/step - loss: 0.0092 - acc: 1.0000 - val_loss: 0.9039 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.42150\n",
      "Epoch 49/100\n",
      "124/124 [==============================] - 0s 633us/step - loss: 0.0095 - acc: 1.0000 - val_loss: 0.8802 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.42150\n",
      "Epoch 50/100\n",
      "124/124 [==============================] - 0s 563us/step - loss: 0.0092 - acc: 1.0000 - val_loss: 0.8014 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.42150\n",
      "Epoch 51/100\n",
      "124/124 [==============================] - 0s 579us/step - loss: 0.0086 - acc: 1.0000 - val_loss: 0.7857 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.42150\n",
      "Epoch 52/100\n",
      "124/124 [==============================] - 0s 602us/step - loss: 0.0124 - acc: 1.0000 - val_loss: 0.9226 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.42150\n",
      "Epoch 53/100\n",
      "124/124 [==============================] - 0s 661us/step - loss: 0.0081 - acc: 1.0000 - val_loss: 0.9201 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.42150\n",
      "Epoch 54/100\n",
      "124/124 [==============================] - 0s 670us/step - loss: 0.0060 - acc: 1.0000 - val_loss: 0.8135 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.42150\n",
      "Epoch 55/100\n",
      "124/124 [==============================] - 0s 560us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 0.7866 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.42150\n",
      "Epoch 56/100\n",
      "124/124 [==============================] - 0s 604us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 0.8501 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.42150\n",
      "Epoch 57/100\n",
      "124/124 [==============================] - 0s 769us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 0.9100 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.42150\n",
      "Epoch 58/100\n",
      "124/124 [==============================] - 0s 655us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 0.9062 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.42150\n",
      "Epoch 59/100\n",
      "124/124 [==============================] - 0s 640us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 0.8161 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.42150\n",
      "Epoch 60/100\n",
      "124/124 [==============================] - 0s 696us/step - loss: 0.0042 - acc: 1.0000 - val_loss: 0.7733 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.42150\n",
      "Epoch 61/100\n",
      "124/124 [==============================] - 0s 661us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 0.9326 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.42150\n",
      "Epoch 62/100\n",
      "124/124 [==============================] - 0s 588us/step - loss: 0.0040 - acc: 1.0000 - val_loss: 0.9187 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.42150\n",
      "Epoch 63/100\n",
      "124/124 [==============================] - 0s 614us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 0.9040 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.42150\n",
      "Epoch 64/100\n",
      "124/124 [==============================] - 0s 626us/step - loss: 0.0028 - acc: 1.0000 - val_loss: 0.9398 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.42150\n",
      "Epoch 65/100\n",
      "124/124 [==============================] - 0s 574us/step - loss: 0.0029 - acc: 1.0000 - val_loss: 0.9037 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.42150\n",
      "Epoch 66/100\n",
      "124/124 [==============================] - 0s 676us/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.9611 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.42150\n",
      "Epoch 67/100\n",
      "124/124 [==============================] - 0s 621us/step - loss: 0.0024 - acc: 1.0000 - val_loss: 0.9466 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.42150\n",
      "Epoch 68/100\n",
      "124/124 [==============================] - 0s 626us/step - loss: 0.0024 - acc: 1.0000 - val_loss: 0.9190 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.42150\n",
      "Epoch 69/100\n",
      "124/124 [==============================] - 0s 606us/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.9681 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.42150\n",
      "Epoch 70/100\n",
      "124/124 [==============================] - 0s 878us/step - loss: 0.0022 - acc: 1.0000 - val_loss: 1.0006 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.42150\n",
      "Epoch 71/100\n",
      "124/124 [==============================] - 0s 591us/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.9418 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.42150\n",
      "Epoch 72/100\n",
      "124/124 [==============================] - 0s 645us/step - loss: 0.0020 - acc: 1.0000 - val_loss: 1.0141 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.42150\n",
      "Epoch 73/100\n",
      "124/124 [==============================] - 0s 626us/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.9501 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.42150\n",
      "Epoch 74/100\n",
      "124/124 [==============================] - 0s 644us/step - loss: 0.0018 - acc: 1.0000 - val_loss: 1.0046 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.42150\n",
      "Epoch 75/100\n",
      "124/124 [==============================] - 0s 585us/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.9611 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.42150\n",
      "Epoch 76/100\n",
      "124/124 [==============================] - 0s 652us/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.9814 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.42150\n",
      "Epoch 77/100\n",
      "124/124 [==============================] - 0s 617us/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.9813 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.42150\n",
      "Epoch 78/100\n",
      "124/124 [==============================] - 0s 639us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 1.0498 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.42150\n",
      "Epoch 79/100\n",
      "124/124 [==============================] - 0s 604us/step - loss: 0.0016 - acc: 1.0000 - val_loss: 1.0633 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.42150\n",
      "Epoch 80/100\n",
      "124/124 [==============================] - 0s 623us/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.9361 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.42150\n",
      "Epoch 81/100\n",
      "124/124 [==============================] - 0s 577us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 1.0373 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.42150\n",
      "Epoch 82/100\n",
      "124/124 [==============================] - 0s 801us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 1.0100 - val_acc: 0.8438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00082: val_loss did not improve from 0.42150\n",
      "Epoch 83/100\n",
      "124/124 [==============================] - 0s 640us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 1.0626 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.42150\n",
      "Epoch 84/100\n",
      "124/124 [==============================] - 0s 601us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.9853 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.42150\n",
      "Epoch 85/100\n",
      "124/124 [==============================] - 0s 644us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 1.0419 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.42150\n",
      "Epoch 86/100\n",
      "124/124 [==============================] - 0s 593us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.9819 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.42150\n",
      "Epoch 87/100\n",
      "124/124 [==============================] - 0s 590us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 1.0876 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.42150\n",
      "Epoch 88/100\n",
      "124/124 [==============================] - 0s 586us/step - loss: 0.0010 - acc: 1.0000 - val_loss: 1.0518 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.42150\n",
      "Epoch 89/100\n",
      "124/124 [==============================] - 0s 639us/step - loss: 0.0010 - acc: 1.0000 - val_loss: 1.0233 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.42150\n",
      "Epoch 90/100\n",
      "124/124 [==============================] - 0s 602us/step - loss: 9.7863e-04 - acc: 1.0000 - val_loss: 1.0748 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.42150\n",
      "Epoch 91/100\n",
      "124/124 [==============================] - 0s 575us/step - loss: 9.3314e-04 - acc: 1.0000 - val_loss: 1.0372 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.42150\n",
      "Epoch 92/100\n",
      "124/124 [==============================] - 0s 644us/step - loss: 8.8472e-04 - acc: 1.0000 - val_loss: 1.0764 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.42150\n",
      "Epoch 93/100\n",
      "124/124 [==============================] - 0s 604us/step - loss: 8.5269e-04 - acc: 1.0000 - val_loss: 1.0189 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.42150\n",
      "Epoch 94/100\n",
      "124/124 [==============================] - 0s 657us/step - loss: 8.1550e-04 - acc: 1.0000 - val_loss: 1.0900 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.42150\n",
      "Epoch 95/100\n",
      "124/124 [==============================] - 0s 671us/step - loss: 7.7978e-04 - acc: 1.0000 - val_loss: 1.0929 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.42150\n",
      "Epoch 96/100\n",
      "124/124 [==============================] - 0s 638us/step - loss: 7.6908e-04 - acc: 1.0000 - val_loss: 1.1041 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.42150\n",
      "Epoch 97/100\n",
      "124/124 [==============================] - 0s 657us/step - loss: 7.4983e-04 - acc: 1.0000 - val_loss: 1.1105 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.42150\n",
      "Epoch 98/100\n",
      "124/124 [==============================] - 0s 554us/step - loss: 7.1741e-04 - acc: 1.0000 - val_loss: 1.1006 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.42150\n",
      "Epoch 99/100\n",
      "124/124 [==============================] - 0s 583us/step - loss: 7.2654e-04 - acc: 1.0000 - val_loss: 1.0703 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.42150\n",
      "Epoch 100/100\n",
      "124/124 [==============================] - 0s 562us/step - loss: 7.1550e-04 - acc: 1.0000 - val_loss: 1.1069 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.42150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fd60aaa1e50>"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "             metrics=['acc'])\n",
    "\n",
    "modelpath = save_path+'{epoch:d}-{val_loss:.4f}.h5'\n",
    "# modelpath = save_path+'my_model.h5'\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss',\n",
    "                              verbose=1, save_best_only=True)\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=5,\n",
    "         validation_split=0.2, verbose=1, callbacks=[checkpointer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "8e578615",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "loaded_model = load_model(save_path+'16-0.4215.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "c8e08302",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "52/52 [==============================] - 0s 2ms/sample - loss: 0.4019 - acc: 0.8462\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.84615386"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.evaluate(X_test, y_test)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db0d3f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp3710",
   "language": "python",
   "name": "nlp3710"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
