{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b489702b",
   "metadata": {},
   "source": [
    "# 이론적 배경\n",
    " ## 감정 분석\n",
    " - 감성 분석? 감정분석? 그냥 섞어 쓰는게 통념이다.\n",
    " - Sentimental Analysis or Emotional Analysis or Opinion Mining\n",
    " - 사전기반, ML, DL 세 가지 방법이 있다.\n",
    " - 어려운 게 아니고, 분류 문제이다.\n",
    " - 사전기반은 '감정 사전'이 필요하고, ML-DL은 'label'이 필요하다.\n",
    " - '감정 사전'이 '모델'의 역할을 한다.\n",
    " \n",
    " ## 사전기반 감정 분석\n",
    " - 잘 만들어 놓으면 안정적으로 사용가능\n",
    " - 범용성이 높음(80% 이상)\n",
    " - 도메인이 달라지더라도 안정성을 어느 정도 보장한다. 사전만 잘 만들어 두면.\n",
    " - 여전히 **현업**에서는 사전기반 감정 분석을 사용하고 있음.\n",
    " - 자연어처리 회사는 저마다 각자의 '감정 사전'을 보유한다.\n",
    " - **감정의 개념, 형태소의 개념을 명확히 알아야 '감정 사전'을 만들 수 있다.**\n",
    " \n",
    " ## ML, DL 감정 분석\n",
    " - 문장의 맥락을 본다.\n",
    " - 도메인이 달라지면 성능이 훅 떨어진다.\n",
    " \n",
    " ## Opinion Mining\n",
    " - 감정 6대 분석(긍정, (중립), 부정, 기쁨, 슬픔, 놀람, 분노)는 사실상 분리하기 어려워.\n",
    " - 그래서 6대 분석이 아니라 긍정, 중립, 부정 3단계(긍부정 분석)로 나누는 걸 opinion mining이라 한다.\n",
    " - <u>**우리는 여기서 긍부정 레벨을 사용한다.**</u>\n",
    " \n",
    " ## 감정이란?\n",
    " - 감정을 객관적으로 알 수 있는 건, 안면인식과 혈류 분석임.\n",
    " - Ekman: 감정과 표정을 매핑하는 연구를 함. authority가 있음.\n",
    " - 9가지 감정체계가 나중에는 54개 분류까지 확장됨. 너무함. 이건 사람들이 잘 안 받아들임.\n",
    " - 보통 3~5개(중립 빼고) 레벨로 분석한다.\n",
    " - 빈도분석, 전통적인 사전 이용 방법, 머신러닝, 딥러닝을 이용한 방법이 사용됨.\n",
    " \n",
    " ## 분석 감정의 종류\n",
    "- 2분류:긍정–부정\n",
    "- 3분류:긍정–중립–부정(가장 많이 사용. 뭐 쉬우니까.)\n",
    "- 4분류: neutral, happy, sad, ANGER(disgust+anger)\n",
    "- 5분류: neutral, happy, SURPRISE(surprise+fear), sad, ANGER(disgust+anger)\n",
    "- 6분류: neutral, happy, surprise, fear, sad, ANGER(disgust+anger) \n",
    "- 7분류: neutral, happy, surprise, fear, sad, disgust, anger\n",
    "\n",
    "## 감정어의 종류\n",
    "- 감정어: 말하는 이의 감정을 주관적으로 표현하는 것으로 극성이 잘 안 바뀜.\n",
    "    - 화나다. 즐겁다. 괴롭다. 슬프다.\n",
    "- 평가어: 대상에 대한 감정을 사실적으로 평가하는 것으로서 맥락에 따라 극성이 바뀜.\n",
    "    - 깨끗하다: 냉장고가 깨끗하다. \n",
    "    - 더럽다: 더럽게 깨끗하다.\n",
    "    - 강화하다: 자금 경직성을 강화하다. \n",
    "    - 약화하다: 고통을 약화하다\n",
    "- 감정어를 기본으로 하면서, 도메인 별로 평가어를 예외 처리하며 구축해야 함.\n",
    "\n",
    " \n",
    " # 응용분야\n",
    "  ## 심리학\n",
    "  - 1930~50년대, 심리학에서 text를 통해 내담자의 심리상태를 파악하고자 함.\n",
    "  - 전남대 국문학과 김은영 박사의 심리 감정 동사 판별. 그러나 그 개수가 550개로 너무 적어 현업에서 활요하기는 곤란함.\n",
    "  - 중국의 어떤 대학. 권위가 있어서 번역해서 사용함.\n",
    "  - 서울대, 연세대 그렇게 질이 좋지 않음.\n",
    "  - \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1c2805",
   "metadata": {},
   "source": [
    "# 감성 분석 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a0991d",
   "metadata": {},
   "source": [
    "## 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71eb28ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 읽기와 쓰기를 위한 사용자 함수\n",
    "\n",
    "def read_data(filename, encoding='cp949', start=1):\n",
    "    with open(filename, 'r', encoding=encoding) as f:\n",
    "        data = [line.split('\\t') for line in f.read().splitlines()]\n",
    "        data = data[start:]\n",
    "    return data\n",
    "\n",
    "def write_data(data, filename, encoding='cp949'):\n",
    "    with open(filename, 'w', encoding=encoding) as f:\n",
    "        f.write(data)\n",
    "\n",
    "path = '/Users/jsha/gjai/nlp/pytest/'\n",
    "file = 'ratings.txt'\n",
    "\n",
    "data = read_data(path+file)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bffd623",
   "metadata": {},
   "source": [
    "## 형태소 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20aa2cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filepath:  /Users/jsha/opt/anaconda3/envs/nlp3710/lib/python3.7/site-packages\n",
      "classpath:  /Users/jsha/opt/anaconda3/envs/nlp3710/lib/python3.7/site-packages/rhinoMorph/lib/rhino.jar\n",
      "RHINO started!\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm # 이거 엄청 느리네. 쓰지말자.\n",
    "import rhinoMorph\n",
    "\n",
    "rn = rhinoMorph.startRhino()\n",
    "\n",
    "morphed_data = ''\n",
    "# for data_each in tqdm(data):\n",
    "for data_each in data:\n",
    "    morphed_data_each = rhinoMorph.onlyMorph_list(rn, data_each[1],\n",
    "                    pos= ['NNG', 'NNP', 'VV', 'VA', 'XR', 'IC', \n",
    "                          'MM', 'MAG', 'MAJ'])\n",
    "    joined_data_each = ' '.join(morphed_data_each)\n",
    "    if joined_data_each:\n",
    "        morphed_data += data_each[0] + '\\t' + \\\n",
    "        joined_data_each + '\\t' + data_each[2] + '\\n'\n",
    "write_data(morphed_data, path+'ratings_morphed.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e432e27b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "197559"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = read_data(path+'ratings_morphed.txt')\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16d099b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_id = [line[0] for line in data]\n",
    "data_text = [line[1] for line in data]\n",
    "data_senti = [line[2] for line in data]\n",
    "\n",
    "positive = read_data(path+'positive.txt') # 긍정 감정 사전 읽기\n",
    "negative = read_data(path+'negative.txt') # 부정 감정 사전 읽기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5a9ed1",
   "metadata": {},
   "source": [
    "## 감정단어 파악"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "757edd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cntWorldInLine(data, senti):\n",
    "    senti_found = []\n",
    "    for onedata in data:\n",
    "        oneline_word = onedata.split(' ')     # 한줄의 데이터를 공백 단위로 분리하여 리스트로 저장\n",
    "        senti_temp = 0                        # 그 줄에서 발견된 감정단어의 수를 담는 변수\n",
    "        for sentiword in senti:               # 감정 사전의 어휘\n",
    "            if sentiword[0] in oneline_word:  #sentiword[0] 하여 리스트 원소를 추출(문자열)\n",
    "                senti_temp += 1               # 현재의 감정단어와 일치하면 숫자를 하나 올려 줌.(중복허용 X)\n",
    "        senti_found.append(senti_temp)        # 현재의 줄에서 찾은 감성단어의 숫자를 해당 위치에 저장\n",
    "    return senti_found\n",
    "\n",
    "data_senti_poscnt = cntWorldInLine(data_text, positive)\n",
    "data_senti_negcnt = cntWorldInLine(data_text, negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d918de43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 1, 0, 0, 2, 1, 0, 0, 0, 1, 1, 1, 0, 1, 2, 0, 1, 0, 1, 0]\n",
      "[1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(data_senti_poscnt[:20])\n",
    "print(data_senti_negcnt[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefe47d5",
   "metadata": {},
   "source": [
    "## 감정점수 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6dad4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "newdata = pd.DataFrame({'id':data_id, 'text':data_text, 'original':data_senti,\n",
    "                        'pos':data_senti_poscnt, 'neg':data_senti_negcnt})\n",
    "newdata['senti_score'] = newdata['pos'] - newdata['neg']\n",
    "\n",
    "newdata.loc[newdata.senti_score > 0, 'new'] = 1\n",
    "newdata.loc[newdata.senti_score <= 0, 'new'] = 0\n",
    "\n",
    "newdata.loc[pd.to_numeric(newdata.original) == newdata.new, 'matched']= 'True'\n",
    "newdata.loc[pd.to_numeric(newdata.original) != newdata.new, 'matched'] = 'False'\n",
    "\n",
    "newdata.head()\n",
    "score = newdata.matched.str.count('True').sum() / \\\n",
    "(newdata.matched.str.count('True').sum() + newdata.matched.str.count('False').sum()) * 100\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86f212a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata.to_csv('newfile.csv', encoding='cp949', index=False)\n",
    "newdata.to_csv('newfile.txt', sep='\\t', encoding='cp949', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b476e2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp3710",
   "language": "python",
   "name": "nlp3710"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
