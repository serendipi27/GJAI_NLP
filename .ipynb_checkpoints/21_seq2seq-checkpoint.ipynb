{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "017fb6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data length: 1000\n",
      "data type: <class 'pandas.core.frame.DataFrame'>\n",
      "data shape: (1000, 2)\n",
      "data sample:\n",
      "                 source      target\n",
      "735  I lost my wallet.  지갑을 잃어버렸어.\n",
      "485      We succeeded.   우리는 성공했어.\n",
      "506     I feel lonely.        외로워.\n",
      "462      Tom chuckled.  톰이 싱긋 웃었어.\n",
      "539     Stop laughing.      그만 웃어.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "working_path = '/Users/jsha/gjai/nlp/21_practice/'\n",
    "data_path = '/Users/jsha/gjai/nlp/pytest/data/eng-kor/'\n",
    "file_name = 'eng-kor_small.txt'\n",
    "\n",
    "data = pd.read_csv(data_path+file_name, names=['source','target'],\n",
    "                  sep='\\t', encoding='utf-8')\n",
    "\n",
    "print('data length:', len(data))\n",
    "print('data type:', type(data))\n",
    "print('data shape:', data.shape)\n",
    "print('data sample:\\n', data.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e266edfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.target length: 1000\n",
      "data.target type: <class 'pandas.core.series.Series'>\n",
      "data.target shape: (1000,)\n",
      "data.target length: 413                이해해.\n",
      "352         톰이 속임수를 썼어.\n",
      "897    너는 내 것을 가질 수 있다.\n",
      "516       나는 그를 죽일 것이다.\n",
      "685             그거 연필이야\n",
      "Name: target, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print('data.target length:', len(data.target))\n",
    "print('data.target type:', type(data.target))\n",
    "print('data.target shape:', data.target.shape)\n",
    "print('data.target length:', data.target.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abf7d300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "data.target_input:\n",
      " 0                 \\t가.\\n\n",
      "1                \\t안녕.\\n\n",
      "2                \\t뛰어!\\n\n",
      "3                \\t뛰어.\\n\n",
      "4                \\t누구?\\n\n",
      "             ...        \n",
      "995     \\t노래하는 거 좋아해요?\\n\n",
      "996      \\t노래하는 거 좋아해?\\n\n",
      "997    \\t고양이를 좋아하지 않아?\\n\n",
      "998      \\t꿈은 이루어질 거야.\\n\n",
      "999     \\t모두 그녀를 사랑한다.\\n\n",
      "Name: target, Length: 1000, dtype: object\n",
      "\n",
      "data.target_output:\n",
      " 0                 가.\\n\n",
      "1                안녕.\\n\n",
      "2                뛰어!\\n\n",
      "3                뛰어.\\n\n",
      "4                누구?\\n\n",
      "            ...       \n",
      "995     노래하는 거 좋아해요?\\n\n",
      "996      노래하는 거 좋아해?\\n\n",
      "997    고양이를 좋아하지 않아?\\n\n",
      "998      꿈은 이루어질 거야.\\n\n",
      "999     모두 그녀를 사랑한다.\\n\n",
      "Name: target, Length: 1000, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jsha/opt/anaconda3/envs/nlp3710/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/jsha/opt/anaconda3/envs/nlp3710/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "data.target_input = data.target.apply(lambda x: '\\t'+x+'\\n')\n",
    "data.target_output = data.target.apply(lambda x: x+'\\n')\n",
    "print('\\ndata.target_input:\\n', data.target_input)\n",
    "print('\\ndata.target_output:\\n', data.target_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03625f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.target_input length: 1000\n",
      "data.target_input type: <class 'pandas.core.series.Series'>\n",
      "data.target_input shape: (1000,)\n",
      "data.target_input sampe:\n",
      " 118         \\t집에 와.\\n\n",
      "131        \\t손대지 마.\\n\n",
      "74         \\t톰을 잡아.\\n\n",
      "887    \\t톰은 피를 토했다.\\n\n",
      "764       \\t그거 연필이야\\n\n",
      "Name: target, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print('data.target_input length:', len(data.target_input))\n",
    "print('data.target_input type:', type(data.target_input))\n",
    "print('data.target_input shape:', data.target_input.shape)\n",
    "print('data.target_input sampe:\\n', data.target_input.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b17d564a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source sentence max length:  20\n",
      "target sentence max length:  19\n"
     ]
    }
   ],
   "source": [
    "max_src_len = data.source.apply(lambda x: len(x)).max()\n",
    "print('source sentence max length: ', max_src_len)\n",
    "\n",
    "max_tar_len = data.target_input.apply(lambda x: len(x)).max() - 2\n",
    "print('target sentence max length: ', max_tar_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9d516a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "전체에서 64개의 고유한 토큰을 찾았습니다.\n",
      "word_index_source:  {' ': 1, 'e': 2, 'o': 3, '.': 4, 'a': 5, 't': 6, 'i': 7, 's': 8, 'n': 9, 'r': 10, 'l': 11, 'd': 12, 'm': 13, 'h': 14, 'y': 15, 'u': 16, 'T': 17, 'g': 18, 'I': 19, 'c': 20, 'p': 21, 'w': 22, 'k': 23, \"'\": 24, 'v': 25, 'b': 26, 'f': 27, '?': 28, 'S': 29, '!': 30, 'W': 31, 'H': 32, 'C': 33, 'D': 34, 'E': 35, 'K': 36, 'A': 37, 'G': 38, 'Y': 39, 'N': 40, 'x': 41, 'F': 42, 'B': 43, 'L': 44, 'M': 45, 'q': 46, ',': 47, 'P': 48, 'R': 49, 'O': 50, 'z': 51, 'J': 52, 'j': 53, 'Q': 54, '-': 55, '7': 56, ':': 57, '4': 58, '5': 59, 'U': 60, '2': 61, '0': 62, '1': 63, '3': 64}\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer_source = Tokenizer(num_words=None, char_level=True,\n",
    "                            lower=False)\n",
    "tokenizer_source.fit_on_texts(data.source)\n",
    "word_index_source = tokenizer_source.word_index\n",
    "\n",
    "print('\\n전체에서 %s개의 고유한 토큰을 찾았습니다.' %len(word_index_source))\n",
    "print('word_index_source: ', word_index_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60f3d3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "전체에서 558개의 고유한 토큰을 찾았습니다.\n",
      "word_index_source:  {' ': 1, '\\t': 2, '\\n': 3, '.': 4, '어': 5, '이': 6, '톰': 7, '해': 8, '아': 9, '은': 10, '다': 11, '그': 12, '가': 13, '는': 14, '?': 15, '나': 16, '거': 17, '!': 18, '고': 19, '하': 20, '을': 21, '했': 22, '요': 23, '있': 24, '야': 25, '지': 26, '었': 27, '사': 28, '난': 29, '말': 30, '들': 31, '기': 32, '게': 33, '리': 34, '를': 35, '니': 36, '도': 37, '우': 38, '좋': 39, '와': 40, '내': 41, '에': 42, '람': 43, '무': 44, '자': 45, '마': 46, '서': 47, '봐': 48, '한': 49, '계': 50, '안': 51, '네': 52, '시': 53, '속': 54, '너': 55, '수': 56, '모': 57, '만': 58, '짓': 59, '라': 60, '두': 61, '누': 62, '일': 63, '세': 64, '정': 65, '웃': 66, '로': 67, '않': 68, '줘': 69, '았': 70, '제': 71, '렸': 72, '걸': 73, '없': 74, '려': 75, '물': 76, '미': 77, '저': 78, '여': 79, '건': 80, '죽': 81, '으': 82, '워': 83, '조': 84, '주': 85, '린': 86, '신': 87, '것': 88, '의': 89, '져': 90, '심': 91, '좀': 92, '운': 93, '인': 94, '구': 95, '진': 96, '렇': 97, '날': 98, '입': 99, '래': 100, '울': 101, '전': 102, '빨': 103, '피': 104, '히': 105, '할': 106, '보': 107, '러': 108, '파': 109, '소': 110, '양': 111, '대': 112, '발': 113, '질': 114, '용': 115, '부': 116, '생': 117, '졌': 118, '실': 119, '빠': 120, '녀': 121, '공': 122, '테': 123, '났': 124, '새': 125, '쳤': 126, '노': 127, '까': 128, '직': 129, '적': 130, '프': 131, '겼': 132, '왔': 133, '동': 134, '합': 135, '잘': 136, '버': 137, '떠': 138, '불': 139, '먹': 140, '비': 141, '군': 142, '혼': 143, '남': 144, '습': 145, '살': 146, '잠': 147, '작': 148, '알': 149, '절': 150, '장': 151, '되': 152, '늦': 153, '집': 154, '개': 155, '행': 156, '바': 157, '간': 158, '봤': 159, '열': 160, '오': 161, '돼': 162, '잊': 163, '눈': 164, '재': 165, '감': 166, '상': 167, '원': 168, '문': 169, '과': 170, '중': 171, '책': 172, '언': 173, '길': 174, '움': 175, '당': 176, '읽': 177, '경': 178, '참': 179, '스': 180, '쳐': 181, '웠': 182, '렀': 183, '복': 184, '성': 185, '짜': 186, '싫': 187, '름': 188, '화': 189, '늙': 190, '음': 191, '색': 192, '르': 193, '락': 194, '벽': 195, '얼': 196, '앉': 197, '믿': 198, '답': 199, '따': 200, '약': 201, '죄': 202, '번': 203, '타': 204, '천': 205, '선': 206, '키': 207, '랑': 208, '엄': 209, '춤': 210, '였': 211, '숨': 212, '쉬': 213, '차': 214, '넌': 215, '증': 216, '쟁': 217, '필': 218, '폐': 219, '머': 220, '명': 221, '디': 222, '녕': 223, '왜': 224, '연': 225, '슬': 226, '퍼': 227, '완': 228, '영': 229, '잡': 230, '잖': 231, '둘': 232, '둬': 233, '끝': 234, '켜': 235, '끔': 236, '찍': 237, '유': 238, '냄': 239, '분': 240, '침': 241, '돌': 242, '회': 243, '멋': 244, '골': 245, '괜': 246, '찮': 247, '킬': 248, '췄': 249, '싸': 250, '외': 251, '잤': 252, '평': 253, '돈': 254, '식': 255, '탁': 256, '굴': 257, '박': 258, '방': 259, '몰': 260, '독': 261, '배': 262, '느': 263, '싶': 264, '학': 265, '교': 266, '력': 267, '변': 268, '꿈': 269, '호': 270, '갈': 271, '뛰': 272, '쏴': 273, '점': 274, '격': 275, '때': 276, '끄': 277, '꺼': 278, '귀': 279, '못': 280, '크': 281, '뭐': 282, '밌': 283, '금': 284, '멈': 285, '춰': 286, '졸': 287, '쪽': 288, '반': 289, '걔': 290, '억': 291, '더': 292, '냈': 293, '줬': 294, '른': 295, '애': 296, '퇴': 297, '흥': 298, '놓': 299, '포': 300, '임': 301, '담': 302, '투': 303, '순': 304, '걱': 305, '든': 306, '강': 307, '많': 308, '받': 309, '찾': 310, '축': 311, '희': 312, '루': 313, '매': 314, '결': 315, '친': 316, '검': 317, '달': 318, '텐': 319, '후': 320, '샴': 321, '위': 322, '깐': 323, '볼': 324, '짝': 325, '설': 326, '환': 327, '엽': 328, '깊': 329, '빌': 330, '손': 331, '송': 332, '론': 333, '코': 334, '써': 335, '확': 336, '릴': 337, '존': 338, ',': 339, '빴': 340, '예': 341, '패': 342, '끙': 343, '국': 344, '데': 345, '떨': 346, '밍': 347, '옹': 348, '썼': 349, '익': 350, '갔': 351, '낄': 352, '치': 353, '틀': 354, '현': 355, '각': 356, '냥': 357, '추': 358, '땅': 359, '껴': 360, '덜': 361, '깨': 362, '엇': 363, '뭔': 364, '꽃': 365, '향': 366, '폭': 367, '드': 368, '섰': 369, '냐': 370, '단': 371, '큰': 372, '똑': 373, '끓': 374, '랩': 375, '꿔': 376, '초': 377, '꼈': 378, '례': 379, '판': 380, '창': 381, '같': 382, '찼': 383, '즉': 384, '뀌': 385, '카': 386, '샀': 387, '럽': 388, '밖': 389, '술': 390, '꽤': 391, '허': 392, '면': 393, '케': 394, '랐': 395, '런': 396, '꼼': 397, '겠': 398, '럴': 399, '굉': 400, '힘': 401, '흘': 402, '짖': 403, '준': 404, '쥐': 405, '7': 406, '4': 407, '5': 408, '죠': 409, '착': 410, '꾸': 411, '벅': 412, '뜨': 413, '왼': 414, '좌': 415, '얘': 416, '커': 417, '찌': 418, '뜻': 419, '쓰': 420, '옆': 421, '헉': 422, '댔': 423, '덕': 424, '품': 425, '활': 426, '맥': 427, '긴': 428, '충': 429, '최': 430, '란': 431, '별': 432, '종': 433, '찡': 434, '득': 435, '씩': 436, '씨': 437, '앓': 438, '셨': 439, '릎': 440, '꿇': 441, '쇠': 442, '붉': 443, '앞': 444, '령': 445, '숟': 446, '낡': 447, '흐': 448, '망': 449, '승': 450, '낙': 451, '싱': 452, '긋': 453, '빙': 454, '레': 455, '찔': 456, '궁': 457, '렁': 458, '렵': 459, '멀': 460, '쩡': 461, '듣': 462, '쓱': 463, '휘': 464, '병': 465, '겨': 466, '납': 467, '앙': 468, '긍': 469, '총': 470, '쨌': 471, '백': 472, '업': 473, '뭇': 474, '쪼': 475, '편': 476, '녹': 477, '티': 478, '켓': 479, '항': 480, '섭': 481, '처': 482, '랬': 483, '록': 484, '롭': 485, '목': 486, '삐': 487, '윤': 488, '잔': 489, '황': 490, '될': 491, '육': 492, '멍': 493, '청': 494, '꽉': 495, '표': 496, '겐': 497, '몇': 498, '십': 499, '셔': 500, '산': 501, '갰': 502, '갑': 503, '잃': 504, '농': 505, '체': 506, '관': 507, '뭘': 508, '딨': 509, '획': 510, '료': 511, '줄': 512, '낼': 513, '톱': 514, '곱': 515, '걷': 516, '흔': 517, '훔': 518, '널': 519, '취': 520, '철': 521, '극': 522, '낯': 523, '뿐': 524, '삶': 525, '짧': 526, '께': 527, '악': 528, '민': 529, '응': 530, '메': 531, '토': 532, '슨': 533, '욕': 534, '끼': 535, '앵': 536, '2': 537, '0': 538, '1': 539, '3': 540, '년': 541, '태': 542, '곧': 543, '닮': 544, '림': 545, '탓': 546, '옮': 547, '쉽': 548, '등': 549, '밤': 550, '끈': 551, '묶': 552, '월': 553, '깜': 554, '놀': 555, '쯤': 556, '올': 557, '컵': 558}\n"
     ]
    }
   ],
   "source": [
    "tokenizer_target = Tokenizer(num_words=None, char_level=True,\n",
    "                            lower=False)\n",
    "tokenizer_target.fit_on_texts(data.target_input)\n",
    "word_index_target = tokenizer_target.word_index\n",
    "\n",
    "print('\\n전체에서 %s개의 고유한 토큰을 찾았습니다.' %len(word_index_target))\n",
    "print('word_index_source: ', word_index_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4abe20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result of encoder_input sequencing: \n",
      "Go. [38, 3, 4]\n",
      "Hi. [32, 7, 4]\n",
      "Run! [49, 16, 9, 30]\n",
      "Run. [49, 16, 9, 4]\n"
     ]
    }
   ],
   "source": [
    "encoder_input = tokenizer_source.texts_to_sequences(data.source)\n",
    "\n",
    "print('\\nResult of encoder_input sequencing: ')\n",
    "print(data.source[0], encoder_input[0])\n",
    "print(data.source[1], encoder_input[1])\n",
    "print(data.source[2], encoder_input[2])\n",
    "print(data.source[3], encoder_input[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "550c67ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go."
     ]
    }
   ],
   "source": [
    "for num in encoder_input[0]:\n",
    "    for key, value in word_index_source.items():\n",
    "        if num == value:\n",
    "            print(key, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9664f781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result of decoder_input sequencing: \n",
      "\t가.\n",
      " [2, 13, 4, 3]\n",
      "\t안녕.\n",
      " [2, 51, 223, 4, 3]\n",
      "\t뛰어!\n",
      " [2, 272, 5, 18, 3]\n",
      "\n",
      "Result of decoder_target sequencing: \n",
      "가.\n",
      " [13, 4, 3]\n",
      "안녕.\n",
      " [51, 223, 4, 3]\n",
      "뛰어!\n",
      " [272, 5, 18, 3]\n"
     ]
    }
   ],
   "source": [
    "decoder_input = tokenizer_target.texts_to_sequences(data.target_input)\n",
    "decoder_output = tokenizer_target.texts_to_sequences(data.target_output)\n",
    "\n",
    "print('\\nResult of decoder_input sequencing: ')\n",
    "print(data.target_input[0], decoder_input[0])\n",
    "print(data.target_input[1], decoder_input[1])\n",
    "print(data.target_input[2], decoder_input[2])\n",
    "\n",
    "print('\\nResult of decoder_target sequencing: ')\n",
    "print(data.target_output[0], decoder_output[0])\n",
    "print(data.target_output[1], decoder_output[1])\n",
    "print(data.target_output[2], decoder_output[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31b4b31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.source type: <class 'pandas.core.series.Series'>\n",
      "encoder_input type: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print('data.source type:', type(data.source))\n",
    "print('encoder_input type:', type(encoder_input))\n",
    "\n",
    "# print('data.source:\\n', data.source)\n",
    "# print('encoder_input:\\n', encoder_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c94e0d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Padding result sample: \n",
      "\t가.\n",
      " [ 2 13  4  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "encoder_input = pad_sequences(encoder_input, maxlen=max_src_len,\n",
    "                             padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen=max_tar_len,\n",
    "                             padding='post')\n",
    "decoder_output = pad_sequences(decoder_output, maxlen=max_tar_len,\n",
    "                              padding='post')\n",
    "\n",
    "print('\\nPadding result sample: ')\n",
    "print(data.target_input[0], decoder_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3b78ff",
   "metadata": {},
   "source": [
    "### padding = \"post\"로 하는 이유 2가지.\n",
    "1. 무의미하게 만들어진 패딩을 처음부터 처리할 필요가 없으므로.\n",
    "2. 훈련일때는 문제가 아니지만, predict 때 새로운 문장이 들어왔을 때는 문제가 된다. 이전 스텝의 결과가 다음 스텝의 input으로 들어가기 때문에(teaching force) padding이 문장 앞에 있으면, 다음 스텝에 전달하는 문자나 단어가 0이 되므로 문장 생성이 제대로 이어지기 어렵다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c879521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_input length: 1000\n",
      "decoder_input type: <class 'numpy.ndarray'>\n",
      "decoder_input shape: (1000, 19)\n"
     ]
    }
   ],
   "source": [
    "print('decoder_input length:', len(decoder_input))\n",
    "print('decoder_input type:', type(decoder_input))\n",
    "print('decoder_input shape:', decoder_input.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5aaca0",
   "metadata": {},
   "source": [
    "# 음절 단위라 워드 임베딩을 하지 않는다.\n",
    "- 그래서 to_categorical을 이용해서 3D로 만든다.\n",
    "- 워드임베딩을 못쓰니까 차선책으로.\n",
    "- 결국 원핫 인코딩을 통해 2D에서 3D로 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "382f8b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "encoder_input = to_categorical(encoder_input, num_classes=len(word_index_source)+1)\n",
    "decoder_input = to_categorical(decoder_input, num_classes=len(word_index_target)+1)\n",
    "decoder_output = to_categorical(decoder_output, num_classes=len(word_index_target)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba750890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result of One-Hot Encodded decoder_input sequencing: \n",
      "(1000, 19, 559)\n",
      "\t가.\n",
      " [[0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print('\\nResult of One-Hot Encodded decoder_input sequencing: ')\n",
    "print(decoder_input.shape)\n",
    "print(data.target_input[0], decoder_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b123154",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-0\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0.]\n",
      "0-1\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0.]\n",
      "0-2\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0.]\n",
      "0-18\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print('0-0\\n', decoder_input[0][0])\n",
    "print('0-1\\n', decoder_input[0][1])\n",
    "print('0-2\\n', decoder_input[0][2])\n",
    "print('0-18\\n', decoder_input[0][18])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abde067",
   "metadata": {},
   "source": [
    "### to_categorical() & texts_to_matrix() 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4427b445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_index:  {'우리': 1, '소망': 2, '꿈': 3, '통일': 4}\n",
      "data: [[1, 2, 3]]\n",
      "to_categorical:  [[[0. 1. 0. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 0. 0. 1.]]]\n",
      "[[0. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "texts = ['우리 소망 꿈 통일']\n",
    "tokenizer = Tokenizer(num_words=4)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "word_index = tokenizer.word_index\n",
    "data = tokenizer.texts_to_sequences(texts)\n",
    "print('word_index: ', word_index)\n",
    "print('data:', data)\n",
    "print('to_categorical: ', to_categorical(data, num_classes=4))\n",
    "print(tokenizer.texts_to_matrix(texts, mode='binary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41657905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_input length: 1000\n",
      "decoder_input type: <class 'numpy.ndarray'>\n",
      "decoder_input shape: (1000, 19, 559)\n"
     ]
    }
   ],
   "source": [
    "print('decoder_input length:', len(decoder_input))\n",
    "print('decoder_input type:', type(decoder_input))\n",
    "print('decoder_input shape:', decoder_input.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0605572a",
   "metadata": {},
   "source": [
    "# 훈련용 Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b744bd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "\n",
    "encoder_inputs = Input(shape=(None, len(word_index_source)+1))\n",
    "\n",
    "encoder_lstm = LSTM(units=256, return_state=True)\n",
    "\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62aca35",
   "metadata": {},
   "source": [
    "# 훈련용 Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e613701c",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, len(word_index_target)+1))\n",
    "\n",
    "decoder_lstm = LSTM(units=256, return_sequences=True, return_state=True)\n",
    "\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "\n",
    "decoder_dense = Dense(len(word_index_target)+1, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e31669a",
   "metadata": {},
   "source": [
    "# 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67621a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/1\n"
     ]
    }
   ],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit(x=[encoder_input, decoder_input], y=decoder_output, batch_size=64, epochs=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff07e27",
   "metadata": {},
   "source": [
    "# 예측용 Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f75fb037",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e2cc5a4752bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencoder_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Model' is not defined"
     ]
    }
   ],
   "source": [
    "encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e4ee46",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs,\n",
    "                                                 initial_state=decoder_states_inputs)\n",
    "\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs,\n",
    "                      outputs=[decoder_outputs] + decoder_states)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2d2316",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_src = dict((i, char) for char, i in word_index_source.items())\n",
    "index_to_tar = dict((i, char) for char, i in word_index_target.items())\n",
    "print(index_to_tar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f76062",
   "metadata": {},
   "source": [
    "# 예측함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1358de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    target_seq = np.zeros((1, 1, len(word_index_target)+1))\n",
    "    target_seq[0, 0, word_index_target['\\t']] = 1.\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "    \n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        sampled_token_index = np.argmax(output_tokens)\n",
    "        \n",
    "        if(sampled_token_index==0):\n",
    "            sampled_token_index = 1\n",
    "        \n",
    "        sampled_char = index_to_tar[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "        \n",
    "        if (sampled_char == '\\n' or len(decoded_sentence) > max_tar_len):\n",
    "            stop_condition = True\n",
    "            \n",
    "        target_seq = np.zeros((1, 1, len(word_index_target)+1))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "        states_value = [h, c]\n",
    "        \n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e2d5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for seq_index in [1, 2, 3]:\n",
    "    input_seq  = encoder_input[seq_index:seq_index+1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    \n",
    "    print('-'*35)\n",
    "    print('입력 문장:', data.source[seq_index])\n",
    "    print('정답 문장:', data.target[seq_index][1:len(data.target[seq_index])])\n",
    "    print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e2a353",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96594d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp3710",
   "language": "python",
   "name": "nlp3710"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
