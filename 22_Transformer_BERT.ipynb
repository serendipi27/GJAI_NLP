{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c98b167",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import rc\n",
    "import os\n",
    "\n",
    "rc('font', family='AppleGothic')\n",
    "\n",
    "model_name = 'transformer'\n",
    "path = '/Users/jsha/gjai/nlp/pytest/'\n",
    "save_path = '/Users/jsha/gjai/nlp/22_practice/'\n",
    "\n",
    "if not os.path.exists(save_path+model_name):\n",
    "    os.mkdir(save_path+model_name)\n",
    "    print(f'making dir {save_path+model_name}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6990d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data length:  19\n",
      "data sample:                   Q            A  label\n",
      "0           12시 땡!   하루가 또 가네요.      0\n",
      "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
      "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
      "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
      "4          PPL 심하네   눈살이 찌푸려지죠.      0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(path+'chatdata_small.csv', names=['Q', 'A', 'label'],\n",
    "                  sep = ',', header=0, encoding='cp949')\n",
    "print('data length: ', len(data))\n",
    "print('data sample: ', data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b031713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:  ['12시 땡!', '1지망 학교 떨어졌어', '3박4일 놀러가고 싶다', '3박4일 정도 놀러가고 싶다', 'PPL 심하네', 'SD카드 망가졌어', 'SD카드 안돼', 'SNS 맞팔 왜 안하지ㅠㅠ', 'SNS 시간낭비인 거 아는데 매일 하는 중', 'SNS 시간낭비인데 자꾸 보게됨', 'SNS보면 나만 빼고 다 행복해보여', '가끔 궁금해', '가끔 뭐하는지 궁금해', '가끔은 혼자인게 좋다', '가난한 자의 설움', '가만 있어도 땀난다', '가상화폐 쫄딱 망함', '가스불 켜고 나갔어', '가스불 켜놓고 나온거 같아']\n",
      "outputs:  ['하루가 또 가네요.', '위로해 드립니다.', '여행은 언제나 좋죠.', '여행은 언제나 좋죠.', '눈살이 찌푸려지죠.', '다시 새로 사는 게 마음 편해요.', '다시 새로 사는 게 마음 편해요.', '잘 모르고 있을 수도 있어요.', '시간을 정하고 해보세요.', '시간을 정하고 해보세요.', '자랑하는 자리니까요.', '그 사람도 그럴 거예요.', '그 사람도 그럴 거예요.', '혼자를 즐기세요.', '돈은 다시 들어올 거예요.', '땀을 식혀주세요.', '어서 잊고 새출발 하세요.', '빨리 집에 돌아가서 끄고 나오세요.', '빨리 집에 돌아가서 끄고 나오세요.']\n"
     ]
    }
   ],
   "source": [
    "inputs, outputs = list(data['Q']), list(data['A'])\n",
    "print('inputs: ', inputs)\n",
    "print('outputs: ', outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68f7c705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "outputs_input:\n",
      " 16        <SOS> 어서 잊고 새출발 하세요. <EOS>\n",
      "1              <SOS> 위로해 드립니다. <EOS>\n",
      "7       <SOS> 잘 모르고 있을 수도 있어요. <EOS>\n",
      "6     <SOS> 다시 새로 사는 게 마음 편해요. <EOS>\n",
      "11         <SOS> 그 사람도 그럴 거예요. <EOS>\n",
      "Name: A, dtype: object\n",
      "\n",
      "outputs_output:\n",
      " 1         위로해 드립니다. <EOS>\n",
      "15        땀을 식혀주세요. <EOS>\n",
      "8     시간을 정하고 해보세요. <EOS>\n",
      "10      자랑하는 자리니까요. <EOS>\n",
      "2       여행은 언제나 좋죠. <EOS>\n",
      "Name: A, dtype: object\n"
     ]
    }
   ],
   "source": [
    "outputs_input = data.A.apply(lambda x: '<SOS> ' +x+ ' <EOS>')\n",
    "outputs_output = data.A.apply(lambda x: x+ ' <EOS>')\n",
    "\n",
    "print('\\noutputs_input:\\n', outputs_input.sample(5))\n",
    "print('\\noutputs_output:\\n', outputs_output.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56ae30df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "전체에서 100개의 고유한 토큰을 찾았습니다.\n",
      "word_index:  {'SOS': 1, 'EOS': 2, 'SNS': 3, '다시': 4, '거예요': 5, '3박4일': 6, '놀러가고': 7, '싶다': 8, 'SD카드': 9, '가끔': 10, '궁금해': 11, '가스불': 12, '여행은': 13, '언제나': 14, '좋죠': 15, '새로': 16, '사는': 17, '게': 18, '마음': 19, '편해요': 20, '시간을': 21, '정하고': 22, '해보세요': 23, '그': 24, '사람도': 25, '그럴': 26, '빨리': 27, '집에': 28, '돌아가서': 29, '끄고': 30, '나오세요': 31, '12시': 32, '땡': 33, '1지망': 34, '학교': 35, '떨어졌어': 36, '정도': 37, 'PPL': 38, '심하네': 39, '망가졌어': 40, '안돼': 41, '맞팔': 42, '왜': 43, '안하지ㅠㅠ': 44, '시간낭비인': 45, '거': 46, '아는데': 47, '매일': 48, '하는': 49, '중': 50, '시간낭비인데': 51, '자꾸': 52, '보게됨': 53, 'SNS보면': 54, '나만': 55, '빼고': 56, '다': 57, '행복해보여': 58, '뭐하는지': 59, '가끔은': 60, '혼자인게': 61, '좋다': 62, '가난한': 63, '자의': 64, '설움': 65, '가만': 66, '있어도': 67, '땀난다': 68, '가상화폐': 69, '쫄딱': 70, '망함': 71, '켜고': 72, '나갔어': 73, '켜놓고': 74, '나온거': 75, '같아': 76, '하루가': 77, '또': 78, '가네요': 79, '위로해': 80, '드립니다': 81, '눈살이': 82, '찌푸려지죠': 83, '잘': 84, '모르고': 85, '있을': 86, '수도': 87, '있어요': 88, '자랑하는': 89, '자리니까요': 90, '혼자를': 91, '즐기세요': 92, '돈은': 93, '들어올': 94, '땀을': 95, '식혀주세요': 96, '어서': 97, '잊고': 98, '새출발': 99, '하세요': 100}\n",
      "vocab_size: 100\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "inputs_series = pd.Series(inputs)\n",
    "inputs_outputs = pd.concat([inputs_series, outputs_input], axis=0)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=None, char_level=False, lower=False)\n",
    "tokenizer.fit_on_texts(inputs_outputs)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print('\\n전체에서 %s개의 고유한 토큰을 찾았습니다.' % len(word_index))\n",
    "print('word_index: ', word_index)\n",
    "print('vocab_size:', len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f1183cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jsha/gjai/nlp/22_practice/transformer --- Folder create complete \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "if not os.path.exists(save_path+model_name):\n",
    "    print('{} --- Folder alread exists \\n'.format(save_path+model_name))\n",
    "else:\n",
    "    os.makedirs(save_path+model_name, exist_ok=True)\n",
    "    print('{} --- Folder create complete \\n'.format(save_path+model_name))\n",
    "\n",
    "with open(save_path+model_name+'/transformer.pickle', 'wb') as file:\n",
    "    pickle.dump(tokenizer, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89800f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result of encoder_input sequencing: \n",
      "12시 땡! [32, 33]\n",
      "1지망 학교 떨어졌어 [34, 35, 36]\n",
      "3박4일 놀러가고 싶다 [6, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "encoder_input = tokenizer.texts_to_sequences(list(inputs))\n",
    "print('\\nResult of encoder_input sequencing: ')\n",
    "print(inputs[0], encoder_input[0])\n",
    "print(inputs[1], encoder_input[1])\n",
    "print(inputs[2], encoder_input[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24605540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result of encoder_input sequencing: \n",
      "12시 땡! [1, 77, 78, 79, 2]\n",
      "1지망 학교 떨어졌어 [1, 80, 81, 2]\n",
      "3박4일 놀러가고 싶다 [1, 13, 14, 15, 2]\n",
      "하루가 또 가네요. [77, 78, 79, 2]\n",
      "위로해 드립니다. [80, 81, 2]\n",
      "여행은 언제나 좋죠. [13, 14, 15, 2]\n"
     ]
    }
   ],
   "source": [
    "decoder_input = tokenizer.texts_to_sequences(list(outputs_input))\n",
    "decoder_target = tokenizer.texts_to_sequences(list(outputs_output))\n",
    "\n",
    "print('\\nResult of encoder_input sequencing: ')\n",
    "print(inputs[0], decoder_input[0])\n",
    "print(inputs[1], decoder_input[1])\n",
    "print(inputs[2], decoder_input[2])\n",
    "\n",
    "print(outputs[0], decoder_target[0])\n",
    "print(outputs[1], decoder_target[1])\n",
    "print(outputs[2], decoder_target[2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4075515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sencetence max length:  8\n"
     ]
    }
   ],
   "source": [
    "sentence_max_length = inputs_outputs.apply(lambda x: len(x.split())).max()\n",
    "print('sencetence max length: ', sentence_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2b61fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "encoder_input_pad = pad_sequences(encoder_input, maxlen=sentence_max_length,\n",
    "                                 padding='post')\n",
    "decoder_input_pad = pad_sequences(decoder_input, maxlen=sentence_max_length,\n",
    "                                 padding='post')\n",
    "decoder_target_pad = pad_sequences(decoder_target, maxlen=sentence_max_length,\n",
    "                                 padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c74f7704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "encoder_input_pad shape:  (19, 8)\n",
      "inputs:  1지망 학교 떨어졌어\n",
      "encoder_input:  [34, 35, 36]\n",
      "encoder_input_pad:  [34 35 36  0  0  0  0  0]\n",
      "\n",
      "decoder_input_pad shape:  (19, 8)\n",
      "outputs_input:  <SOS> 위로해 드립니다. <EOS>\n",
      "decoder_input:  [1, 80, 81, 2]\n",
      "decoder_input_pad:  [ 1 80 81  2  0  0  0  0]\n",
      "\n",
      "decoder_target_pad shape:  (19, 8)\n",
      "outputs_target:  위로해 드립니다. <EOS>\n",
      "decoder_target:  [80, 81, 2]\n",
      "decoder_target_pad:  [80 81  2  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "print('\\nencoder_input_pad shape: ', encoder_input_pad.shape)\n",
    "print('inputs: ', inputs[1])\n",
    "print('encoder_input: ', encoder_input[1])\n",
    "print('encoder_input_pad: ', encoder_input_pad[1])\n",
    "\n",
    "print('\\ndecoder_input_pad shape: ', decoder_input_pad.shape)\n",
    "print('outputs_input: ', outputs_input[1])\n",
    "print('decoder_input: ', decoder_input[1])\n",
    "print('decoder_input_pad: ', decoder_input_pad[1])\n",
    "\n",
    "print('\\ndecoder_target_pad shape: ', decoder_target_pad.shape)\n",
    "print('outputs_target: ', outputs_output[1])\n",
    "print('decoder_target: ', decoder_target[1])\n",
    "print('decoder_target_pad: ', decoder_target_pad[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41ed4eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import enum\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f0b4ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED_NUM = 1234\n",
    "tf.random.set_seed(SEED_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfed3a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_INDEX = 0\n",
    "STD_INDEX = 1\n",
    "END_INDEX = 2\n",
    "\n",
    "index_inputs = encoder_input_pad\n",
    "index_outputs = decoder_input_pad\n",
    "index_targets = decoder_target_pad\n",
    "\n",
    "char2idx_dict = word_index\n",
    "idx2char_dict = {y: x for x, y in word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6311f2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx_dict['<PAD>'] = 0\n",
    "\n",
    "char2idx_dict['<SOS>'] = char2idx_dict['SOS']\n",
    "del char2idx_dict['SOS']\n",
    "\n",
    "char2idx_dict['<END>'] = char2idx_dict['EOS']\n",
    "del char2idx_dict['EOS']\n",
    "\n",
    "idx2char_dict[0] = '<PAD>'\n",
    "idx2char_dict[1] = '<SOS>'\n",
    "idx2char_dict[1] = '<END>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5da801ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'char2idx': {'SNS': 3, '다시': 4, '거예요': 5, '3박4일': 6, '놀러가고': 7, '싶다': 8, 'SD카드': 9, '가끔': 10, '궁금해': 11, '가스불': 12, '여행은': 13, '언제나': 14, '좋죠': 15, '새로': 16, '사는': 17, '게': 18, '마음': 19, '편해요': 20, '시간을': 21, '정하고': 22, '해보세요': 23, '그': 24, '사람도': 25, '그럴': 26, '빨리': 27, '집에': 28, '돌아가서': 29, '끄고': 30, '나오세요': 31, '12시': 32, '땡': 33, '1지망': 34, '학교': 35, '떨어졌어': 36, '정도': 37, 'PPL': 38, '심하네': 39, '망가졌어': 40, '안돼': 41, '맞팔': 42, '왜': 43, '안하지ㅠㅠ': 44, '시간낭비인': 45, '거': 46, '아는데': 47, '매일': 48, '하는': 49, '중': 50, '시간낭비인데': 51, '자꾸': 52, '보게됨': 53, 'SNS보면': 54, '나만': 55, '빼고': 56, '다': 57, '행복해보여': 58, '뭐하는지': 59, '가끔은': 60, '혼자인게': 61, '좋다': 62, '가난한': 63, '자의': 64, '설움': 65, '가만': 66, '있어도': 67, '땀난다': 68, '가상화폐': 69, '쫄딱': 70, '망함': 71, '켜고': 72, '나갔어': 73, '켜놓고': 74, '나온거': 75, '같아': 76, '하루가': 77, '또': 78, '가네요': 79, '위로해': 80, '드립니다': 81, '눈살이': 82, '찌푸려지죠': 83, '잘': 84, '모르고': 85, '있을': 86, '수도': 87, '있어요': 88, '자랑하는': 89, '자리니까요': 90, '혼자를': 91, '즐기세요': 92, '돈은': 93, '들어올': 94, '땀을': 95, '식혀주세요': 96, '어서': 97, '잊고': 98, '새출발': 99, '하세요': 100, '<PAD>': 0, '<SOS>': 1, '<END>': 2}, 'idx2char': {1: '<END>', 2: 'EOS', 3: 'SNS', 4: '다시', 5: '거예요', 6: '3박4일', 7: '놀러가고', 8: '싶다', 9: 'SD카드', 10: '가끔', 11: '궁금해', 12: '가스불', 13: '여행은', 14: '언제나', 15: '좋죠', 16: '새로', 17: '사는', 18: '게', 19: '마음', 20: '편해요', 21: '시간을', 22: '정하고', 23: '해보세요', 24: '그', 25: '사람도', 26: '그럴', 27: '빨리', 28: '집에', 29: '돌아가서', 30: '끄고', 31: '나오세요', 32: '12시', 33: '땡', 34: '1지망', 35: '학교', 36: '떨어졌어', 37: '정도', 38: 'PPL', 39: '심하네', 40: '망가졌어', 41: '안돼', 42: '맞팔', 43: '왜', 44: '안하지ㅠㅠ', 45: '시간낭비인', 46: '거', 47: '아는데', 48: '매일', 49: '하는', 50: '중', 51: '시간낭비인데', 52: '자꾸', 53: '보게됨', 54: 'SNS보면', 55: '나만', 56: '빼고', 57: '다', 58: '행복해보여', 59: '뭐하는지', 60: '가끔은', 61: '혼자인게', 62: '좋다', 63: '가난한', 64: '자의', 65: '설움', 66: '가만', 67: '있어도', 68: '땀난다', 69: '가상화폐', 70: '쫄딱', 71: '망함', 72: '켜고', 73: '나갔어', 74: '켜놓고', 75: '나온거', 76: '같아', 77: '하루가', 78: '또', 79: '가네요', 80: '위로해', 81: '드립니다', 82: '눈살이', 83: '찌푸려지죠', 84: '잘', 85: '모르고', 86: '있을', 87: '수도', 88: '있어요', 89: '자랑하는', 90: '자리니까요', 91: '혼자를', 92: '즐기세요', 93: '돈은', 94: '들어올', 95: '땀을', 96: '식혀주세요', 97: '어서', 98: '잊고', 99: '새출발', 100: '하세요', 0: '<PAD>'}, 'vocab_size': 101, 'pad_symbol': '<PAD>', 'std_symbol': '<SOS>', 'end_symbol': '<END>'}\n"
     ]
    }
   ],
   "source": [
    "prepro_configs = {'char2idx':char2idx_dict, 'idx2char':idx2char_dict,\n",
    "                 'vocab_size':len(word_index), 'pad_symbol':'<PAD>',\n",
    "                 'std_symbol':'<SOS>', 'end_symbol':'<END>'}\n",
    "print(prepro_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43e37c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx = prepro_configs['char2idx']\n",
    "end_index = prepro_configs['end_symbol']\n",
    "vocab_size = prepro_configs['vocab_size']\n",
    "BATCH_SIZE = 2\n",
    "MAX_SEQUENCE = 25\n",
    "EPOCHS = 30\n",
    "VALID_SPLIT = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bf1491",
   "metadata": {},
   "source": [
    "### 밑의 kargs에 대한 내용은 노트에 있음. 2022.10.24.일자 노트를 찾아"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87e7c62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kargs = {'model_name': model_name,\n",
    "        'num_layers':2,\n",
    "        'd_model':512, \n",
    "        'num_heads':8,\n",
    "         'dff':2048,\n",
    "         'input_vocab_size':vocab_size,\n",
    "         'target_vocab_size':vocab_size,\n",
    "         'maximum_position_encoding':MAX_SEQUENCE,\n",
    "         'end_token_idx':char2idx[end_index],\n",
    "         'rate':0.1\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "716b235a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1dba991c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "358bf208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "    enc_padding_mask = create_padding_mask(inp) # encoder \n",
    "    dec_padding_mask = create_padding_mask(inp) # decoder 두번째 어텐션 블록에서 사용\n",
    "    \n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "    # combined_mask는 decoder 첫번째 어텐션에서 사용\n",
    "    \n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4cc49cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(index_inputs, index_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80e5d55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2*i//2) / np.float32(d_model))\n",
    "    \n",
    "    return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f7fd81a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                           np.arange(d_model)[np.newaxis, :],\n",
    "                           d_model)\n",
    "    \n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, ::2])\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])    \n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    \n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96759e7e",
   "metadata": {},
   "source": [
    "### 돌발 퀴즈: 임베딩의 결과는 (batch_size, input_length, feature_dimension)입니다. 이 셋의 관계를 서술하시오.\n",
    "\n",
    "- batch_size: 데이터의 전체 문장 수를 의미함.\n",
    "- input_length: 패딩의 결과로 나온 문장별 단어의 수.\n",
    "- feature_dimension: 임베딩 층을 지나면서 나온 결과로서 각 단어별 유사도를 표현하는 특징의 수."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1e65fc",
   "metadata": {},
   "source": [
    "# 스케일 내적 어텐션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc0e146a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attendion(q,k,v, mask):\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b = True)\n",
    "    \n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "    \n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "        \n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "    \n",
    "    output = tf.matmul(attention_weights, v)\n",
    "    \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1026c74c",
   "metadata": {},
   "source": [
    "# 멀티 헤드 어텐션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb29c23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kargs):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = kargs['num_heads']\n",
    "        self.d_model = kargs['d_model']\n",
    "        \n",
    "        assert self.d_model % self.num_heads == 0\n",
    "        \n",
    "        self.depth = self.d_model // self.num_heads\n",
    "        \n",
    "        self.wq = tf.keras.layers.Dense(kargs['d_model'])\n",
    "        self.wk = tf.keras.layers.Dense(kargs['d_model'])        \n",
    "        self.wv = tf.keras.layers.Dense(kargs['d_model'])        \n",
    "        \n",
    "        self.dense = tf.keras.layers.Dense(kargs['d_model'])\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        \n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    # 이 함수는 class에서 상속받은 메소드. model.fit할 때 자동으로 실행된다.\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        \n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "        \n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "        \n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
    "        \n",
    "        output = self.dense(concat_attention)\n",
    "        \n",
    "        return output, attention_weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "76250c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward_network(**kargs):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(kargs['dff'], activation='relu'),\n",
    "        tf.keras.layers.Dense(kargs['d_model'])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ae5450ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.mha = MultiHeadAttention(**kargs)\n",
    "        self.ffn = feed_forward_network(**kargs)\n",
    "        \n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.alyernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dropout1 = tf.keras.layers.Dropout(kargs['rate'])\n",
    "        self.dropout2 = tf.keras.layers.Dropout(kargs['rate'])\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        attn_output, _ = self.mha(x, x, x, mask)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "        \n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "        \n",
    "        return out2\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e335292f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kargs):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        self.mha1 = MultiHeadAttention(**kargs)\n",
    "        self.mha2 = MultiHeadAttention(**kargs)\n",
    "        self.ffn = feed_forward_network(**kargs)\n",
    "        \n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)        \n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)        \n",
    "        \n",
    "        self.dropout1 = tf.keras.layers.Dropout(kargs['rate'])\n",
    "        self.dropout2 = tf.keras.layers.Dropout(kargs['rate'])        \n",
    "        self.dropout3 = tf.keras.layers.Dropout(kargs['rate'])        \n",
    "        \n",
    "    def call(self, x, enc_output, look_ahead_mask, padding_mask):\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
    "        att1 = self.dropout1(attn1)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "        \n",
    "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
    "        att2 = self.dropout2(attn2)\n",
    "        out2 = self.layernorm2(attn2 + out1)\n",
    "        \n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output)\n",
    "        out3 = self.layernorm3(ffn_output + out2)\n",
    "        \n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635bb230",
   "metadata": {},
   "source": [
    "# 인코더 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1203ec89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = kargs['d_model']\n",
    "        self.num_layers = kargs['num_layers']\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim=kargs['input_vocab_size'],\n",
    "                                                  output_dim=self.d_model)\n",
    "        self.pos_encoding = positional_encoding(position=kargs['maximum_position_encoding'],\n",
    "                                               d_model=self.d_model)\n",
    "        \n",
    "        self.enc_layers = [EncoderLayer(**kargs) for _ in range(self.num_layers)]\n",
    "        \n",
    "        self.dropout = tf.keras.layers.Dropout(kargs['rate'])\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32)) # 이건 옵션.\n",
    "        x += self.pos_encoding[:, :seq_len, :] \n",
    "        \n",
    "        x = self.dropout(x) # 이건 옵션.\n",
    "        \n",
    "        # enc_layer 반복. 여기서는 두번 돌리는 걸로 나옴. 가중치 학습을 시키는 것.\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, mask)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed46fff4",
   "metadata": {},
   "source": [
    "# 디코더 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "73d7396c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kargs):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.d_model = kargs['d_model']\n",
    "        self.num_layers = kargs['num_layers']\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim=kargs['target_vocab_size'],\n",
    "                                                  output_dim=self.d_model)\n",
    "        \n",
    "        self.pos_encoding = positional_encoding(position=kargs['maximum_position_encoding'],\n",
    "                                                              d_model=self.d_model)\n",
    "        \n",
    "        self.dec_layers = [DecoderLayer(**kargs) for _ in range(self.num_layers)]\n",
    "        \n",
    "        self.dropout = tf.keras.layers.Dropout(kargs['rate'])\n",
    "        \n",
    "    def call(self, x, enc_output, look_ahead_mask, padding_mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, look_ahead_mask, padding_mask)\n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "            \n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b9b477",
   "metadata": {},
   "source": [
    "# 트랜스포머 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c8a6533a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, **kargs):\n",
    "        super().__init__(name=kargs['model_name'])\n",
    "        self.end_token_idx = kargs['end_token_idx']\n",
    "        \n",
    "        self.encoder = Encoder(**kargs)\n",
    "        self.decoder = Decoder(**kargs)\n",
    "        \n",
    "        self.final_layer = tf.keras.layers.Dense(kargs['target_vocab_size'])\n",
    "        \n",
    "    def call(self, x):\n",
    "        inp, tar = x\n",
    "        \n",
    "        enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(inp, tar)\n",
    "        \n",
    "        # 인코더 출력 결과(batch_size, inp_seq_len, d_model)\n",
    "        enc_output = self.encoder(inp, enc_padding_mask)\n",
    "        \n",
    "        dec_output, _ = self.decoder(tar, enc_output, look_ahead_mask, dec_padding_mask)\n",
    "        \n",
    "        final_output = self.final_layer(dec_output)\n",
    "        \n",
    "        return final_output\n",
    "    \n",
    "    def inference(self, x):\n",
    "        inp = x\n",
    "        tar = tf.expand_dims([STD_INDEX], axis=0)\n",
    "        \n",
    "        enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(inp, tar)\n",
    "        enc_output = self.encoder(inp, enc_padding_mask)\n",
    "        \n",
    "        predict_tokens = list()\n",
    "        for t in range(MAX_SEQUENCE):\n",
    "            dec_output, _ = self.decoder(tar, enc_output, look_ahead_mask, dec_padding_mask)\n",
    "            final_output = self.final_layer(dec_output)\n",
    "            outputs = tf.argmax(final_output, axis=-1).numpy()\n",
    "            print('outputs', outputs)\n",
    "            pred_token = outputs[0][-1]\n",
    "            if pred_token == self.end_token_idx:\n",
    "                break\n",
    "            predict_tokens.append(pred_token)\n",
    "            tar = tf.expand_dims([STD_INDEX] + predict_tokens, axis=0)\n",
    "            _, look_ahead_mask, dec_padding_mask = create_masks(inp, tar)\n",
    "            \n",
    "        return predict_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4c689cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')\n",
    "\n",
    "def loss(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real,0))\n",
    "    \n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "def accuracy(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    mask = tf.expand_dims(tf.cast(mask, dtype=pred.dtype), axis=-1)\n",
    "    pred *= mask\n",
    "    acc = train_accuracy(real, pred)\n",
    "    \n",
    "    return tf.reduce_mean(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ac286ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(**kargs)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "             loss=loss, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "33117cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jsha/gjai/nlp/22_practice/transformer --- Folder already exists \n",
      "\n"
     ]
    }
   ],
   "source": [
    "earlystop_callback = EarlyStopping(monitor='val_accuracy',\n",
    "                                  min_delta=0.0001, patience=10)\n",
    "\n",
    "checkpoint_path = save_path+model_name+'/weights.h5'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    print('{} --- Folder already exists \\n'.format(checkpoint_dir))\n",
    "else:\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    print('{} --- Folder create complete \\n'.format(checkpoint_dir))\n",
    "    \n",
    "checkpointer = ModelCheckpoint(checkpoint_path, monitor='val_accuracy',\n",
    "                                  verbose=1, save_best_only=True, \n",
    "                                   save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064211d0",
   "metadata": {},
   "source": [
    "# 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ced45aa3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-9d82a867d707>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m                    \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                    \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mVALID_SPLIT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                    callbacks=[earlystop_callback, checkpointer])\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/nlp3710/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nlp3710/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m           distribution_strategy=strategy)\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nlp3710/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m         steps=steps_per_epoch)\n\u001b[0m\u001b[1;32m    517\u001b[0m     (x, y, sample_weights,\n\u001b[1;32m    518\u001b[0m      \u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nlp3710/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2431\u001b[0m     \u001b[0mis_compile_called\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2432\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2433\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compile_from_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2434\u001b[0m       \u001b[0mis_compile_called\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nlp3710/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_compile_from_inputs\u001b[0;34m(self, all_inputs, target, orig_inputs, orig_target)\u001b[0m\n\u001b[1;32m   2666\u001b[0m         \u001b[0msample_weight_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_weight_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2667\u001b[0m         \u001b[0mrun_eagerly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_eagerly\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2668\u001b[0;31m         experimental_run_tf_function=self._experimental_run_tf_function)\n\u001b[0m\u001b[1;32m   2669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2670\u001b[0m   \u001b[0;31m# TODO(omalleyt): Consider changing to a more descriptive function name.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nlp3710/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nlp3710/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m                           self.loss_functions, target_tensors):\n\u001b[1;32m    342\u001b[0m       \u001b[0mendpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TrainingEndpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m       \u001b[0mendpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_training_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_eagerly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_eagerly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_training_endpoints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nlp3710/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mcreate_training_target\u001b[0;34m(self, target, run_eagerly)\u001b[0m\n\u001b[1;32m   3036\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3037\u001b[0m         target_dtype = losses.LABEL_DTYPES_FOR_LOSSES.get(\n\u001b[0;32m-> 3038\u001b[0;31m             self.loss_fn, K.dtype(self.output))\n\u001b[0m\u001b[1;32m   3039\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3040\u001b[0m         target = K.placeholder(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nlp3710/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36mdtype\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1247\u001b[0m   \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m   \"\"\"\n\u001b[0;32m-> 1249\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'dtype'"
     ]
    }
   ],
   "source": [
    "history = model.fit([index_inputs, index_outputs], index_targets,\n",
    "                   batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "                   validation_split=VALID_SPLIT,\n",
    "                   callbacks=[earlystop_callback, checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e127fcc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp3710",
   "language": "python",
   "name": "nlp3710"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
